Document,Number,Title,First_App_Date,Keywords,Abstract,Claims,Current_Assignee,Summary,Problem,Solution,Topic,Status
https://patents.google.com/patent/US10318760B2,US10318760B2,System and method for privacy protection of seating biometrics,2017-04-10,"method, occupant, settings, biometrics, device, configured, receive, protection, including, positioned, within, transmitted, share, providing, related, biometric, response, privacy, transmit, plurality, setting, indicative, vehicle, includes, memory, being, provided, externally, prevent, embodiment, information, signal, least, sensors, further, system, controller, from, seating, internally, seat, first","In one embodiment, a system for providing privacy protection of biometric related information for an occupant in a vehicle is provided. The system includes a memory device and at least one controller. The at least one controller including the memory device and is configured to receive first biometric information for a vehicle occupant from a plurality of biometric sensors positioned within a seat of the vehicle and to receive a first signal indicative of a first privacy setting from a plurality of privacy settings to share the first biometric information. The at least one controller is further configured to transmit the first biometric information internally within the vehicle in response to the first signal and to prevent the first biometric information from being transmitted externally from the vehicle in response to the first signal.","1. A system for providing privacy protection of biometric related information for an occupant in a vehicle, the system comprising:
a memory device; and
at least one controller including the memory device and being configured to:
receive first biometric information for a vehicle occupant from a plurality of biometric sensors positioned within a seat of the vehicle;
receive a first signal indicative of a first privacy setting from a plurality of privacy settings to share the first biometric information; and
transmit the first biometric information internally within the vehicle in response to the first signal,
wherein the first privacy setting enables the vehicle to deploy an airbag in the vehicle in response to a vehicle collision and the first privacy setting corresponds to preventing the first biometric information from being transmitted externally from the vehicle in response to the first signal. 1. A system for providing privacy protection of biometric related information for an occupant in a vehicle, the system comprising:
a memory device; and
at least one controller including the memory device and being configured to:
receive first biometric information for a vehicle occupant from a plurality of biometric sensors positioned within a seat of the vehicle;
receive a first signal indicative of a first privacy setting from a plurality of privacy settings to share the first biometric information; and
transmit the first biometric information internally within the vehicle in response to the first signal,
wherein the first privacy setting enables the vehicle to deploy an airbag in the vehicle in response to a vehicle collision and the first privacy setting corresponds to preventing the first biometric information from being transmitted externally from the vehicle in response to the first signal. 2. The system of claim 1, wherein the at least one controller is further configured to receive a second signal indicative of a second privacy setting from the plurality of privacy settings. 3. The system of claim 2, wherein the at least one controller is further configured to transmit the first biometric information internally within the vehicle and externally from the vehicle to one of an emergency call center and a mobile device that is positioned external to the vehicle based on the second signal. 4. The system of claim 3, wherein the at least one controller is further configured to receive the first signal and the second signal from a user interface positioned in the vehicle. 5. The system of claim 3, wherein the at least one controller is further configured to receive the first signal and the second signal from a mobile device. 6. The system of claim 1, wherein the first biometric information corresponds to at least one of a weight of the vehicle occupant, a height of the vehicle occupant, and a body shape of the vehicle occupant. 7. The system of claim 1, wherein the at least one controller is further configured to receive second biometric information and to receive a second signal indicative of second privacy setting from the plurality of privacy settings. 8. The system of claim 7, wherein the at least one controller is further configured to transmit the second biometric information internally within the vehicle and externally from the vehicle to one of an emergency call center and a mobile device that is positioned external to the vehicle based on the second signal. 9. The system of claim 8, wherein the second biometric information corresponds to at least one of a weight of the vehicle occupant, a height of the vehicle occupant, a body shape of the vehicle occupant, a heart rate of the vehicle occupant, a medical history of the vehicle occupant, an image of the vehicle occupant, information corresponding to current medical prescriptions of the vehicle occupant, and information corresponding to bloodwork for the vehicle occupant. 10. A system for providing privacy protection of biometric related information for an occupant in a vehicle, the system comprising:
a vehicle seat; and
a controller positioned within the vehicle seat and being configured to:
receive first biometric information for a vehicle occupant from a plurality of biometric sensors;
receive a first signal indicative of a first privacy setting from a plurality of privacy settings to share the first biometric information; and
transmit the first biometric information internally within the vehicle in response to the first signal,
wherein the first privacy setting enables the vehicle to deploy an airbag in the vehicle in response to a vehicle collision and the first privacy setting corresponds to preventing the first biometric information from being transmitted externally from the vehicle in response to the first signal. 10. A system for providing privacy protection of biometric related information for an occupant in a vehicle, the system comprising:
a vehicle seat; and
a controller positioned within the vehicle seat and being configured to:
receive first biometric information for a vehicle occupant from a plurality of biometric sensors;
receive a first signal indicative of a first privacy setting from a plurality of privacy settings to share the first biometric information; and
transmit the first biometric information internally within the vehicle in response to the first signal,
wherein the first privacy setting enables the vehicle to deploy an airbag in the vehicle in response to a vehicle collision and the first privacy setting corresponds to preventing the first biometric information from being transmitted externally from the vehicle in response to the first signal. 11. The system of claim 10, wherein the controller is further configured to receive a second signal indicative of a second privacy setting from the plurality of privacy settings. 12. The system of claim 11, wherein the controller is further configured to transmit the first biometric information internally within the vehicle and to enable the first biometric information to be transmitted externally from the vehicle to one of an emergency call center and a mobile device that is positioned external to the vehicle based on the second signal. 13. The system of claim 12, wherein the controller is further configured to receive the first signal and the second signal from a user interface positioned in the vehicle. 14. The system of claim 12, wherein the controller is further configured to receive the first signal and the second signal from a mobile device. 15. The system of claim 10, wherein the first biometric information corresponds to at least one of a weight of the vehicle occupant, a height of the vehicle occupant, and a body shape of the vehicle occupant. 16. The system of claim 10, wherein the controller is further configured to receive second biometric information and to receive a second signal indicative of a second privacy setting from the plurality of privacy settings. 17. The system of claim 16, wherein the controller is further configured to transmit the second biometric information internally within the vehicle and to enable the second biometric information to be transmitted externally from the vehicle based on the second signal. 18. The system of claim 17, wherein the second biometric information corresponds to at least one of a weight of the vehicle occupant, a height of the vehicle occupant, a body shape of the vehicle occupant, a heart rate of the vehicle occupant, a medical history of the vehicle occupant, an image of the vehicle occupant, information corresponding to current medical prescriptions of the vehicle occupant, and information corresponding to bloodwork for the vehicle occupant. 19. A system for providing privacy protection of biometric related information for an occupant in a vehicle, the system comprising:
a memory device; and
a controller including the memory device and being configured to:
receive biometric information for a vehicle occupant from a first mobile device; and
receive a signal indicative of a privacy setting from a plurality of privacy settings to share the biometric information from one of a user interface mounted in the vehicle and the first mobile device,
wherein the privacy setting enables the vehicle to deploy an airbag in the vehicle in response to a vehicle collision and enables a transmission of the biometric information externally from the vehicle to one of an emergency call center and a second mobile device based on the signal. 19. A system for providing privacy protection of biometric related information for an occupant in a vehicle, the system comprising:
a memory device; and
a controller including the memory device and being configured to:
receive biometric information for a vehicle occupant from a first mobile device; and
receive a signal indicative of a privacy setting from a plurality of privacy settings to share the biometric information from one of a user interface mounted in the vehicle and the first mobile device,
wherein the privacy setting enables the vehicle to deploy an airbag in the vehicle in response to a vehicle collision and enables a transmission of the biometric information externally from the vehicle to one of an emergency call center and a second mobile device based on the signal. 20. The system of claim 19, wherein the biometric information corresponds to wherein the biometric information corresponds to at least one of a weight of the vehicle occupant, a height of the vehicle occupant, a body shape of the vehicle occupant, a heart rate of the vehicle occupant, a medical history of the vehicle occupant, an image of the vehicle occupant, information corresponding to current medical prescriptions of the vehicle occupant, and information corresponding to bloodwork for the vehicle occupant.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318763B2,US10318763B2,Smart de-identification using date jittering,2016-12-20,"method, boundary, amount, capped, calculating, second, anonymized, without, receiving, risk, produce, overlapping, than, limits, said, query, dataset, grouping, records, data, cohort, requested, return, possess, each, group, into, less, smart, respective, source, predetermined, using, jittering, capping, includes, between, that, some, more, date, maximum, identification, having, temporally, least, forming, traits, jitter, shifted, system, querying, find, from, groups, adjacent, first, time","System and method to produce an anonymized cohort having less than a predetermined risk of re-identification. The method includes receiving a data query of requested traits for the anonymized cohort, querying a data source to find records that possess at least some of the traits, forming a dataset from at least some of the records, and grouping the dataset in time into a first boundary group, a second boundary group, and one or more non-boundary groups temporally between the first boundary group and second boundary group. For each non-boundary group, calculating maximum time limits the non-boundary group can be time-shifted without overlapping an adjacent group, calculating a group jitter amount, capping the group jitter amount by the maximum time limits and by respective predetermined jitter limits, and jittering said non-boundary group by the capped group jitter amount to produce an anonymized dataset. Return the anonymized dataset.","1. A method to produce an anonymized cohort that includes members having less than a predetermined risk of re-identification, comprising:
receiving a data query via a user-facing communication channel to request an anonymized cohort, the data query comprising requested traits to include in the members of the anonymized cohort;
querying a data source having data records describing the members, using a data query transmitted via a data source-facing communication channel, to find the data records that possess at least some of the requested traits;
forming a dataset from at least some of the data records;
grouping the data records in time, by a processor coupled to the user-facing communication channel and the data source-facing communication channel, into a first boundary group, a second boundary group, and one or more non-boundary groups temporally between the first boundary group and second boundary group;
for each non-boundary group within the dataset, performing the steps of:
calculating, by the processor, maximum positive and negative time limits said non-boundary group can be time-shifted without overlapping an adjacent group;
calculating, by the processor, a group jitter amount;
capping, by the processor, the group jitter amount by the maximum positive and negative time limits; and
jittering, by the processor, said non-boundary group by the capped group jitter; and
providing, via the user-facing communication channel, the dataset. 1. A method to produce an anonymized cohort that includes members having less than a predetermined risk of re-identification, comprising:
receiving a data query via a user-facing communication channel to request an anonymized cohort, the data query comprising requested traits to include in the members of the anonymized cohort;
querying a data source having data records describing the members, using a data query transmitted via a data source-facing communication channel, to find the data records that possess at least some of the requested traits;
forming a dataset from at least some of the data records;
grouping the data records in time, by a processor coupled to the user-facing communication channel and the data source-facing communication channel, into a first boundary group, a second boundary group, and one or more non-boundary groups temporally between the first boundary group and second boundary group;
for each non-boundary group within the dataset, performing the steps of:
calculating, by the processor, maximum positive and negative time limits said non-boundary group can be time-shifted without overlapping an adjacent group;
calculating, by the processor, a group jitter amount;
capping, by the processor, the group jitter amount by the maximum positive and negative time limits; and
jittering, by the processor, said non-boundary group by the capped group jitter; and
providing, via the user-facing communication channel, the dataset. 2. The method of claim 1, further comprising capping the group jitter by respective predetermined forward and backward jitter limits. 3. The method of claim 2, wherein the predetermined forward and backward jitter limits are user-configurable. 4. The method of claim 2, wherein the predetermined forward and backward jitter limits are dependent upon a characteristic of the data. 5. The method of claim 2, wherein the predetermined forward and backward jitter limits are dependent upon how the data will be used. 6. The method of claim 2, wherein the predetermined forward and backward jitter limits comprise a first set of limits for a first non-boundary group and a second set of limits for a second non-boundary group, wherein the first set of limits is different than the second set of limits. 7. The method of claim 1, wherein calculating a group jitter amount comprises using a deterministic function. 8. The method of claim 1, wherein calculating a group jitter amount comprises using a hash function. 9. The method of claim 8, wherein the hash function output is scaled to the capped group jitter. 10. The method of claim 8, wherein the hash function hashes a date value from each respective non-boundary group by use of a secret key. 11. The method of claim 1, wherein the maximum positive and negative time limits is a function of a desired analytic value. 12. The method of claim 10, wherein the secret key comprises a project sub-key specific to a project, and an entity sub-key specific to an entity whose data is being processed. 13. The method of claim 12, wherein the project sub-key comprises a 128-bit randomly-generated universally unique identifier (UUID). 14. The method of claim 1, further comprising shifting the first boundary group, the second boundary group, and the one or more non-boundary groups by a predetermined amount of time. 15. The method of claim 1, wherein the dataset comprises medical data. 16. The method of claim 1, wherein the dataset comprises confidential non-medical data. 17. A system to produce an anonymized cohort that includes members having less than a predetermined risk of re-identification, comprising:
a communication interface to a database of medical data;
a processor coupled to a memory and to the database, the memory storing instructions to be executed by the processor, the instructions causing the processor to perform the steps of:
receiving a data query via a user-facing communication channel to request an anonymized cohort, the data query comprising requested traits to include in the members of the anonymized cohort;
querying a data source having data records describing the members, using a data query transmitted via a data source-facing communication channel, to find the data records that possess at least some of the requested traits;
forming a dataset from at least some of the data records;
grouping the data records in time, by a processor coupled to the user-facing communication channel and the data source-facing communication channel, into a first boundary group, a second boundary group, and one or more non-boundary groups temporally between the first boundary group and second boundary group;
for each non-boundary group within the dataset, performing the steps of:
calculating maximum positive and negative time limits said non-boundary group can be time-shifted without overlapping an adjacent group;
calculating a group jitter amount;
capping the group jitter amount by the maximum positive and negative time limits; and
jittering said non-boundary group by the capped group jitter; and
providing, via the user-facing communication channel, the dataset. 17. A system to produce an anonymized cohort that includes members having less than a predetermined risk of re-identification, comprising:
a communication interface to a database of medical data;
a processor coupled to a memory and to the database, the memory storing instructions to be executed by the processor, the instructions causing the processor to perform the steps of:
receiving a data query via a user-facing communication channel to request an anonymized cohort, the data query comprising requested traits to include in the members of the anonymized cohort;
querying a data source having data records describing the members, using a data query transmitted via a data source-facing communication channel, to find the data records that possess at least some of the requested traits;
forming a dataset from at least some of the data records;
grouping the data records in time, by a processor coupled to the user-facing communication channel and the data source-facing communication channel, into a first boundary group, a second boundary group, and one or more non-boundary groups temporally between the first boundary group and second boundary group;
for each non-boundary group within the dataset, performing the steps of:
calculating maximum positive and negative time limits said non-boundary group can be time-shifted without overlapping an adjacent group;
calculating a group jitter amount;
capping the group jitter amount by the maximum positive and negative time limits; and
jittering said non-boundary group by the capped group jitter; and
providing, via the user-facing communication channel, the dataset. 18. The system of claim 17, wherein calculating a group jitter amount comprises using a hash function. 19. The system of claim 18, wherein the hash function hashes a date value from each respective non-boundary group by use of a secret key, wherein the secret key comprises a project sub-key specific to a project, and an entity sub-key specific to an entity whose data is being processed. 20. The system of claim 17, further comprising shifting the first boundary group, the second boundary group, and the one or more non-boundary groups by a predetermined amount of time.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318764B2,US10318764B2,Method and apparatus for differentiated access control,2010-09-24,"method, launched, second, connection, device, differentiated, enabled, computing, application, plurality, with, checking, apparatus, subsets, being, having, whether, least, control, expired, from, preventing, access, lost, timer","A method for differentiated access control on a computing device having a connection with a second device, the method checking whether a timer has expired on the second device or if a connection is lost to the second device; and preventing at least one of the plurality of application subsets from being launched or enabled if the timer has expired on the second device or the connection is lost to the second device.","1. A method for providing differentiated access control on a computing device having a connection with a second device, the method comprising:
receiving, at the computing device, an input to start an application belonging to a first of a plurality of application subsets, the starting the application comprising executing the application on the computing device;
receiving, at the computing device, a user input to unlock the second device;
verifying the user input with the second device to unlock the first of the plurality of application subsets;
checking whether a timer has expired on the second device or—whether a connection is lost to the second device, and based on expiration of the timer or loss of the connection to the second device, locking the first of the plurality of applications subsets on the computing device, the locking including:
allowing access to applications within at least a second of the plurality of application subsets, including allowing launching and enabling of at least one application within the at least the second of the plurality of application subsets;
preventing each application of the first of the plurality of application subsets from being launched on the computing device while the first of the plurality of applications subsets is locked; and
disabling a graphical user interface associated with each application of the first of the plurality of application subsets


wherein the timer is associated with a countdown timer managed by the second device for decreasing time. 1. A method for providing differentiated access control on a computing device having a connection with a second device, the method comprising:
receiving, at the computing device, an input to start an application belonging to a first of a plurality of application subsets, the starting the application comprising executing the application on the computing device;
receiving, at the computing device, a user input to unlock the second device;
verifying the user input with the second device to unlock the first of the plurality of application subsets;
checking whether a timer has expired on the second device or—whether a connection is lost to the second device, and based on expiration of the timer or loss of the connection to the second device, locking the first of the plurality of applications subsets on the computing device, the locking including:
allowing access to applications within at least a second of the plurality of application subsets, including allowing launching and enabling of at least one application within the at least the second of the plurality of application subsets;
preventing each application of the first of the plurality of application subsets from being launched on the computing device while the first of the plurality of applications subsets is locked; and
disabling a graphical user interface associated with each application of the first of the plurality of application subsets


wherein the timer is associated with a countdown timer managed by the second device for decreasing time. 2. The method of claim 1, wherein each of the plurality of application subsets utilizes a separate file system in memory of the computing device. 3. The method of claim 1, wherein the disabling comprises changing a graphical representation of an application window or icon displayed on the computing device. 4. The method of claim 3, wherein the graphical representation is a grid style and the changing replaces the application window with an obscured or locked graphic. 5. The method of claim 1, wherein the disabling comprises making an application window inaccessible. 6. The method of claim 1, wherein the first of the plurality of application subsets includes applications designated as corporate applications. 7. The method of claim 6, wherein the designating is based on an enterprise information technology policy. 8. The method of claim 6, wherein the designating is done on the computing device. 9. The method of claim 1, further comprising unlocking each application associated with the first of the plurality of applications subsets in response to successful authentication. 10. A computing device comprising:
a processor;
a communications subsystem;
a user interface; and
memory,

wherein the computing device communicates with a second device, and wherein the computing device is configured to:
receive an input to start an application belonging to a first of a plurality of application subsets, the starting the application comprising executing the application on the computing device;
receive a user input to unlock the second device;
verify the user input with the second device to unlock the first of the plurality of application subsets;
determine whether a timer has expired on the second device or whether a connection is lost to the second device, and based on expiration of the timer or loss of the connection to the second device, lock the first of the plurality of applications subsets on the computing device, the locking including:
allowing access to applications within at least a second of the plurality of application subsets, including allowing launching and enabling of at least one application within the at least the second of the plurality of application subsets;
preventing each application of the first of the plurality of application subsets from being launched on the computing device while the first of the plurality of applications subsets is locked; and
disabling a graphical user interface associated with each application of the first of the plurality of application subsets


wherein the timer is associated with a countdown timer managed by the second device, for decreasing time. 10. A computing device comprising:
a processor;
a communications subsystem;
a user interface; and
memory,

wherein the computing device communicates with a second device, and wherein the computing device is configured to:
receive an input to start an application belonging to a first of a plurality of application subsets, the starting the application comprising executing the application on the computing device;
receive a user input to unlock the second device;
verify the user input with the second device to unlock the first of the plurality of application subsets;
determine whether a timer has expired on the second device or whether a connection is lost to the second device, and based on expiration of the timer or loss of the connection to the second device, lock the first of the plurality of applications subsets on the computing device, the locking including:
allowing access to applications within at least a second of the plurality of application subsets, including allowing launching and enabling of at least one application within the at least the second of the plurality of application subsets;
preventing each application of the first of the plurality of application subsets from being launched on the computing device while the first of the plurality of applications subsets is locked; and
disabling a graphical user interface associated with each application of the first of the plurality of application subsets


wherein the timer is associated with a countdown timer managed by the second device, for decreasing time. 11. The computing device of claim 10, wherein each of the plurality of application subsets utilizes a separate file system in memory of the computing device. 12. The computing device of claim 10, wherein the computing device is configured to disable a graphical user interface by changing a graphical representation of an application window or icon displayed on the computing device. 13. The computing device of claim 12, wherein the graphical representation is a grid style and the changing replaces the application window with an obscured or locked graphic. 14. The computing device of claim 10, wherein the computing device is configured to disable a graphical user interface by making an application window inaccessible. 15. The computing device of claim 10, wherein the computing device comprises at least one of a mobile device, smartphone, a tablet computer, a desktop computer, or a laptop computer. 16. A non-transitory computer readable medium for storing instruction code, which, when executed by a processor of a computing device are configured to provide differentiated access control on the computing device having a connection with a second device, the instruction code comprising instructions for:
receiving an input to start an application belonging to a first of a plurality of application subsets, the starting the application comprising executing the application on the computing device;
receiving a user input to unlock the second device;
verifying the user input with the second device to unlock the first of the plurality of application subsets;
checking whether a timer has expired on the second device or whether a connection is lost to the second device, and based on expiration of the timer or loss of the connection to the second device, locking the first of the plurality of applications subsets on the computing device, the locking including:
allowing access to applications within at least a second of the plurality of application subsets, including allowing launching and enabling of at least one application within the at least the second of the plurality of application subsets;
preventing each application of the first of the plurality of application subsets from being launched on the computing device while the first of the plurality of applications subsets is locked; and
disabling a graphical user interface associated with each application of the first of the plurality of application subsets


wherein the timer is associated with a countdown timer managed by the second device, for decreasing time. 16. A non-transitory computer readable medium for storing instruction code, which, when executed by a processor of a computing device are configured to provide differentiated access control on the computing device having a connection with a second device, the instruction code comprising instructions for:
receiving an input to start an application belonging to a first of a plurality of application subsets, the starting the application comprising executing the application on the computing device;
receiving a user input to unlock the second device;
verifying the user input with the second device to unlock the first of the plurality of application subsets;
checking whether a timer has expired on the second device or whether a connection is lost to the second device, and based on expiration of the timer or loss of the connection to the second device, locking the first of the plurality of applications subsets on the computing device, the locking including:
allowing access to applications within at least a second of the plurality of application subsets, including allowing launching and enabling of at least one application within the at least the second of the plurality of application subsets;
preventing each application of the first of the plurality of application subsets from being launched on the computing device while the first of the plurality of applications subsets is locked; and
disabling a graphical user interface associated with each application of the first of the plurality of application subsets


wherein the timer is associated with a countdown timer managed by the second device, for decreasing time. 17. The non-transitory computer readable medium of claim 16, wherein each of the plurality of application subsets utilizes a separate file system in memory of the computing device. 18. The non-transitory computer readable medium of claim 16, wherein the disabling comprises changing a graphical representation of an application window or icon displayed on the computing device. 19. The non-transitory computer readable medium of claim 18, wherein the graphical representation is a grid style and the changing replaces the application window with an obscured or locked graphic. 20. The non-transitory computer readable medium of claim 16, wherein the disabling comprises making an application window inaccessible. 21. The non-transitory computer readable medium of claim 16, wherein the first of the plurality of application subsets includes applications designated as corporate applications. 22. The non-transitory computer readable medium of claim 21, wherein the designating is based on an enterprise information technology policy. 23. The non-transitory computer readable medium of claim 21, wherein the designating is done on the computing device. 24. The non-transitory computer readable medium of claim 16, wherein the instruction code further comprises instructions unlocking each application associated with the first of the plurality of applications subsets in response to successful authentication.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318759B2,US10318759B2,Method and apparatus for enforcing data privacy,2011-02-28,"method, profile, device, local, policies, caused, thereof, maintaining, enforcement, data, privacy, processes, determine, with, combination, request, granting, applications, apparatus, user, platform, determines, more, then, objects, enforcing, information, described, management, associated, approach, from, access","An approach for maintaining user privacy information is described. A privacy management platform determines a request, from one or more applications, for access to local data associated with a device. The platform then determines and processes one or more privacy profile objects associated with the local data to determine one or more privacy policies associated with the local data, the device, or a combination thereof. Enforcement of the one or more privacy policies is then caused for granting access to the local data.","1. A method comprising:
receiving, at a privacy platform, an alert that one or more applications has initiated a request for a sharing of local data stored at a device, wherein the privacy platform is independent of the one or more applications, and wherein the local data includes sensor data collected by one or more sensors of the device;
in response to the alert and prior to the sharing of the local data by the one or more applications, determining one or more privacy profile objects specifying one or more privacy policies for with respect to the sharing of the local data, wherein the one or more privacy profile objects are independent of the one or more applications and are created by a trusted external organization, the one or more privacy profile objects being configured for the user to selectively grant the sharing of the local data by the one or more applications, and wherein the trusted external organization is independent from one or more service providers associated with the one or more applications;
initiating an enforcement of the one or more privacy policies by applying at least one transformation of the local data to generate transformed local data that satisfies the one or more privacy policies, wherein the enforcement of the one or more privacy policies further comprises generating a prompt requesting an approval from a user of the device for granting the sharing of the local data by the one or more applications; and
fulfilling the request for the sharing of the local data by granting the one or more applications a sharing right for the transformed local data in place of the local data. 1. A method comprising:
receiving, at a privacy platform, an alert that one or more applications has initiated a request for a sharing of local data stored at a device, wherein the privacy platform is independent of the one or more applications, and wherein the local data includes sensor data collected by one or more sensors of the device;
in response to the alert and prior to the sharing of the local data by the one or more applications, determining one or more privacy profile objects specifying one or more privacy policies for with respect to the sharing of the local data, wherein the one or more privacy profile objects are independent of the one or more applications and are created by a trusted external organization, the one or more privacy profile objects being configured for the user to selectively grant the sharing of the local data by the one or more applications, and wherein the trusted external organization is independent from one or more service providers associated with the one or more applications;
initiating an enforcement of the one or more privacy policies by applying at least one transformation of the local data to generate transformed local data that satisfies the one or more privacy policies, wherein the enforcement of the one or more privacy policies further comprises generating a prompt requesting an approval from a user of the device for granting the sharing of the local data by the one or more applications; and
fulfilling the request for the sharing of the local data by granting the one or more applications a sharing right for the transformed local data in place of the local data. 2. The method of claim 1, wherein the enforcement of the one or more privacy policies further comprises presenting an indicator representative of the one or more privacy profile objects in a user interface of the device. 3. The method of claim 1, wherein the enforcement of the one or more privacy policies further comprises denying the sharing of the local data by the one or more applications. 4. The method of claim 1, further comprising:
processing the one or more privacy profile objects to determine one or more resources related to the enforcement of the one or more privacy policies. 5. The method of claim 1, wherein the at least one transformation includes transforming the local data from a first level of data granularity to a second level of data granularity of the transformed local data. 6. The method of claim 5, wherein the first level of data granularity and the second level of data granularity are previously stored in at least one source of the local data. 7. The method of claim 1, wherein the at least one transformation of the local data includes decreasing level of accuracy of the local data, replacing partial of the local data with some other data, or/and adjusting granularity level of the local data. 8. The method of claim 1, wherein the privacy policy objects are remotely programmable. 9. An apparatus comprising:
at least one processor; and
at least one memory including computer program code for one or more programs, the at least one memory and the computer program code configured to, with the at least one processor, cause the apparatus to perform at least the following,
receive, at a privacy platform, an alert that one or more applications has initiated a request for a sharing of local data stored at a device, wherein the privacy platform is independent of the one or more applications, and wherein the local data includes sensor data collected by one or more sensors of the device;
in response to the alert and prior to the sharing of the local data by the one or more applications, determine one or more privacy profile objects specifying one or more privacy policies for with respect to the sharing of the local data, wherein the one or more privacy profile objects are independent of the one or more applications and are created by a trusted external organization, the one or more privacy profile objects being configured for the user to selectively grant the sharing of the local data by the one or more applications, and wherein the trusted external organization is independent from one or more service providers associated with the one or more applications;
initiate an enforcement of the one or more privacy policies by applying at least one transformation of the local data to generate transformed local data that satisfies the one or more privacy policies, wherein the enforcement of the one or more privacy policies further comprises generating a prompt requesting an approval from a user of the device for granting the sharing of the local data by the one or more applications; and
fulfill the request for the sharing of the local data by granting the one or more applications a sharing right for the transformed local data in place of the local data. 9. An apparatus comprising:
at least one processor; and
at least one memory including computer program code for one or more programs, the at least one memory and the computer program code configured to, with the at least one processor, cause the apparatus to perform at least the following,
receive, at a privacy platform, an alert that one or more applications has initiated a request for a sharing of local data stored at a device, wherein the privacy platform is independent of the one or more applications, and wherein the local data includes sensor data collected by one or more sensors of the device;
in response to the alert and prior to the sharing of the local data by the one or more applications, determine one or more privacy profile objects specifying one or more privacy policies for with respect to the sharing of the local data, wherein the one or more privacy profile objects are independent of the one or more applications and are created by a trusted external organization, the one or more privacy profile objects being configured for the user to selectively grant the sharing of the local data by the one or more applications, and wherein the trusted external organization is independent from one or more service providers associated with the one or more applications;
initiate an enforcement of the one or more privacy policies by applying at least one transformation of the local data to generate transformed local data that satisfies the one or more privacy policies, wherein the enforcement of the one or more privacy policies further comprises generating a prompt requesting an approval from a user of the device for granting the sharing of the local data by the one or more applications; and
fulfill the request for the sharing of the local data by granting the one or more applications a sharing right for the transformed local data in place of the local data. 10. The apparatus of claim 9, wherein the enforcement of the one or more privacy policies further comprises presenting an indicator representative of the one or more privacy profile objects in a user interface of the device. 11. The apparatus of claim 9, wherein the enforcement of the one or more privacy policies further comprises denying the sharing of the local data by the one or more applications. 12. The apparatus of claim 9, wherein the apparatus is further caused to:
process the one or more privacy profile objects to determine one or more resources related to the enforcement of the one or more privacy policies. 13. The apparatus of claim 9, wherein the at least one transformation includes transforming the local data from a first level of data granularity to a second level of data granularity of the transformed local data. 14. The apparatus of claim 13, wherein the first level of data granularity and the second level of data granularity are previously stored in at least one source of the local data. 15. The apparatus of claim 9, wherein the at least one transformation of the local data includes decreasing level of accuracy of the local data, replacing partial of the local data with some other data, or/and adjusting granularity level of the local data. 16. The apparatus of claim 9, wherein the privacy policy objects are remotely programmable. 17. A non-transitory computer-readable storage medium carrying one or more sequences of one or more instructions which, when executed by one or more processors, cause an apparatus to perform:
receiving, at a privacy platform, an alert that one or more applications has initiated a request for a sharing of local data stored at a device, wherein the privacy platform is independent of the one or more applications, and wherein the local data includes sensor data collected by one or more sensors of the device;
in response to the alert and prior to the sharing of the local data by the one or more applications, determining one or more privacy profile objects specifying one or more privacy policies for with respect to the sharing of the local data, wherein the one or more privacy profile objects are independent of the one or more applications and are created by a trusted external organization, the one or more privacy profile objects being configured for the user to selectively grant the sharing of the local data by the one or more applications, and wherein the trusted external organization is independent from one or more service providers associated with the one or more applications;
initiating an enforcement of the one or more privacy policies by applying at least one transformation of the local data to generate transformed local data that satisfies the one or more privacy policies, wherein the enforcement of the one or more privacy policies further comprises generating a prompt requesting an approval from a user of the device for granting the sharing of the local data by the one or more applications; and
fulfilling the request for the sharing of the local data by granting the one or more applications a sharing right for the transformed local data in place of the local data. 17. A non-transitory computer-readable storage medium carrying one or more sequences of one or more instructions which, when executed by one or more processors, cause an apparatus to perform:
receiving, at a privacy platform, an alert that one or more applications has initiated a request for a sharing of local data stored at a device, wherein the privacy platform is independent of the one or more applications, and wherein the local data includes sensor data collected by one or more sensors of the device;
in response to the alert and prior to the sharing of the local data by the one or more applications, determining one or more privacy profile objects specifying one or more privacy policies for with respect to the sharing of the local data, wherein the one or more privacy profile objects are independent of the one or more applications and are created by a trusted external organization, the one or more privacy profile objects being configured for the user to selectively grant the sharing of the local data by the one or more applications, and wherein the trusted external organization is independent from one or more service providers associated with the one or more applications;
initiating an enforcement of the one or more privacy policies by applying at least one transformation of the local data to generate transformed local data that satisfies the one or more privacy policies, wherein the enforcement of the one or more privacy policies further comprises generating a prompt requesting an approval from a user of the device for granting the sharing of the local data by the one or more applications; and
fulfilling the request for the sharing of the local data by granting the one or more applications a sharing right for the transformed local data in place of the local data. 18. The non-transitory computer-readable storage medium of claim 17, wherein the enforcement of the one or more privacy policies further comprises presenting an indicator representative of the one or more privacy profile objects in a user interface of the device. 19. The non-transitory computer-readable storage medium of claim 17, wherein the enforcement of the one or more privacy policies further comprises denying the sharing of the local data by the one or more applications. 20. The non-transitory computer-readable storage medium of claim 17, wherein the at least one transformation of the local data includes decreasing level of accuracy of the local data, replacing partial of the local data with some other data, or/and adjusting granularity level of the local data.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318766B2,US10318766B2,"Method for the secured recording of data, corresponding device and program",,"method, second, device, implemented, encrypted, said, secured, corresponding, eliminating, program, data, derived, steps, root, obtaining, recorded, representing, using, delivering, memory, content, hash, general, encrypting, determining, having, disclosed, file, recording, from, imprint, comprising, first","A method for the secured recording of data, implemented in a data-recording device having a first non-secured memory and a second secured memory, is disclosed. The method has the steps of: obtaining a derived key corresponding to the data in the second memory from a root key recorded in the second memory; encrypting data using the derived key, delivering encrypted data; recording the encrypted data in the first memory; determining a hash imprint of said data; recording said hash imprint in a hash file recorded in the first memory; recording a general hash imprint, representing the content of the hash file comprising said hash imprint, in the second memory; and eliminating the data in the second memory.","1. A method for the secured recording of data, implemented in a data-recording device comprising a first non-secured memory and a second secured memory, the method comprising:
obtaining a derived key corresponding to the data recorded in the second secured memory from a root key recorded in the second secured memory;
encrypting the data using the derived key, thereby delivering encrypted data;
recording the encrypted data in the first non-secured memory;
determining a hash imprint of said data by applying a hash function to the data recorded in the second memory;
recording said hash imprint in association with the data in a hash file recorded in the first non-secured memory, the hash imprint for verifying the integrity of the data;
determining a general hash imprint, representing the content of the hash file comprising the hash imprint, by applying another hash function to the hash file;
recording the general hash imprint in the second secured memory, the general hash imprint for verifying the integrity of the hash file; and
subsequently to said recording of the encrypted data in the first non-secured memory, eliminating the data from the second secured memory, the data having been previously encrypted. 1. A method for the secured recording of data, implemented in a data-recording device comprising a first non-secured memory and a second secured memory, the method comprising:
obtaining a derived key corresponding to the data recorded in the second secured memory from a root key recorded in the second secured memory;
encrypting the data using the derived key, thereby delivering encrypted data;
recording the encrypted data in the first non-secured memory;
determining a hash imprint of said data by applying a hash function to the data recorded in the second memory;
recording said hash imprint in association with the data in a hash file recorded in the first non-secured memory, the hash imprint for verifying the integrity of the data;
determining a general hash imprint, representing the content of the hash file comprising the hash imprint, by applying another hash function to the hash file;
recording the general hash imprint in the second secured memory, the general hash imprint for verifying the integrity of the hash file; and
subsequently to said recording of the encrypted data in the first non-secured memory, eliminating the data from the second secured memory, the data having been previously encrypted. 2. The method according to claim 1 wherein, after it is obtained, the derived key is recorded in the second secured memory, the method furthermore comprises eliminating the derived key from the second secured memory after the encryption of said data. 3. The method according of claim 1, wherein the root key is generated randomly in the data-recording device. 4. The method according to claim 1 wherein, during the obtaining, the derived key is obtained from:
a user identifier representing a user of the data-recording device; and
a data identifier. 5. The method according to claim 4, wherein, during said recording of the encrypted data, said encrypted data are recorded in the first non-secured memory in the form of an encrypted file to which a file name is assigned comprising the user identifier and the data identifier. 6. The method according to claim 1, the method comprising, after said recording of the hash imprint in the hash file, recording a copy of the hash file in a secured back-up memory of the data-recording device. 7. A method for the secured retrieval of data, implemented in a data-recording device comprising a first non-secured memory and a second secured memory, the method comprising:
determining a general hash imprint of a hash file recorded in the first non-secured memory by applying a hash function to the hash file;
verifying the integrity of the hash file recorded in the first non-secured memory by comparing the general hash imprint determined for the hash file with a recorded general hash imprint recorded in the second secured memory; and

when the hash file is detected as having integrity, upon reception of a request for access to data:
obtaining a derived key corresponding to encrypted data recorded in the first non-secured memory, from a root key recorded in the second secured memory;
decrypting the encrypted data using the obtained derived key so as to retrieve said data;
recording said data in the second secured memory;
determining a hash imprint of said data by applying another hash function to said data recorded in the second memory;
verifying the integrity of the data recorded in the second secured memory by comparing the hash imprint determined for the data with a recorded hash imprint recorded in the hash file in association with said data; and
authorizing access to the data in the second secured memory in response to said access request, only if the data has been determined as having integrity. 7. A method for the secured retrieval of data, implemented in a data-recording device comprising a first non-secured memory and a second secured memory, the method comprising:
determining a general hash imprint of a hash file recorded in the first non-secured memory by applying a hash function to the hash file;
verifying the integrity of the hash file recorded in the first non-secured memory by comparing the general hash imprint determined for the hash file with a recorded general hash imprint recorded in the second secured memory; and

when the hash file is detected as having integrity, upon reception of a request for access to data:
obtaining a derived key corresponding to encrypted data recorded in the first non-secured memory, from a root key recorded in the second secured memory;
decrypting the encrypted data using the obtained derived key so as to retrieve said data;
recording said data in the second secured memory;
determining a hash imprint of said data by applying another hash function to said data recorded in the second memory;
verifying the integrity of the data recorded in the second secured memory by comparing the hash imprint determined for the data with a recorded hash imprint recorded in the hash file in association with said data; and
authorizing access to the data in the second secured memory in response to said access request, only if the data has been determined as having integrity. 8. The method according to claim 7, the method comprising obtaining a user identifier representing a user of the data-recording device and a data identifier, in which the derived key is obtained from the root key by using the user identifier and the data identifier. 9. The method according to claim 7, the method comprising, subsequently to an access to the data:
encrypting data using the derived key, delivering encrypted data;
recording the encrypted data in the first non-secured memory;
determining the hash imprint of said data;
recording said hash imprint in association with the data in the hash file recorded in the first non-secured memory;
recording the general hash imprint, representing the content of the hash file comprising said hash imprint, in the second secured memory; and
subsequently to said recording of the encrypted data in the first non-secured memory, eliminating the data that has been encrypted from the second secured memory. 10. The method according to claim 7, the method further comprising after an access by the user to the data in the second secured memory:
determining a second hash imprint of said data following said access; and
comparing said second hash imprint with the hash imprint recorded in the hash file in association with the data, in order to detect whether the data have been modified during said access;
wherein, only if it has been detected that the data have been modified during said access:
encrypting data using the derived key, delivering encrypted data;
recording the encrypted data in the first non-secured memory;
determining the hash imprint of said data;
recording said hash imprint in association with the data in the hash file recorded in the first non-secured memory;
recording the general hash imprint, representing the content of the hash file comprising said hash imprint, in the second secured memory; and
subsequently to said recording of the encrypted data in the first non-secured memory, eliminating the data that has been encrypted from the second secured memory. 11. A data-recording device comprising:
a first non-secured memory;
a second secured memory; and
a processor configured to:
obtain a derived key corresponding to data recorded in the second secured memory from a root key recorded in the second secured memory;
encrypt, using the derived key, said data so as to deliver encrypted data;
record the encrypted data in the first non-secured memory;
determine a hash imprint of said data by applying a hash function to the data recorded in the second secured memory;
record said hash imprint, in association with the data, in a hash file recorded in the first non-secured memory, the hash imprint for verification of the integrity of the data;
determine a general hash imprint, representing the content of the hash file comprising the hash imprint, by applying another hash function to the hash file;
record, in the second secured memory, the general hash imprint, the general hash imprint for verification of the integrity of the hash file; and
after said recording of the encrypted data in the first non-secured memory, eliminate the data from the second secured memory, the data having been previously encrypted. 11. A data-recording device comprising:
a first non-secured memory;
a second secured memory; and
a processor configured to:
obtain a derived key corresponding to data recorded in the second secured memory from a root key recorded in the second secured memory;
encrypt, using the derived key, said data so as to deliver encrypted data;
record the encrypted data in the first non-secured memory;
determine a hash imprint of said data by applying a hash function to the data recorded in the second secured memory;
record said hash imprint, in association with the data, in a hash file recorded in the first non-secured memory, the hash imprint for verification of the integrity of the data;
determine a general hash imprint, representing the content of the hash file comprising the hash imprint, by applying another hash function to the hash file;
record, in the second secured memory, the general hash imprint, the general hash imprint for verification of the integrity of the hash file; and
after said recording of the encrypted data in the first non-secured memory, eliminate the data from the second secured memory, the data having been previously encrypted. 12. The data-recording device according to claim 11, wherein the processor is further configured to:
record, in the second secured memory, said derived key obtained from the root key; and
eliminate the derived key recorded in the second secured memory after the encryption of said data.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318765B2,US10318765B2,Protecting critical data structures in an embedded hypervisor system,2014-05-02,"protecting, method, resource, hardware, hypervisor, during, rights, decrypted, monitor, device, encrypted, embedded, computing, structures, component, security, systems, data, continuously, critical, determine, stored, prior, with, where, when, execution, using, real, operating, that, being, memory, transaction, filter, time, chip, execute, processor, then, securing, uses, least, executes, uploaded, authenticates, configures, associated, system, secure, access, executed","A system and method for securing a hypervisor and operating systems that execute on a computing device. An encrypted hypervisor is uploaded to a hardware chip. Prior to being executed, the hypervisor is decrypted using a secure security processor and stored in an on-chip memory. When a processor on the hardware chip executes the hypervisor, at least one on-chip component continuously authenticates the hypervisor during execution. A hypervisor configures a processor with access rights associated with an operating system, where the access rights determine access of the operating system to an at least one resource. A transaction filter then uses the access rights associated with the operating system to monitor the access of the operating system to the at least one resource in real-time as the operating system executes on a processor.","1. A system on a chip, comprising:
a security processor configured to receive access rights associated with an operating system from an off-chip memory;
a hypervisor configured to:
install the access rights associated with the operating system on a processor executing on the chip, wherein the access rights associated with the operating system determine access of the operating system to at least one resource, and
execute hypervisor specific code as the operating system executes on the processor, the hypervisor specific code including configuring access rights within a hypervisor page table of the off-chip memory according to the access rights of the operating system;

an on-chip memory configured to store the hypervisor, the hypervisor being associated with a digital signature when stored in the on-chip memory;
a transaction filter configured to prevent, using the access rights of the operating system, unauthorized access of the operating system to the least one resource in real-time as the operating system executes on the processor;
a write blocker configured to utilize the digital signature to differentiate between the hypervisor and other components executing on the chip to ensure only the hypervisor modifies the hypervisor page table as the operating system executes on the processor;
a background hardware checker configured to verify a code digest value associated with the hypervisor specific code matches the digital signature associated with the hypervisor as the operating system executes on the processor to verify the hypervisor is executing the hypervisor specific code; and
an instruction checker configured to verify the hypervisor executes the hypervisor specific code from a specific range of addresses in the on-chip memory. 1. A system on a chip, comprising:
a security processor configured to receive access rights associated with an operating system from an off-chip memory;
a hypervisor configured to:
install the access rights associated with the operating system on a processor executing on the chip, wherein the access rights associated with the operating system determine access of the operating system to at least one resource, and
execute hypervisor specific code as the operating system executes on the processor, the hypervisor specific code including configuring access rights within a hypervisor page table of the off-chip memory according to the access rights of the operating system;

an on-chip memory configured to store the hypervisor, the hypervisor being associated with a digital signature when stored in the on-chip memory;
a transaction filter configured to prevent, using the access rights of the operating system, unauthorized access of the operating system to the least one resource in real-time as the operating system executes on the processor;
a write blocker configured to utilize the digital signature to differentiate between the hypervisor and other components executing on the chip to ensure only the hypervisor modifies the hypervisor page table as the operating system executes on the processor;
a background hardware checker configured to verify a code digest value associated with the hypervisor specific code matches the digital signature associated with the hypervisor as the operating system executes on the processor to verify the hypervisor is executing the hypervisor specific code; and
an instruction checker configured to verify the hypervisor executes the hypervisor specific code from a specific range of addresses in the on-chip memory. 2. The system on the chip of claim 1, wherein the security processor is further configured to:
decrypt the access rights of the operating system; and
store the access rights of the operating system in the hypervisor and the transaction filter. 3. The system on the chip of claim 2, wherein the security processor is further configured to decrypt the access rights of the operating system prior to storing the access rights of the operating system in the hypervisor and the transaction filter. 4. The system on the chip of claim 2, wherein the access rights of the operating system are encrypted using the digital signature when the access rights of the operating system are stored in the off-chip memory. 5. The system on the chip of claim 1, wherein the hypervisor is further configured to load the operating system into the processor,
wherein the system further comprises an operating system register configured to store an operating system identifier of the operating system, and
wherein the hypervisor is further configured to store the operating system identifier in the operating system register prior to the processor executing the operating system. 6. The system on the chip of claim 5, wherein the transaction filter is further configured to:
retrieve the operating system identifier from the operating system register; and
monitor, using the operating system identifier in combination with the access rights of the operating system, the access of the operating system to the at least one resource as the operating system executes on the processor. 7. The system on the chip of claim 1, wherein the at least one resource includes a hypervisor accessible memory table located on the off-chip memory. 8. The system on the chip of claim 1, wherein the transaction filter is further configured to monitor instructions transmitted by the operating system to a bus coupled to the at least one resource. 9. The system on the chip of claim 8, wherein the transaction filter is further configured to block access of the operating system to the at least one resource when the monitoring of the instructions determines the operating system does not have access to the at least one resource. 10. The system on the chip of claim 1, wherein the processor is further configured to store an operating system identifier of the operating system; and
wherein the transaction filter is further configured to:
retrieve the operating system identifier from the processor; and
monitor, using the operating system identifier in combination with the access rights of the operating system, the access of the operating system to the at least one resource as the operating system executes on the processor. 11. A system on a chip, comprising:
an off-chip memory that stores a hypervisor prior to the hypervisor being uploaded to the chip;
an on-chip memory configured to store the hypervisor, the hypervisor being associated with a digital signature when stored in the on-chip memory;
a processor configured to execute hypervisor specific code of the hypervisor stored in the on-chip memory as an operating system executes on the processor, the hypervisor specific code including configuring access rights within a hypervisor page table of the off-chip memory according to access rights of the operating system;
a transaction filter configured to prevent, using the access rights of the operating system, unauthorized access of the operating system to the least one resource in real-time as the operating system executes on the processor;
a write blocker configured to utilize the digital signature to differentiate between the hypervisor and other components executing on the chip to ensure only the hypervisor modifies the hypervisor page table as the operating system executes on the processor;
a background hardware checker configured to verify a code digest value associated with the hypervisor specific code matches the digital signature associated with the hypervisor as the operating system executes on the processor to verify the hypervisor is executing the hypervisor specific code; and
an instruction checker configured to verify the hypervisor executes the hypervisor specific code from a specific range of addresses in the on-chip memory. 11. A system on a chip, comprising:
an off-chip memory that stores a hypervisor prior to the hypervisor being uploaded to the chip;
an on-chip memory configured to store the hypervisor, the hypervisor being associated with a digital signature when stored in the on-chip memory;
a processor configured to execute hypervisor specific code of the hypervisor stored in the on-chip memory as an operating system executes on the processor, the hypervisor specific code including configuring access rights within a hypervisor page table of the off-chip memory according to access rights of the operating system;
a transaction filter configured to prevent, using the access rights of the operating system, unauthorized access of the operating system to the least one resource in real-time as the operating system executes on the processor;
a write blocker configured to utilize the digital signature to differentiate between the hypervisor and other components executing on the chip to ensure only the hypervisor modifies the hypervisor page table as the operating system executes on the processor;
a background hardware checker configured to verify a code digest value associated with the hypervisor specific code matches the digital signature associated with the hypervisor as the operating system executes on the processor to verify the hypervisor is executing the hypervisor specific code; and
an instruction checker configured to verify the hypervisor executes the hypervisor specific code from a specific range of addresses in the on-chip memory. 12. The system on the chip of claim 11, wherein the hypervisor is stored in the off-chip memory in an encrypted form. 13. The system on the chip of claim 12, further comprising a security processor configured to authenticate the hypervisor stored in the encrypted form in the off-chip memory before the hypervisor is stored in the on-chip memory. 14. The system on the chip of claim 11, wherein the transaction filter is further configured to monitor instructions transmitted by the operating system to a bus coupled to the at least one resource. 15. The system on the chip of claim 14, wherein the transaction filter is further configured to block access of the operating system to the at least one resource when the monitoring of the instructions determines the operating system does not have access to the at least one resource. 16. The system on the chip of claim 11, wherein the processor is further configured to store an operating system identifier of the operating system; and
wherein the transaction filter is further configured to:
retrieve the operating system identifier from the processor; and
monitor, using the operating system identifier in combination with the access rights of the operating system, the access of the operating system to the at least one resource as the operating system executes on the processor. 17. A method, comprising:
installing access rights associated with an operating system on a processor executing on a chip, wherein the access rights associated with the operating system determine access of the operating system to at least one resource;
uploading a hypervisor to the chip from an off-chip memory storage, wherein the hypervisor is encrypted prior to being uploaded to the chip;
decrypting the hypervisor using a security processor;
based on the decrypting, storing the hypervisor in an on-chip memory storage prior to executing the hypervisor;
executing hypervisor specific code of the hypervisor from the on-chip memory storage as the operating system executes on the processor, the hypervisor specific code including configuring access rights within a hypervisor page table of the off-chip memory storage according to the access rights of the operating system;
preventing, using the access rights of the operating system, unauthorized access of the operating system to the least one resource in real-time as the operating system executes on the processor;
verifying a code digest value associated with the hypervisor specific code matches the digital signature associated with the hypervisor as the operating system executes on the processor to verify the hypervisor is executing the hypervisor specific code; and
verifying the hypervisor executes the hypervisor specific code from a specific range of addresses in the on-chip memory storage. 17. A method, comprising:
installing access rights associated with an operating system on a processor executing on a chip, wherein the access rights associated with the operating system determine access of the operating system to at least one resource;
uploading a hypervisor to the chip from an off-chip memory storage, wherein the hypervisor is encrypted prior to being uploaded to the chip;
decrypting the hypervisor using a security processor;
based on the decrypting, storing the hypervisor in an on-chip memory storage prior to executing the hypervisor;
executing hypervisor specific code of the hypervisor from the on-chip memory storage as the operating system executes on the processor, the hypervisor specific code including configuring access rights within a hypervisor page table of the off-chip memory storage according to the access rights of the operating system;
preventing, using the access rights of the operating system, unauthorized access of the operating system to the least one resource in real-time as the operating system executes on the processor;
verifying a code digest value associated with the hypervisor specific code matches the digital signature associated with the hypervisor as the operating system executes on the processor to verify the hypervisor is executing the hypervisor specific code; and
verifying the hypervisor executes the hypervisor specific code from a specific range of addresses in the on-chip memory storage.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318767B2,US10318767B2,Multi-tier security framework,2014-12-10,"created, multiple, device, framework, embedded, security, hierarchy, applied, application, computer, with, execution, combined, multi, tenant, tier, memory, processing, also, storage, server, software, described, layers, levels, system, tiers","A security framework for a multi-tenant, multi-tier computer system with embedded processing is described. A multi-tenant security framework is created for a combined processing and storage hierarchy of multiple tiers. The multi-tenant security framework is applied to multiple execution levels of the memory device. The multi-tenant security framework is applied to multiple layers of application server software of the memory device. The multi-tenant security framework is also applied to multiple layers of storage server software of the memory device.","1. A computer system, comprising:
A first tier, a second tier, and a third tier, wherein:
the first tier comprises a plurality of applications running on a processor, wherein the processor is the primary computing function of the first tier;
the second tier comprises a persistent memory array controller, wherein the persistent memory array controller comprises a processor and a storage system, wherein the storage system includes code to implement a security framework at the second tier, and wherein the security framework provides independent permissions to the plurality of applications; and
the third tier comprises a persistent memory device controller, wherein the persistent memory device controller comprises a processor and a storage system and comprises a nonvolatile memory device, wherein the storage system includes code to implement the security framework at the third tier, and wherein the security framework provides independent permissions to the plurality of applications. 1. A computer system, comprising:
A first tier, a second tier, and a third tier, wherein:
the first tier comprises a plurality of applications running on a processor, wherein the processor is the primary computing function of the first tier;
the second tier comprises a persistent memory array controller, wherein the persistent memory array controller comprises a processor and a storage system, wherein the storage system includes code to implement a security framework at the second tier, and wherein the security framework provides independent permissions to the plurality of applications; and
the third tier comprises a persistent memory device controller, wherein the persistent memory device controller comprises a processor and a storage system and comprises a nonvolatile memory device, wherein the storage system includes code to implement the security framework at the third tier, and wherein the security framework provides independent permissions to the plurality of applications. 2. The computer system of claim 1, wherein the security framework provides multi-tenant protection between the plurality of applications. 3. The computer system of claim 1, wherein the security framework provides multi-tier protection between the first tier, the second tier, and the third tier. 4. The computer system of claim 1, wherein the persistent memory array controller further comprises storage functions, application functions, and management functions. 5. The computer system of claim 4, wherein access to the storage functions, application functions, and management functions by the plurality of applications is controlled by the security framework. 6. The computer system of claim 1, wherein the persistent memory device controller further comprises storage functions, application functions, and management functions. 7. The computer system of claim 6, wherein access to the storage functions, application functions, and management functions by the plurality of applications is controlled by the security framework. 8. The computer system of claim 1, wherein access to nonvolatile memory devices by the plurality of applications is controlled by the security framework. 9. The computer system of claim 1, wherein the security framework allows access of a first application of the plurality of applications to storage and memory on the second tier and the third tier, and wherein the security framework denies access of a second application of the plurality of applications to storage and memory on the second tier and the third tier. 10. The computer system of claim 1, wherein a protection policy is defined for each of the plurality of applications, and wherein the protection policy includes a protection domain, an application ID, a permission level, and a communication pathway are defined for each of the plurality of applications. 11. A method to provide security on a memory device with embedded processing, comprising:
creating a multi-tenant security framework for a combined processing and storage hierarchy of multiple tiers;
applying the multi-tenant security framework to multiple execution levels of the memory device;
applying the multi-tenant security framework to multiple layers of application server software of the memory device; and
applying the multi-tenant security framework to multiple layers of storage server software of the memory device. 11. A method to provide security on a memory device with embedded processing, comprising:
creating a multi-tenant security framework for a combined processing and storage hierarchy of multiple tiers;
applying the multi-tenant security framework to multiple execution levels of the memory device;
applying the multi-tenant security framework to multiple layers of application server software of the memory device; and
applying the multi-tenant security framework to multiple layers of storage server software of the memory device. 12. The method of claim 11, wherein the multi-tenant security framework is to acknowledge whether an execution level is permitted to access and modify data objects outside the bounds of the execution level. 13. The method of claim 11, wherein the multi-tenant security framework is controlling access to data by creating a trusted zone of functionality for the multiple execution levels and for hardware levels in the memory device. 14. A method to implement a security framework for multiple tenants across multiple tiers with independent computing resources in a computer system, comprising:
acknowledging when a tier of the plurality of tiers in a computing and storage hierarchy has computing resources capable of creating, accessing, or modifying data objects both within the tier, and within any other tier of the plurality of tiers;
defining a multi-tenant security framework all at tiers of the plurality of tiers;
creating a trusted zone of functionality for a programmable core located on each tier of the plurality of tiers, wherein the trusted zone of functionality is validated before access to data is allowed;
defining a protection domain ID for a transaction on fabric between a tier of the plurality of tiers on the computer device with embedded processing; and
maintaining data read and write capabilities across shared hardware and between multiple tenants on the computer device with embedded processing. 14. A method to implement a security framework for multiple tenants across multiple tiers with independent computing resources in a computer system, comprising:
acknowledging when a tier of the plurality of tiers in a computing and storage hierarchy has computing resources capable of creating, accessing, or modifying data objects both within the tier, and within any other tier of the plurality of tiers;
defining a multi-tenant security framework all at tiers of the plurality of tiers;
creating a trusted zone of functionality for a programmable core located on each tier of the plurality of tiers, wherein the trusted zone of functionality is validated before access to data is allowed;
defining a protection domain ID for a transaction on fabric between a tier of the plurality of tiers on the computer device with embedded processing; and
maintaining data read and write capabilities across shared hardware and between multiple tenants on the computer device with embedded processing. 15. The method of claim 14, further comprising defining a protection policy for each of the multiple tenants across multiple tiers, and wherein the protection policy includes a protection domain, an application ID, a permission level, and a communication pathway defined for the multiple tenants across multiple tiers.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318770B2,US10318770B2,RFID error detection systems and methods,2017-05-19,"criterion, reading, multiple, configured, previous, error, within, criteria, corresponding, systems, improve, retail, identify, evaluation, herein, antennas, each, with, when, sets, thresholds, methods, number, that, exclusion, exclude, provided, some, database, comprise, detection, rules, current, embodiments, provide, control, associated, threshold, access, apply, circuit, rfid, read","In some embodiments, systems and methods are provided herein to improve RFID tag reading. Some embodiments provide retail RFID tag exclusion systems that comprise: a set of RFID antennas; an RFID criteria database of sets of RFID tag read criteria; and an RFID evaluation control circuit configured to: access a set of current tag read criteria associated with an RFID tag; access multiple sets of previous tag read criteria; access and apply a set of tag evaluation rules and identify when a threshold number of the set of current tag read criteria are each within corresponding criteria thresholds of a corresponding previous tag read criterion of a previous set of tag read criteria; and exclude the RFID tag when the threshold number of the current tag read criteria are within the corresponding criteria thresholds of the corresponding previous tag read criterion of the set of previous tag read criteria.","1. A retail RFID tag exclusion system, comprising:
a set of at least one RFID antennas positioned proximate a limited area within a retail shopping facility and configured to detect RFID tags within the limited area;
an RFID criteria database receiving and storing sets of RFID tag read criteria; and
an RFID evaluation control circuit implementing code stored on a memory and configured to:
access a first set of current tag read criteria associated with a current reading of a first RFID tag by the set of at least one RFID antennas;
access, through the RFID criteria database, multiple sets of previous tag read criteria;
access a set of at least one tag evaluation rules to evaluate detected RFID tags;
apply the set of tag evaluation rules and identify when a threshold number of the first set of current tag read criteria are each within corresponding criteria thresholds of a corresponding previous tag read criterion of a second set of previous tag read criteria; and
exclude the first RFID tag when the threshold number of the first set of current tag read criteria are within the corresponding criteria thresholds of the corresponding previous tag read criterion of the second set of previous tag read criteria. 1. A retail RFID tag exclusion system, comprising:
a set of at least one RFID antennas positioned proximate a limited area within a retail shopping facility and configured to detect RFID tags within the limited area;
an RFID criteria database receiving and storing sets of RFID tag read criteria; and
an RFID evaluation control circuit implementing code stored on a memory and configured to:
access a first set of current tag read criteria associated with a current reading of a first RFID tag by the set of at least one RFID antennas;
access, through the RFID criteria database, multiple sets of previous tag read criteria;
access a set of at least one tag evaluation rules to evaluate detected RFID tags;
apply the set of tag evaluation rules and identify when a threshold number of the first set of current tag read criteria are each within corresponding criteria thresholds of a corresponding previous tag read criterion of a second set of previous tag read criteria; and
exclude the first RFID tag when the threshold number of the first set of current tag read criteria are within the corresponding criteria thresholds of the corresponding previous tag read criterion of the second set of previous tag read criteria. 2. The system of claim 1, wherein the RFID evaluation control circuit is configured to:
identify a first set of items associated with a first person;
cause a listing of items comprising the first set of items to be presented to the first person;
receive a notification specified by the first person that a first item should not be included in the listing; and
cause a corresponding third set of tag read criteria associated with the first item to be added to the RFID criteria database. 3. The system of claim 2, wherein the RFID evaluation control circuit in causing the listing of items to be presented to the first person is configured to cause a point-of-sale system to render the listing through a graphical user interface displayed on a display associated with the point-of-sale system. 4. The system of claim 2, wherein the RFID evaluation control circuit is configured to identify when the first item is identified by a threshold number of persons as not to be included in the respective listings associated with the persons, and cause an audit notification to be generated directing an audit of a first product corresponding to the first item. 5. The system of claim 1, wherein the RFID evaluation control circuit is configured to:
identify when a threshold number of a third set of previous tag read criteria have each been within corresponding criteria thresholds of a threshold number of other sets of tag read criteria; and
designate the third set of previous tag read criteria as an erroneous set of tag reads to be used to exclude subsequent sets of tag read criteria that have the threshold number of tag read criteria that each are within the corresponding criteria thresholds of the third set of previous tag reads. 6. The system of claim 1, wherein the RFID evaluation control circuit is configured to:
identify a first set of items associated with a first customer based on an RFID tag read for each of the items of the first set of items, including the first RFID tag, and intended to be purchased by the first customer;
wherein the RFID evaluation control circuit in excluding the first RFID tag is configured to autonomously cause the first item to be excluded by a point-of-sale system from a purchase set of items to be purchased by the first customer. 7. The system of claim 1, wherein the RFID evaluation control circuit is configured to:
identify, based on an audit performed at the retail shopping facility of items of a product associated with the first RFID tag, that a first item exclusively associated with a third set of previous tag read criteria is no longer present at the shopping facility; and
cause the third set of previous tag read criteria associated with the first item to be removed from the RFID criteria database. 8. The system of claim 1, wherein the RFID evaluation control circuit is configured to:
identify that a threshold period of time has passed since a third set of previous tag read criteria was detected relative to the limited area; and
cause the third set of previous tag read criteria to be removed from the RFID criteria database. 9. The system of claim 1, wherein the set of current tag read criteria does not include an identifier of the first RFID tag. 10. A method of excluding erroneous retail RFID tags, comprising:
accessing a first set of current tag read criteria associated with a current reading of a first RFID tag by a set of at least one RFID antennas;
accessing, through an RFID criteria database, multiple sets of previous tag read criteria;
accessing a set of at least one tag evaluation rules to evaluate detected RFID tags;
applying the set of tag evaluation rules and identifying when a threshold number of the first set of current tag read criteria are each within corresponding criteria thresholds of a corresponding previous tag read criterion of a second set of previous tag read criteria; and
excluding the first RFID tag when the threshold number of the first set of current tag read criteria are within the corresponding criteria thresholds of the corresponding previous tag read criterion of the second set of previous tag read criteria. 10. A method of excluding erroneous retail RFID tags, comprising:
accessing a first set of current tag read criteria associated with a current reading of a first RFID tag by a set of at least one RFID antennas;
accessing, through an RFID criteria database, multiple sets of previous tag read criteria;
accessing a set of at least one tag evaluation rules to evaluate detected RFID tags;
applying the set of tag evaluation rules and identifying when a threshold number of the first set of current tag read criteria are each within corresponding criteria thresholds of a corresponding previous tag read criterion of a second set of previous tag read criteria; and
excluding the first RFID tag when the threshold number of the first set of current tag read criteria are within the corresponding criteria thresholds of the corresponding previous tag read criterion of the second set of previous tag read criteria. 11. The method of claim 10, further comprising:
identifying a first set of items associated with a first person;
causing a listing of items comprising the first set of items to be presented to the first person;
receiving a notification specified by the first person that a first item should not be included in the listing; and
causing a corresponding third set of tag read criteria associated with the first item to be added to the RFID criteria database. 12. The method of claim 11, wherein the causing the listing of items to be presented to the first person comprises causing a point-of-sale system to render the listing through a graphical user interface displayed on a display associated with the point-of-sale system. 13. The method of claim 11, further comprising:
identifying when the first item is identified by a threshold number of persons as not to be included in the respective listings associated with the persons; and
causing an audit notification to be generated directing an audit of a first product corresponding to the first item. 14. The method of claim 10, further comprising:
identifying when a threshold number of a third set of previous tag read criteria have each been within corresponding criteria thresholds of a threshold number of other sets of tag read criteria; and
designating the third set of previous tag read criteria as an erroneous set of tag reads to be used to exclude subsequent sets of tag read criteria that have the threshold number of tag read criteria that each are within the corresponding criteria thresholds of the third set of previous tag reads. 15. The method of claim 10, further comprising:
identifying a first set of items associated with a first customer based on an RFID tag read for each of the items of the first set of items, including the first RFID tag, and intended to be purchased by the first customer;
wherein the excluding the first RFID tag comprises autonomously causing the first item to be excluded by a point-of-sale system from a purchase set of items to be purchased by the first customer. 16. The method of claim 10, further comprising:
identifying, based on an audit performed at the retail shopping facility of items of a product associated with the first RFID tag, that a first item exclusively associated with a third set of previous tag read criteria is no longer present at the shopping facility; and
causing the third set of previous tag read criteria associated with the first item to be removed from the RFID criteria database. 17. The method of claim 10, further comprising:
identifying that a threshold period of time has passed since a third set of previous tag read criteria was detected relative to an area; and
causing the third set of previous tag read criteria to be removed from the RFID criteria database. 18. The method of claim 10, wherein the set of current tag read criteria does not include an identifier of the first RFID tag.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318769B2,US10318769B2,Wireless tag apparatus and related methods,2013-06-26,"housing, assets, positioned, within, transmitted, asset, wherein, related, response, accelerometer, wake, with, transmitter, wireless, methods, apparatus, includes, received, communication, provided, externally, processor, transmits, signal, activation, from, monitoring, located","An asset tag apparatus and methods of monitoring assets with an asset tag are provided. The asset tag apparatus includes a housing and a wireless transmitter located within the housing. A processor is located within the housing, wherein the processor is in communication with the wireless transmitter. An accelerometer is positioned within the housing, wherein the accelerometer is in communication with the processor, wherein a wake-up signal is transmitted from the accelerometer to the processor in response to an activation of the accelerometer, and wherein the wireless transmitter transmits a signal externally from the housing in response to the wake-up signal received by the processor.","1. A wireless tag apparatus comprising:
a wireless receiver configured to receive a first radio frequency (RF) signal of a first frequency range, the first RF signal including data pertaining to an identity of a remote source of the first RF signal; and
a wireless transmitter configured to transmit a second RF signal of a second frequency range that differs from the first frequency range, the second RF signal including the data pertaining to the identity of the remote source of the first RF signal, wherein the wireless transmitter is configured to transmit the second RF signal:
at a first transmission rate when the wireless tag apparatus is in a low-power state; and
at a second transmission rate when the wireless tag apparatus is in an active state, wherein the second transmission rate is greater than the first transmission rate;

wherein the wireless tag apparatus is configured to be paired with an asset of interest such that the asset of interest is able to be wirelessly tracked utilizing a computing device external to and in wireless communication with the wireless tag apparatus. 1. A wireless tag apparatus comprising:
a wireless receiver configured to receive a first radio frequency (RF) signal of a first frequency range, the first RF signal including data pertaining to an identity of a remote source of the first RF signal; and
a wireless transmitter configured to transmit a second RF signal of a second frequency range that differs from the first frequency range, the second RF signal including the data pertaining to the identity of the remote source of the first RF signal, wherein the wireless transmitter is configured to transmit the second RF signal:
at a first transmission rate when the wireless tag apparatus is in a low-power state; and
at a second transmission rate when the wireless tag apparatus is in an active state, wherein the second transmission rate is greater than the first transmission rate;

wherein the wireless tag apparatus is configured to be paired with an asset of interest such that the asset of interest is able to be wirelessly tracked utilizing a computing device external to and in wireless communication with the wireless tag apparatus. 2. The wireless tag apparatus of claim 1, wherein the first RF signal is at least one of a Wi-Fi signal and a Bluetooth signal. 3. The wireless tag apparatus of claim 2, wherein the second RF signal is a Bluetooth signal. 4. The wireless tag apparatus of claim 3, wherein the second frequency range is in an ISM band of between 2.4-2.485 GHz. 5. The wireless tag apparatus of claim 4, wherein the first frequency range is in a 915 MHz ISM band. 6. The wireless tag apparatus of claim 4, wherein the second RF signal is encoded utilizing a Bluetooth Low Energy (BLE) communication protocol. 7. The wireless tag apparatus of claim 1, wherein the wireless receiver is configured to scan for the first RF signal for a channel scan time that is greater than a transmission period of the first RF signal. 8. The wireless tag apparatus of claim 1, wherein the second RF signal further includes data pertaining to at least one of:
a unique tag address associated with the wireless tag apparatus;
a manufacture code associated with the wireless tag apparatus;
a status of the wireless tag apparatus;
a power level of a power supply of the wireless tag apparatus;
a power level of a power supply of the remote source of the first RF signal; and
an output of at least one sensor of the wireless tag apparatus. 9. The wireless tag apparatus of claim 1, wherein the data pertaining to the identity of the remote source of the first signal includes a micro-zone identification code. 10. The wireless tag apparatus of claim 1, wherein the wireless transmitter is configured to transmit the second RF signal periodically. 11. The wireless tag apparatus of claim 10, further comprising a timer configured to periodically output a signal that results in transmission of the second RF signal periodically by the wireless transmitter. 12. The wireless tag apparatus of claim 11, wherein the timer is native to a processing element of the wireless tag apparatus. 13. The wireless tag apparatus of claim 1, wherein the wireless transmitter is configured to transmit the second RF signal at the second transmission rate after detection of at least one of:
a movement of the wireless tag apparatus; and
an impact to the wireless tag apparatus. 14. The wireless tag apparatus of claim 13, further comprising at least one sensor configured to detect an orientation of the wireless tag apparatus and the at least one of:
the movement of the wireless tag apparatus; and
the impact to the wireless tag apparatus. 15. The wireless tag apparatus of claim 14, wherein upon detecting the impact while the wireless tag apparatus is oriented in a first orientation, the wireless tag apparatus transitions from the low-power state to the active state, in which active state the wireless tag apparatus is permitted to wirelessly communicate with the external computing device. 16. The wireless tag apparatus of claim 15, wherein in the active state, the wireless tag apparatus enters a pairing mode through which the wireless tag apparatus wirelessly communicates with the external computing device to effectuate pairing of the wireless tag apparatus with the asset of interest. 17. The wireless tag apparatus of claim 16, wherein the impact comprises at least one tap on a housing of the wireless tag apparatus. 18. The wireless tag apparatus of claim 15, wherein upon detecting the impact while the wireless tag apparatus is oriented in a second orientation that differs from the first orientation, the wireless tag apparatus transitions from the active state to the low-power state. 19. The wireless tag apparatus of claim 1, wherein the wireless transmitter is configured to transmit the second RF signal at the second transmission rate after actuation of a button of the wireless tag apparatus. 20. The wireless tag apparatus of claim 1, further comprising at least one of:
a moisture sensor;
a humidity sensor;
a temperature sensor;
a proximity sensor;
a Near Field Communications (NFC) reader;
a Radio Frequency Identification (RFID) reader; and
a magnetic field sensor. 21. The wireless tag apparatus of claim 1, wherein the first RF signal includes data that, when received by the wireless tag apparatus, at least one of:
programs at least one setting of the wireless tag apparatus;
causes the wireless transmitter to transmit the second RF signal at the second transmission rate;
causes an alert code to be generated by the wireless tag apparatus; and
causes an audio output device of the wireless tag apparatus to emit a sound.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318768B2,US10318768B2,Program execution device,2003-08-26,"protecting, second, device, protection, capable, while, program, protects, disconnects, with, alteration, execution, external, analysis, includes, that, unit, provided, against, connected, executing, executes, unauthorized, from, controlling, first","A program execution device capable of protecting a program against unauthorized analysis and alteration is provided. The program execution device includes an execution unit, a first protection unit, and a second protection unit. The execution unit executes a first program and a second program, and is connected with an external device that is capable of controlling the execution. The first protection unit disconnects the execution unit from the external device while the execution unit is executing the first program. The second protection unit protects the first program while the execution unit is executing the second program.","1. An information processing device comprising:
a processor configured to 1) execute a first program component for operating in a secure processing mode and performing tamper detection, and 2) execute a second program component for operating in a normal processing mode and performing a task; and
a hardware memory configured to store at least the second program component,
wherein the processor is configured to execute the first program component for determining whether at least part of the second program component is tampered with, by using a tamper detection value and while operating in the secure processing mode, and
the first program component operated in the secure processing mode cannot be accessed from the second program component operated in the normal processing mode. 1. An information processing device comprising:
a processor configured to 1) execute a first program component for operating in a secure processing mode and performing tamper detection, and 2) execute a second program component for operating in a normal processing mode and performing a task; and
a hardware memory configured to store at least the second program component,
wherein the processor is configured to execute the first program component for determining whether at least part of the second program component is tampered with, by using a tamper detection value and while operating in the secure processing mode, and
the first program component operated in the secure processing mode cannot be accessed from the second program component operated in the normal processing mode. 2. The information processing device regarding to the claim 1,
wherein the tamper detection value is a first hash value. 3. The information processing device regarding to the claim 2,
wherein the first hash value is calculated prior to executing the second program component. 4. The information processing device regarding to the claim 2,
wherein the processor is configured to execute the first program component for determining whether at least part of the second program component is tampered with, by comparing the first hash value and a second hash value which is calculated after the calculation of the first hash value. 5. The information processing device regarding to the claim 1,
wherein the secure processing mode is a higher security level than the normal processing mode. 6. The information processing device regarding to the claim 1,
wherein the processor is configured to execute the second program component after the second program component receives a Key for decryption from the first program component. 7. The information processing device regarding to the claim 6,
wherein the second program component receives the Key from the first component after the processor determines whether at least part of the second program component is tampered with. 8. A portable terminal comprising:
a processor configured to 1) execute a first program component for operating in a secure processing mode and performing tamper detection, and 2) execute a second program component for operating in a normal processing mode and performing a task; and
a hardware memory configured to store at least the second program component,
wherein the processor is configured to execute the first program component for determining whether at least part of the second program component is tampered with, by using a tamper detection value and while operating in the secure processing mode, and
the first program component operated in the secure processing mode cannot be accessed from the second program component operated in the normal processing mode. 8. A portable terminal comprising:
a processor configured to 1) execute a first program component for operating in a secure processing mode and performing tamper detection, and 2) execute a second program component for operating in a normal processing mode and performing a task; and
a hardware memory configured to store at least the second program component,
wherein the processor is configured to execute the first program component for determining whether at least part of the second program component is tampered with, by using a tamper detection value and while operating in the secure processing mode, and
the first program component operated in the secure processing mode cannot be accessed from the second program component operated in the normal processing mode. 9. The portable terminal regarding to the claim 8,
wherein the tamper detection value is a first hash value. 10. The portable terminal device regarding to the claim 9,
wherein the first hash value is calculated prior to executing the second program component. 11. The portable terminal device regarding to the claim 9,
wherein the processor is configured to execute the first program component for determining whether at least part of the second program component is tampered with, by comparing the first hash value and a second hash value which is calculated after the calculation of the first hash value. 12. The portable terminal device regarding to the claim 8,
wherein the secure processing mode is a higher security level than the normal processing mode. 13. The portable terminal device regarding to the claim 8,
wherein the processor is configured to execute the second program component after the second program component receives a Key for decryption from the first program component. 14. The portable terminal device regarding to the claim 13,
wherein the second program component receives the Key from the first component after the processor determines whether at least part of the second program component is tampered with. 15. A method for operating a portable terminal having a processor executing 1) a first program component for operating in a secure processing mode and performing tamper detection, and 2) a second program component for operating in a normal processing mode and performing a task, and a hardware memory storing at least the second program component, the method comprising:
executing the first program component by the processor for determining whether at least part of the second program component is tampered with, by using a tamper detection value and while operating in the secure processing mode; and
executing the second program component by the processor after the processor determines the second program component is not tampered,
wherein the first program component operated in the secure processing mode cannot be accessed from the second program component operated in the normal processing mode. 15. A method for operating a portable terminal having a processor executing 1) a first program component for operating in a secure processing mode and performing tamper detection, and 2) a second program component for operating in a normal processing mode and performing a task, and a hardware memory storing at least the second program component, the method comprising:
executing the first program component by the processor for determining whether at least part of the second program component is tampered with, by using a tamper detection value and while operating in the secure processing mode; and
executing the second program component by the processor after the processor determines the second program component is not tampered,
wherein the first program component operated in the secure processing mode cannot be accessed from the second program component operated in the normal processing mode. 16. The method regarding to the claim 15,
wherein the tamper detection value is a first hash value. 17. The method regarding to the claim 16,
wherein the first hash value is calculated prior to executing the second program component. 18. The method regarding to the claim 16,
wherein the processor is configured to execute the first program component for determining whether at least part of the second program component is tampered with, by comparing the first hash value and a second hash value which is calculated after the calculation of the first hash value. 19. The method regarding to the claim 15,
wherein the secure processing mode is a higher security level than the normal processing mode. 20. The method regarding to the claim 15,
wherein the processor is configured to execute the second program component after the second program component receives a Key for decryption from the first program component. 21. The method regarding to the claim 19,
wherein the second program component receives the Key from the first component after the processor determines whether at least part of the second program component is tampered with.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318771B2,US10318771B2,Inventory assistance device and method,2013-01-31,"method, identifying, device, receiving, configured, personal, positioned, bins, data, through, response, identify, activated, each, inventory, with, communicator, communicate, wireless, item, communication, storage, more, feedback, sending, comprises, having, signal, indicator, coupled, system, emitting, which, tags, controller, from, comprising, assistance","A system comprises two or more item storage bins, each item storage bin comprising: a wireless communication tag emitting a wireless signal having data identifying the item storage bin on which the wireless communication tag is positioned, and an indicator activated by receiving a feedback signal identifying the item storage bin on which the indicator is positioned; and a personal device comprising: a wireless communicator configured to communicate with the wireless communication tags, and a controller coupled to the wireless communicator and configured to identify one of the item storage bins from the wireless signal, and in response to identifying the one item storage bin, sending the feedback signal through the wireless communicator to the indicator.","1. A system, comprising:
two or more item storage bins, each item storage bin comprising:
a wireless communication tag emitting a wireless signal having data identifying the item storage bin on which the wireless communication tag is positioned, and
an indicator activated by receiving, from a personal device, a feedback signal identifying the item storage bin on which the indicator is positioned; and

the personal device, comprising:
a feedback module;
a wireless communicator configured to communicate with the wireless communication tags; and
a controller coupled to the wireless communicator, the controller configured to continuously scan for detection of the tags located within range of the wireless communicator, identify one of the two or more item storage bins from the wireless signal, and, in response to identifying the one item storage bin, sending the feedback signal through the wireless communicator to the indicator. 1. A system, comprising:
two or more item storage bins, each item storage bin comprising:
a wireless communication tag emitting a wireless signal having data identifying the item storage bin on which the wireless communication tag is positioned, and
an indicator activated by receiving, from a personal device, a feedback signal identifying the item storage bin on which the indicator is positioned; and

the personal device, comprising:
a feedback module;
a wireless communicator configured to communicate with the wireless communication tags; and
a controller coupled to the wireless communicator, the controller configured to continuously scan for detection of the tags located within range of the wireless communicator, identify one of the two or more item storage bins from the wireless signal, and, in response to identifying the one item storage bin, sending the feedback signal through the wireless communicator to the indicator. 2. The system of claim 1, wherein the indicator is a light emitting diode. 3. The system of claim 1, wherein the indicator emits light as positive feedback in response to receiving the bin-specific feedback signal. 4. The system of claim 1, wherein the wireless communication tag is a radio frequency identification tag. 5. The system of claim 1, wherein the wireless communicator communicates with the wireless communication tag when the wireless communicator is within a calibrated distance from the wireless communication tag. 6. The system of claim 1, wherein the personal device comprises a memory coupled to the controller and configured to store data identifying the item storage bin that corresponds to the wireless signal emitted from the wireless communication tag positioned thereon. 7. The system of claim 1, wherein the personal device comprises a wristband. 8. The system of claim 1, wherein the personal device comprises a glove. 9. The system of claim 1, wherein the personal device is configured to provide a feedback via the feedback module in response to sending the feedback signal. 10. The system of claim 1, wherein the personal device generates a haptic indication via the feedback module in response to sending the feedback signal. 11. The system of claim 1, wherein the personal device generates an aural indication via the feedback module in response to sending the feedback signal. 12. The system of claim 1, wherein the personal device generates a visual indication via the feedback module in response to sending the feedback signal. 13. A method, comprising:
storing bin-identifying data in a personal device memory about one or more specific item storage bins, the one or more specific item storage bins being a subset of a plurality of item storage bins;
selecting a specific item storage bin from the plurality of item storage bins;
determining an identification of the selected specific item storage bin by detecting, with the personal device, a wireless signal generated by a wireless communication tag disposed on the selected specific item storage bin when the personal device is in proximity to the selected specific item storage bin;
sending, with the personal device, a feedback signal to an indicator positioned on the selected specific item storage bin in response to determining the identification; and
generating a positive feedback with the indicator positioned on the selected specific item storage bin in response to receiving the feedback signal. 13. A method, comprising:
storing bin-identifying data in a personal device memory about one or more specific item storage bins, the one or more specific item storage bins being a subset of a plurality of item storage bins;
selecting a specific item storage bin from the plurality of item storage bins;
determining an identification of the selected specific item storage bin by detecting, with the personal device, a wireless signal generated by a wireless communication tag disposed on the selected specific item storage bin when the personal device is in proximity to the selected specific item storage bin;
sending, with the personal device, a feedback signal to an indicator positioned on the selected specific item storage bin in response to determining the identification; and
generating a positive feedback with the indicator positioned on the selected specific item storage bin in response to receiving the feedback signal. 14. The method of claim 13, wherein the indicator is a light emitting diode. 15. The method of claim 13, wherein the wireless communication tag is a radio frequency identification tag. 16. The method of claim 13, wherein the indicator emits light as positive feedback in response to receiving the feedback signal. 17. The method of claim 13, comprising generating a positive feedback with the personal device when the feedback signal is sent. 18. The method of claim 17, wherein the positive feedback is haptic, aural, visual feedback, or any combination thereof. 19. The method of claim 13, comprising:
communicating the feedback signal to a remote computer system having a memory; and
updating a record stored in the memory. 20. The method of claim 19, wherein the record is inventory levels.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318773B2,US10318773B2,Event RFID timing system and method having integrated participant event location tracking,2011-01-20,"method, integrated, device, timing, receiving, stamps, receiver, transmitted, interface, data, receives, wherein, providing, participant, each, with, reader, source, wireless, using, number, includes, received, that, being, communication, time, provided, location, traveling, detection, event, information, determining, having, transmits, reads, route, tracked, associated, associating, system, along, from, over, rfid, tracking","A system and method for determining and tracking a location of a participant traveling along a route wherein the participant being tracked with an RFID tag using an RFID tag reader and a timing system, the system includes a location device associated with the RFID tag and has a location data receiver for receiving location information from a location providing source, and a wireless communication interface and time stamps each received location information, and transmits tag location data over the wireless interface, and a location detection device that is in wireless communication with the location device receives the transmitted tag location data, and transmits the received location data to the timing system, with the timing system associating the received location data with the RFID tag number as provided by the RFID tag reader provided tag reads of the RFID tag associated with the location device.","1. A system for determining and tracking a location of a participant tracked along a route with a RFID tag having a unique RFID tag number during an event using a timing system communicatively coupled to an RFID tag reader system, the tag reader system is configured to be located at a fixed location in proximity to the route at a first monitored location point for receiving one or more tag reads from the RFID tag as the tracked participant with the RFID tag travels in proximity to said location along the route, and configured for determining a time for each tag read, identifying the tag number of the RFID tag, and transmitting a tag read message including at least a portion of the received tag reads with the RFID tag number and the determined tag read time, the timing system configured for receiving and storing the transmitted tag read message including the tag number and the determined tag read times as transmitted by the tag reader system, wherein the participant has a location device configured for associating with the participant, the location device configured for receiving a plurality of location data as the participant travels along the route, time stamping each received location data, and transmitting location information including the received plurality of location data and the time stamps over a wireless communication interface, said system comprising:
a location detection device configured to be located at a fixed location in proximity to the route at a second monitored location point and configured to be in wireless communication with the location device and configured for receiving the transmitted location information from the location device;
wherein the timing system stores the location information received by the location detection device including the time stamps and associates the stored location information with the RFID tag number of at least one of the one or more tag reads of the received tag read message as a function of the tag read times and the time stamps of the location information, the timing system determining a plurality of tracked locations of the participant along the route of the event as a function of the plurality of location data and time stamps provided by the location information that is associated with the tag number for the participant. 1. A system for determining and tracking a location of a participant tracked along a route with a RFID tag having a unique RFID tag number during an event using a timing system communicatively coupled to an RFID tag reader system, the tag reader system is configured to be located at a fixed location in proximity to the route at a first monitored location point for receiving one or more tag reads from the RFID tag as the tracked participant with the RFID tag travels in proximity to said location along the route, and configured for determining a time for each tag read, identifying the tag number of the RFID tag, and transmitting a tag read message including at least a portion of the received tag reads with the RFID tag number and the determined tag read time, the timing system configured for receiving and storing the transmitted tag read message including the tag number and the determined tag read times as transmitted by the tag reader system, wherein the participant has a location device configured for associating with the participant, the location device configured for receiving a plurality of location data as the participant travels along the route, time stamping each received location data, and transmitting location information including the received plurality of location data and the time stamps over a wireless communication interface, said system comprising:
a location detection device configured to be located at a fixed location in proximity to the route at a second monitored location point and configured to be in wireless communication with the location device and configured for receiving the transmitted location information from the location device;
wherein the timing system stores the location information received by the location detection device including the time stamps and associates the stored location information with the RFID tag number of at least one of the one or more tag reads of the received tag read message as a function of the tag read times and the time stamps of the location information, the timing system determining a plurality of tracked locations of the participant along the route of the event as a function of the plurality of location data and time stamps provided by the location information that is associated with the tag number for the participant. 2. The system of claim 1 wherein the location device is configured to include the RFID tag number of the associated RFID tag for the participant and the location information includes the RFID tag number. 3. The system of claim 1 wherein location device is configured for storing location data in a memory at intervals based on a predetermined rate. 4. The system of claim 3 wherein the location device is configured to receive programming instructions for establishing the predetermined rate, and wherein such predetermined rate can change with the receipt of new programming instructions during the event. 5. The system of claim 1 wherein the location device and the RFID tag are each configured to be carried on a body part of the participant, and wherein the location device is not communicatively coupled to the RFID tag. 6. The system of claim 1 wherein the location device is configured to receive a location data download request from the location detection device based on a download location command, and wherein the location device is configured to transmit the location information to the location detection device responsive to said received location download request. 7. The system of claim 1 wherein the location device is configured to only transmit the received location data responsive to the location detection device receiving an input that a transmission to the location detection device is currently available. 8. The system of claim 1 wherein the timing system is configured to transmit to a remote system, before, during and after the event the plurality of tracked locations associated with a particular RFID tag number responsive to a the timing system receiving a request for participant specific location data that includes the particular RFID tag number from the remote system. 9. The system of claim 8, further comprising the remote system selected from the group consisting of a Kiosk, a website, a mobile phone, a mobile computing device, a portable computer, a tablet, a watch, a news station, or a broadcast network, the remote system being configured to transmit the request for the participant specific location data and the particular RFID tag number to the timing system, to receive the participant specific location data in the form of the plurality of tracked locations as transmitted by the timing system, and to display the received plurality of tracked locations based on the received location data for the particular RFID tag number on a display map of the route indicating the route traveled by the participant or the most recent and current location of the participant. 10. The system of claim 1 wherein the location device is configured for selective activation and deactivation of a wireless location data receiver, and wherein a wireless communication interface of the location device is configured to receive activation and deactivation commands, and wherein the location detection device is configured for transmitting the activation and deactivation commands responsive to a message received from the timing system. 11. The system of claim 1 wherein the location detection device is configured to transmit to the location device a location data request requesting all or a portion of the stored location data from the location device when the location device is configured to be in proximity to the location detection device, and wherein the location device is configured to transmit the requested location data responsive to the received location data request. 12. The system of claim 11 wherein the location detection device is configured to transmit a dump all data command and then transmit a clear all data command when the location device is in proximity to the location detection device, and wherein the location device is configured to transmit all of the stored location data and then clear the location data responsive to receipt of the dump all data command and then transmit a clear all data command from the location detection device. 13. The system of claim 1 wherein the location providing source is a plurality of global positioning system (GPS) satellites and wherein the location data is GPS position data. 14. The system of claim 1 wherein the location providing source is a plurality of wireless transmitters from a cellular network, and wherein the location data is cellular triangulation position data. 15. A method for determining and tracking a location of a participant traveling along a route wherein the participant being tracked with a RFID tag during an event using a timing system communicatively coupled to an RFID tag reader system, the tag reader system configured to be located at a fixed location in proximity to the route at a first monitored location point for receiving one or more tag reads from the RFID tag as the tracked participant with the RFID tag travels in proximity to said location along the route, and configured for determining a time for each tag read, identifying the tag number of the RFID tag, and transmitting a tag read message including at least a portion of the received tag reads with the RFID tag number and the determined tag read time, the timing system configured for receiving and storing the transmitted tag read message including the tag number and the determined tag read times as transmitted by the tag reader system, the tag reader system configured for receiving one or more RFID tag reads from the RFID tag as the tracked participant with the RFID tag travels the route, determining a time for each tag read, identifying the tag number of the RFID tag, and transmitting a tag read message including at least a portion of the received tag reads with the tag number and the determined times, the timing system configured for receiving the tag read messages as transmitted by the tag reader system and storing the tag read message including the tag number from the tag reader system, the method for use with a location device that is configured to be associated with the RFID tag of the participant and configured for:
receiving a plurality of location data from a location providing source;
time stamping each received location data;
and
transmitting location data including the time stamps for each the method comprising:
in a location detection device configured to be located at a fixed location in proximity to the route and in wireless communication with the location device:
receiving the transmitted location data; and
transmitting the received location data to the timing system;
in the timing system:
receiving the location data including the plurality of location data and time stamps;
associating the location data with the RFID tag number as provided by the RFID tag reader system from at least one of the tag reads from the tag associated with the location device;
determining a plurality of tracked locations of the participant along the route of the event as a function of the plurality of location data and time stamps provided by the location information that is associated with the at least one tag read number for the participant
storing the plurality of location data associated with the RFID tag number. 15. A method for determining and tracking a location of a participant traveling along a route wherein the participant being tracked with a RFID tag during an event using a timing system communicatively coupled to an RFID tag reader system, the tag reader system configured to be located at a fixed location in proximity to the route at a first monitored location point for receiving one or more tag reads from the RFID tag as the tracked participant with the RFID tag travels in proximity to said location along the route, and configured for determining a time for each tag read, identifying the tag number of the RFID tag, and transmitting a tag read message including at least a portion of the received tag reads with the RFID tag number and the determined tag read time, the timing system configured for receiving and storing the transmitted tag read message including the tag number and the determined tag read times as transmitted by the tag reader system, the tag reader system configured for receiving one or more RFID tag reads from the RFID tag as the tracked participant with the RFID tag travels the route, determining a time for each tag read, identifying the tag number of the RFID tag, and transmitting a tag read message including at least a portion of the received tag reads with the tag number and the determined times, the timing system configured for receiving the tag read messages as transmitted by the tag reader system and storing the tag read message including the tag number from the tag reader system, the method for use with a location device that is configured to be associated with the RFID tag of the participant and configured for:
receiving a plurality of location data from a location providing source;
time stamping each received location data;
and
transmitting location data including the time stamps for each the method comprising:
in a location detection device configured to be located at a fixed location in proximity to the route and in wireless communication with the location device:
receiving the transmitted location data; and
transmitting the received location data to the timing system;
in the timing system:
receiving the location data including the plurality of location data and time stamps;
associating the location data with the RFID tag number as provided by the RFID tag reader system from at least one of the tag reads from the tag associated with the location device;
determining a plurality of tracked locations of the participant along the route of the event as a function of the plurality of location data and time stamps provided by the location information that is associated with the at least one tag read number for the participant
storing the plurality of location data associated with the RFID tag number. 16. The method of claim 15 wherein in the location device, the storing of location data is at intervals based on a predetermined rate. 17. The method of claim 16 wherein in the location device, receiving programming instructions for establishing the predetermined rate, and wherein such predetermined rate can be changed with the receipt of new programming instructions during an event. 18. The method of claim 15 wherein in the location device, receiving a location data download request, and transmitting the tag location to a location detection device configured to be is responsive to said request. 19. The method of claim 15, wherein in at least one of the timing system and the tag reader system:
detecting a proximity of the RFID tag to the RFID tag reader system; and
transmitting a location data download command to the location detection device, wherein in the location detection device, transmitting the location data download request to the location device. 20. The method of claim 15 wherein in the location detection device, the transmitting is only responsive to the location detection device receiving an input that a transmission to the location detection device is currently available. 21. The method of claim 15 wherein in the timing system, transmitting the location data associated with a particular RFID tag number responsive to a request from a remote system. 22. The method of claim 15, wherein in a remote system selected from the group consisting of a Kiosk, a website, a mobile phone, a mobile computing device, a portable computer, a tablet, a watch, a news station, or a broadcast network;
requesting location data from the timing system for the particular RFID tag number; and
receiving the requested location data in the form of the plurality of tracked locations as transmitted by the timing system, and displaying the plurality of tracked locations as transmitted by the timing system on a display map of the route indicating the route traveled by the participant associated with the particular RFID tag number or the most recent and current location of the participant associated with the particular RFID tag number. 23. The method of claim 15
wherein in the location detection device:
transmitting at least one of an activation and a deactivation command responsive to a message received from the timing system;
wherein in the location device:
selectively activating and deactivating a location data receiver responsive to receipt of the message from the location detection device. 24. The method of claim 15 wherein in the location detection device transmitting to the location tag a location data request requesting all or a portion of the location data from the location device, and wherein in the location device transmitting the location data responsive to the location data request. 25. The system of claim 15 wherein in the location detection device transmitting a dump all data command and then transmitting a clear all data command, and wherein in the location device, transmitting all of the location data responsive to the receipt of the dump all data command and clearing a memory of all location data responsive to receiving the clear all data command.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318774B2,US10318774B2,"Housing, housing unit, and casing unit",,"direction, optical, travels, second, housing, vertical, define, light, defined, lower, single, component, portions, directly, house, inside, path, portion, extending, extend, cavity, arranged, therein, tubular, axis, includes, unit, being, central, connected, incoming, casing, having, indirectly, least, monolithic, along, which, member, radially, first","A housing includes a first tubular portion being tubular, arranged to extend along a central axis extending in a vertical direction, and having a first cavity defined radially inside of the first tubular portion; and a second tubular portion being tubular, connected to a lower portion of the first tubular portion directly or indirectly, and having a second cavity defined radially inside of the second tubular portion. The first cavity is arranged to define a light path along which the incoming light travels, and is connected to the second cavity. The second tubular portion is arranged to house at least a portion of the optical component therein. The first and second tubular portions are defined by a single monolithic member.","1. A housing unit, comprising:
a rotary drive apparatus arranged to rotate an optical component arranged to reflect incoming light coming from a light source; and
a housing arranged to house therein at least a portion of the rotary drive apparatus, the housing including:
a first tubular portion being tubular, arranged to extend along a central axis extending in a vertical direction, and having a first cavity defined radially inside of the first tubular portion; and
a second tubular portion being tubular, connected to a lower portion of the first tubular portion directly or indirectly, and having a second cavity defined radially inside of the second tubular portion; wherein

the first cavity is arranged to define a light path along which the incoming light travels, and is connected to the second cavity;
the second tubular portion is arranged to house at least a portion of the optical component therein;
the first and second tubular portions are defined by a single monolithic member;
the rotary drive apparatus includes:
a motor including a rotating portion arranged to rotate about the central axis; and
a flywheel including the optical component, supported by the rotating portion, and caused by the rotating portion to rotate about the central axis;
the second tubular portion has an opening portion defined in at least one circumferential position; and
the opening portion is arranged to allow reflected light resulting from reflection of the incoming light by the optical component to be emitted to an outside of the rotary drive apparatus therethrough; and

a second through hole extends through the second tubular portion in a radial direction at a position different from that of the opening portion; and
a rotation speed of the rotary drive apparatus is detected by sensing reflected light beams which exit out from the housing unit through the second through hole. 1. A housing unit, comprising:
a rotary drive apparatus arranged to rotate an optical component arranged to reflect incoming light coming from a light source; and
a housing arranged to house therein at least a portion of the rotary drive apparatus, the housing including:
a first tubular portion being tubular, arranged to extend along a central axis extending in a vertical direction, and having a first cavity defined radially inside of the first tubular portion; and
a second tubular portion being tubular, connected to a lower portion of the first tubular portion directly or indirectly, and having a second cavity defined radially inside of the second tubular portion; wherein

the first cavity is arranged to define a light path along which the incoming light travels, and is connected to the second cavity;
the second tubular portion is arranged to house at least a portion of the optical component therein;
the first and second tubular portions are defined by a single monolithic member;
the rotary drive apparatus includes:
a motor including a rotating portion arranged to rotate about the central axis; and
a flywheel including the optical component, supported by the rotating portion, and caused by the rotating portion to rotate about the central axis;
the second tubular portion has an opening portion defined in at least one circumferential position; and
the opening portion is arranged to allow reflected light resulting from reflection of the incoming light by the optical component to be emitted to an outside of the rotary drive apparatus therethrough; and

a second through hole extends through the second tubular portion in a radial direction at a position different from that of the opening portion; and
a rotation speed of the rotary drive apparatus is detected by sensing reflected light beams which exit out from the housing unit through the second through hole. 2. The housing unit according to claim 1, wherein the housing is made of a resin. 3. The housing unit according to claim 1, wherein the housing is made of a metal. 4. The housing unit according to claim 1, wherein the second tubular portion is arranged to have an outside diameter greater than that of the first tubular portion. 5. The housing unit according to claim 1, wherein the second tubular portion further includes a second through hole arranged to pass through the second tubular portion in a radial direction at a position different from that of the opening portion. 6. The housing unit according to claim 1, wherein
at least a portion of the light source is arranged on the central axis; and
the first tubular portion is further arranged to house at least a portion of the light source in the first cavity. 7. A casing unit comprising a casing and a plurality of housing units including a first housing unit and a second housing unit different from the first housing unit, the first housing unit being the housing unit of claim 1, wherein the casing is arranged to house at least a portion of each of the plurality of housing units therein. 8. The casing unit according to claim 7, wherein the first housing unit includes one or more collar portions each of which is arranged to project radially outward from an upper end or a lower end of the first tubular portion or the second tubular portion, and is fixed to the casing through press fitting, adhesion, welding, or screwing. 9. The casing unit according to claim 7, wherein each of the plurality of housing units includes a housing of an equal size. 10. The casing unit according to claim 7, wherein
the second housing unit includes a second housing having a second central axis as a central axis thereof; and
the central axis of the housing of the first housing unit is arranged to cross the second central axis. 11. The casing unit according to claim 7, wherein
each of the plurality of housing units includes a housing; and
central axes of the housings of the plurality of housing units are arranged to cross each other. 12. The casing unit according to claim 7, wherein
in a vertical sectional view, the casing is rectangular and has a greater dimension in a longitudinal direction; and
the first and second housing units are arranged to overlap with each other when viewed in the longitudinal direction of the casing. 13. The casing unit according to claim 7, wherein
in a vertical sectional view, the casing is rectangular and has a smaller dimension in a transverse direction; and
the housing of the first housing unit is arranged to have an axial dimension smaller than a dimension of the casing as measured in the transverse direction.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318777B2,US10318777B2,Target options for a data capture device with intuitive aiming involving specular objects,2016-12-23,"involving, scanner, enhance, mirror, camera, optical, device, specular, back, surface, reflected, scanned, within, reflecting, transmitting, data, operator, view, object, captures, usability, used, with, field, like, aligns, image, source, easily, aligned, allows, that, being, placed, seen, place, once, aiming, capture, objects, target, partially, indicator, options, intuitive, pattern, presentation, projected, which, from, appears, code, over, order","A presentation scanner allows an operator to easily place an object being scanned within the scanner's field of view in order to enhance usability of the scanner. A partially transmitting and partially reflecting surface, like a mirror, is used. An aiming pattern or target indicator is projected from an aiming source to the surface. The aiming pattern or target indicator is seen by an operator who aligns the object, which is reflected from the surface back to the operator, to be placed over or within the aiming pattern that appears at the surface. Once aligned, a camera captures the image of the optical code.","1. An imaging scanner with enhanced usability and aiming, comprising:
a scanning camera positioned to receive an image of a first item transmitted through a mirror, wherein the mirror has a partially reflecting and partially transmitting coating;
the scanning camera sees the image of the first item through the mirror where the first item is positioned by an operator on a first side of the mirror and the scanning camera is on a second side of the mirror;
the mirror located so that the operator can position the first item's data and can aim the first item's data at the scanning camera, wherein the operator has a first line of sight to the first item's data through a reflection on the first side of the mirror and the scanning camera has a second line of sight to the first item's data from the second side of the mirror; and
the mirror also located so that the operator positions the first item's data such that the scanning camera sees the first item's data through the mirror. 1. An imaging scanner with enhanced usability and aiming, comprising:
a scanning camera positioned to receive an image of a first item transmitted through a mirror, wherein the mirror has a partially reflecting and partially transmitting coating;
the scanning camera sees the image of the first item through the mirror where the first item is positioned by an operator on a first side of the mirror and the scanning camera is on a second side of the mirror;
the mirror located so that the operator can position the first item's data and can aim the first item's data at the scanning camera, wherein the operator has a first line of sight to the first item's data through a reflection on the first side of the mirror and the scanning camera has a second line of sight to the first item's data from the second side of the mirror; and
the mirror also located so that the operator positions the first item's data such that the scanning camera sees the first item's data through the mirror. 2. The imaging scanner of claim 1, further comprising an aiming frame that originates from an aiming source that is located in proximity to the scanning camera, the aiming frame is visible when viewed from the first side of the mirror, wherein the aiming frame represents a field of view of the scanning camera. 3. The imaging scanner of claim 2, wherein the operator sees the aiming frame at the first side of the mirror, and wherein the operator adjusts a position of the first item to place the reflection of the first item's data in the aiming frame at the first side of the mirror. 4. The imaging scanner of claim 3, wherein the aiming frame is coaxial with an optical axis of the scanning camera. 5. The imaging scanner of claim 4, wherein the mirror has a reflective coating on the second side. 6. The imaging scanner of claim 4, wherein the first line of sight of the operator is not coaxial with the second line of sight of the scanning camera.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318775B2,US10318775B2,Authenticable digital code and associated systems and methods,2016-06-24,"second, digital, light, machine, detecting, illuminated, determined, positioned, readable, authenticity, security, wavelength, systems, wave, represent, position, formed, medium, signature, authenticable, when, different, length, methods, includes, that, graphically, fluoresces, information, fluorescent, material, printable, least, relative, associated, from, excited, code, first, emits","An authenticable digital code includes a printable medium, a machine-readable digital code, formed on the printable medium, that graphically represent information, and at least one security signature positioned relative to the machine-readable digital code. The security signature includes a fluorescent material that, when excited by light of a first wavelength, fluoresces and emits light at a second wavelength that is different from the first wave length. Authenticity of the authenticable digital code is determined by detecting, when the authenticable digital code is illuminated by light of the first wavelength, light of the second wavelength at a position relative to the machine-readable digital code.","1. An authenticable digital code, comprising:
a printable medium;
a machine-readable digital code, formed on the printable medium, that graphically represents information; and
at least one security signature comprising a fluorescent material that, when excited by first light at a first wavelength, fluoresces and emits second light at a second wavelength, that is different from the first wavelength, the second light emitting at a position relative to the machine-readable digital code for indicating authenticity of the authenticable digital code. 1. An authenticable digital code, comprising:
a printable medium;
a machine-readable digital code, formed on the printable medium, that graphically represents information; and
at least one security signature comprising a fluorescent material that, when excited by first light at a first wavelength, fluoresces and emits second light at a second wavelength, that is different from the first wavelength, the second light emitting at a position relative to the machine-readable digital code for indicating authenticity of the authenticable digital code. 2. The authenticable digital code of claim 1, the fluorescent material being configured to fluoresce and emit the second light based upon the first light being generated by a mobile device without additional light sources or filters. 3. The authenticable digital code of claim 1, the first wavelength corresponding to a blue color and the second wavelength corresponding to a red color. 4. The authenticable digital code of claim 1, the position of the security signature relative to the machine-readable digital code being based upon the information. 5. The authenticable digital code of claim 1, the machine-readable digital code being selected from the group including: a barcode, a matrix code, and a QR code. 6. The authenticable digital code of claim 1, the security signature comprising a graphical representation of a security code value that indicates authenticity of the authenticable digital code. 7. The authenticable digital code of claim 6, the security code value being based upon the information. 8. The authenticable digital code of claim 6, the information being encrypted and associated with the security signature being related such that the information may be decrypted by the security code value. 9. The authenticable digital code of claim 6, the security signature further comprising a second graphical representation of a second security code value that further indicates authenticity of the authenticable digital code. 10. The authenticable digital code of claim 9, the second graphical representation being printed to the printable medium using a second fluorescent material, that, when excited by third light of a third wavelength that is different from the first and second wavelengths, fluoresces and emits fourth light at a fourth wavelength that is different from the first, second and third wavelengths, the fourth light at a second position relative to the machine-readable digital code indicating authenticity of the authenticable digital code. 11. The authenticable digital code of claim 1, the first and second wavelengths corresponding to visible light. 12. A method for manufacturing an authenticable digital code, comprising:
generating a code print image that graphically represents information of the authenticable digital code;
printing the code print image onto a printable medium using one or more visible inks to form a machine-readable digital code;
generating a security print image that graphically represents a security signature;
printing the security print image onto the printable medium in a position relative to the code print image using a fluorescent material that, when excited by first light at a first wavelength, fluoresces and emits second light at a second wavelength that is different from the first wave length, the second light emitting at a position relative to the machine-readable digital code for indicating authenticity of the authenticable digital code. 12. A method for manufacturing an authenticable digital code, comprising:
generating a code print image that graphically represents information of the authenticable digital code;
printing the code print image onto a printable medium using one or more visible inks to form a machine-readable digital code;
generating a security print image that graphically represents a security signature;
printing the security print image onto the printable medium in a position relative to the code print image using a fluorescent material that, when excited by first light at a first wavelength, fluoresces and emits second light at a second wavelength that is different from the first wave length, the second light emitting at a position relative to the machine-readable digital code for indicating authenticity of the authenticable digital code. 13. A system for authenticating an authenticable digital code, comprising:
a processor;
a non-transitory memory communicatively coupled with the processor;
a camera controlled by the processor and having a field of view;
a light emitting display controlled by the processor that emits first light into the field-of-view of the camera; and
software comprising machine-readable instructions stored in the memory that, when executed by the processor, are capable of:
controlling the light emitting display to emit the first light of a first wavelength;
controlling the camera to capture an image of the authenticable digital code positioned within the field of view and illuminated by the first light;
analyzing the image to detect second light of a second wavelength at a position within the image relative to the machine-readable digital code resulting from fluorescence of a security signature on the authenticable digital code; and
decoding and authenticating the authenticable digital code based upon the position and the second wavelength. 13. A system for authenticating an authenticable digital code, comprising:
a processor;
a non-transitory memory communicatively coupled with the processor;
a camera controlled by the processor and having a field of view;
a light emitting display controlled by the processor that emits first light into the field-of-view of the camera; and
software comprising machine-readable instructions stored in the memory that, when executed by the processor, are capable of:
controlling the light emitting display to emit the first light of a first wavelength;
controlling the camera to capture an image of the authenticable digital code positioned within the field of view and illuminated by the first light;
analyzing the image to detect second light of a second wavelength at a position within the image relative to the machine-readable digital code resulting from fluorescence of a security signature on the authenticable digital code; and
decoding and authenticating the authenticable digital code based upon the position and the second wavelength. 14. The system of claim 13, the processor, the memory, the light emitting display and the camera being part of a device selected from the group of mobile devices including a smartphone, a tablet computer, a laptop computer, a notebook computer, an MP3 player, and a personal digital assistant. 15. The system of claim 13, the software further comprising machine-readable instructions that when executed by the processor are capable of de-blurring the first image when the authenticable digital code is positioned outside a focusing range of the camera. 16. The system of claim 13, the first and second wavelengths corresponding to visible light. 17. A method for authenticating an authenticable digital code, comprising the steps of:
controlling a light emitting display to emit first light of a first wavelength;
controlling a camera to capture an image of the authenticable digital code illuminated by the first light;
analyzing the image to detect second light of a second wavelength at a position within the image relative to a machine-readable digital code of the authenticable digital code and resulting from fluorescence of a security signature of the authenticable digital code; and
decoding and authenticating the machine-readable digital code of the authenticable digital code based upon the position of the second light in the image relative to the machine-readable digital code within the image. 17. A method for authenticating an authenticable digital code, comprising the steps of:
controlling a light emitting display to emit first light of a first wavelength;
controlling a camera to capture an image of the authenticable digital code illuminated by the first light;
analyzing the image to detect second light of a second wavelength at a position within the image relative to a machine-readable digital code of the authenticable digital code and resulting from fluorescence of a security signature of the authenticable digital code; and
decoding and authenticating the machine-readable digital code of the authenticable digital code based upon the position of the second light in the image relative to the machine-readable digital code within the image.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318778B2,US10318778B2,Reducing perceived brightness of illumination light source in electro-optical readers that illuminate and read targets by image capture,2012-11-15,"optical, targets, light, element, illuminated, captured, emitted, imaging, reduce, window, reflecting, scatter, array, incident, directed, path, through, portion, apparent, size, return, readers, with, directing, image, reducing, brightness, source, that, increase, diffusing, illuminate, electro, optically, capture, both, illumination, sensors, folded, projected, along, system, from, perceived, blur, located, read","Targets to be electro-optically read by image capture are illuminated with illumination light emitted from an illumination light source and directed along an illumination path through a window to the targets, and return light from the targets is captured through the window and projected along an imaging path to an array of light sensors of an imaging system. An optical element is located in both the illumination path and the imaging path, and has a light-reflecting, non-diffusing portion for directing the captured return light incident on the light-reflecting portion along a folded imaging path to the array, and a light-diffusing portion for diffusing the illumination light incident on the light-diffusing portion along a folded illumination path to scatter and blur an image of the illumination light source, to increase an apparent size of the illumination light source, and to reduce a perceived brightness of the illumination light source.","1. An apparatus for electro-optically reading targets by image capture, comprising:
a housing;
a window supported by the housing;
an illumination source supported by the housing and configured to illuminate an area with illumination light directed along an illumination path through the window to the area;
a solid-state imager supported by the housing and having an array of light sensors having an imaging field of view that extends through the window to the area;
an imaging lens system configured to capture return illumination light from the area through the window, and to project the captured return illumination light along an imaging path to the array of light sensors; and
a light-reflecting optical element located within both the illumination path and the imaging path, the light-reflecting optical element including:
a light-reflecting, non-diffusing portion for folding the imaging path and for directing the captured return illumination light incident on the light-reflecting, non-diffusing portion along the folded imaging path to the array of light sensors; and
a light-diffusing portion for folding the illumination path with diffusive reflection of the illumination light incident on the light-diffusing portion along the folded illumination path. 1. An apparatus for electro-optically reading targets by image capture, comprising:
a housing;
a window supported by the housing;
an illumination source supported by the housing and configured to illuminate an area with illumination light directed along an illumination path through the window to the area;
a solid-state imager supported by the housing and having an array of light sensors having an imaging field of view that extends through the window to the area;
an imaging lens system configured to capture return illumination light from the area through the window, and to project the captured return illumination light along an imaging path to the array of light sensors; and
a light-reflecting optical element located within both the illumination path and the imaging path, the light-reflecting optical element including:
a light-reflecting, non-diffusing portion for folding the imaging path and for directing the captured return illumination light incident on the light-reflecting, non-diffusing portion along the folded imaging path to the array of light sensors; and
a light-diffusing portion for folding the illumination path with diffusive reflection of the illumination light incident on the light-diffusing portion along the folded illumination path. 2. The apparatus of claim 1, wherein the window is substantially planar, and wherein the housing supports the window in one of a generally upright and a generally horizontal plane. 3. The apparatus of claim 1, wherein the illumination light source includes a plurality of light emitting diodes. 4. The apparatus of claim 1, wherein:
the light-reflecting optical element is of one-piece construction;
the light-reflecting, non-diffusing portion is located on a first part of the light-reflecting optical element; and
the light-diffusing portion is located on an outer surface of a second part of the light-reflecting optical element. 5. The apparatus of claim 4, wherein the light-reflecting, non-diffusing portion has a surface finish less than about five micrometers, and wherein the light-diffusing portion has a surface finish exceeding about five micrometers. 6. The apparatus of claim 1, wherein the light-reflecting optical element is constituted of one of glass and plastic. 7. The apparatus of claim 1, further comprising a fold mirror located in both the illumination path and the imaging path, the fold mirror to:
fold the folded imaging path and to reflect the captured return illumination light incident thereon to the light-reflecting optical element; and
fold the illumination path and to reflect the illumination light from the light-reflecting optical element through the window. 8. An apparatus for electro-optically reading targets by image capture, comprising:
a housing;
a window supported by the housing;
an illumination source supported by the housing, and configured to illuminate an area with illumination light directed along an illumination path through the window to the area;
a solid-state imager supported by the housing and having an array of light sensors having an imaging field of view that extends through the window to the area;
an imaging lens system configured to capture return illumination light from the area through the window and to project the captured return illumination light along an imaging path to the array of light sensors; and
a light-reflecting optical element located within both the illumination path and the imaging path, the light-reflecting optical element including:
a light-reflecting, non-diffusing portion to direct the captured return illumination light incident on the light-reflecting, non-diffusing portion along the imaging path to the array of light sensors; and
a light-diffusing portion configured to fold the illumination path with diffusive reflection of the illumination light incident on the light-diffusing portion along the folded illumination path to scatter and blur an image of the illumination light source. 8. An apparatus for electro-optically reading targets by image capture, comprising:
a housing;
a window supported by the housing;
an illumination source supported by the housing, and configured to illuminate an area with illumination light directed along an illumination path through the window to the area;
a solid-state imager supported by the housing and having an array of light sensors having an imaging field of view that extends through the window to the area;
an imaging lens system configured to capture return illumination light from the area through the window and to project the captured return illumination light along an imaging path to the array of light sensors; and
a light-reflecting optical element located within both the illumination path and the imaging path, the light-reflecting optical element including:
a light-reflecting, non-diffusing portion to direct the captured return illumination light incident on the light-reflecting, non-diffusing portion along the imaging path to the array of light sensors; and
a light-diffusing portion configured to fold the illumination path with diffusive reflection of the illumination light incident on the light-diffusing portion along the folded illumination path to scatter and blur an image of the illumination light source. 9. The apparatus of claim 8, wherein:
the light-reflecting optical element is of one-piece construction;
the light-reflecting, non-diffusing portion is located on a first part of the light-reflecting optical element; and
the light-diffusing portion is located on an outer surface of a second part of the light-reflecting optical element. 10. The apparatus of claim 9, wherein the light-reflecting, non-diffusing portion has a surface finish less than about five micrometers, and wherein the light-diffusing portion has a surface finish exceeding about five micrometers. 11. A method of electro-optically reading targets by image capture, comprising:
illuminating an area with illumination light emitted from an illumination light source and directed along an illumination path through a window of a housing to the area;
capturing return illumination light from the area through the window; and
projecting the captured return illumination light along an imaging path to an array of light sensors of a solid-state imager;
wherein the illumination light is provided from and the return illumination light is returned to a light-reflecting optical element located within both the illumination path and the imaging path, wherein the light-reflecting optical element has a light-reflecting, non-diffusing portion for folding the imaging path and for directing the captured return illumination light incident on the light-reflecting, non-diffusing portion along the folded imaging path to the array, and has a light-diffusing portion for folding the illumination path with diffusive reflection of the illumination light incident on the light-diffusing portion along the folded illumination path to scatter and blur an image of the illumination light source. 11. A method of electro-optically reading targets by image capture, comprising:
illuminating an area with illumination light emitted from an illumination light source and directed along an illumination path through a window of a housing to the area;
capturing return illumination light from the area through the window; and
projecting the captured return illumination light along an imaging path to an array of light sensors of a solid-state imager;
wherein the illumination light is provided from and the return illumination light is returned to a light-reflecting optical element located within both the illumination path and the imaging path, wherein the light-reflecting optical element has a light-reflecting, non-diffusing portion for folding the imaging path and for directing the captured return illumination light incident on the light-reflecting, non-diffusing portion along the folded imaging path to the array, and has a light-diffusing portion for folding the illumination path with diffusive reflection of the illumination light incident on the light-diffusing portion along the folded illumination path to scatter and blur an image of the illumination light source. 12. The method of claim 11, wherein the window is substantially planar and in one of a generally upright and a generally horizontal plane. 13. The method of claim 11, wherein the illumination light source comprises a plurality of light emitting diodes. 14. The method of claim 11, wherein the light-reflecting optical element comprises one-piece having the light-reflecting, non-diffusing portion on a first part of the light-reflecting optical element and having the light-diffusing portion on an outer surface of a second part of the light-reflecting optical element. 15. The method of claim 14, wherein the light-reflecting, non-diffusing portion has a surface finish less than about five micrometers, and the light-diffusing portion has a surface finish exceeding about five micrometers. 16. The method of claim 11, wherein the light-reflecting optical element is formed of one of glass and plastic. 17. The method of claim 11, wherein a fold mirror is located in both the illumination path and the imaging path, the fold mirror folding the folded imaging path and reflecting the captured return illumination light incident thereon to the light-reflecting optical element, and folding the illumination path and reflecting the illumination light from the light-reflecting optical element through the window.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318761B2,US10318761B2,Data processing systems and methods for auditing data request compliance,2016-06-10,"perform, parameters, long, advocacy, part, screen, configured, suitable, display, protection, based, officer, process, systems, data, related, results, privacy, takes, particular, ngos, average, fulfill, individuals, request, such, displaying, methods, complaints, auditing, subject, that, processing, audit, provided, more, customer, enable, provide, management, submit, further, system, from, tracking, groups, access, logging, requests, compliance","A privacy management system that is configured to process one or more data subject access requests and further configured to: (1) enable a data protection officer to submit an audit request; (2) perform an audit based on one or more parameters provided as part of the request (e.g., one or more parameters such as how long an average request takes to fulfill, one or more parameters related to logging and/or tracking data subject access requests and/or complaints from one or more particular customer advocacy groups, individuals, NGOs, etc.); and (3) provide one or more audit results to the officer (e.g., by displaying the results on a suitable display screen).","1. A privacy management computer system for auditing one or more responses to one or more data subject access requests received by a particular entity, the system comprising:
one or more computer processors; and
computer memory operatively coupled to the one or more processors, wherein the one or more computer processors are adapted for:
receiving a plurality of data subject access requests via a plurality of webforms on respective computing devices from a plurality of data subject access requestors;
automatically determining a type of each data subject access request, the determined type of data subject access request being selected from a group consisting of: (1) a request to delete personal data of the requestor that is being stored by a particular organization; (2) a request to provide, to the requestor, personal data of the requestor that is being stored by the particular organization; (3) a request to update personal data of the requestor that is being stored by the particular organization; and (4) a request to opt out of having the particular organization use the requestor's personal information in one or more particular ways;
determining, based at least partially on the determined type of each data subject access request, a workflow that is to be used to process each request;
facilitating the processing of each of the plurality of data subject access requests via the workflow;
providing a data subject access request compliance portal;
receiving an audit request, via the data subject access request compliance portal, to audit compliance, by the particular entity with one or more data subject access request requirements, the audit request comprising one or more request parameters;
perform the audit based on the one or more request parameters;
generate a report of one or more results of the audit; and
provide the report to a privacy officer associated with the particular entity. 1. A privacy management computer system for auditing one or more responses to one or more data subject access requests received by a particular entity, the system comprising:
one or more computer processors; and
computer memory operatively coupled to the one or more processors, wherein the one or more computer processors are adapted for:
receiving a plurality of data subject access requests via a plurality of webforms on respective computing devices from a plurality of data subject access requestors;
automatically determining a type of each data subject access request, the determined type of data subject access request being selected from a group consisting of: (1) a request to delete personal data of the requestor that is being stored by a particular organization; (2) a request to provide, to the requestor, personal data of the requestor that is being stored by the particular organization; (3) a request to update personal data of the requestor that is being stored by the particular organization; and (4) a request to opt out of having the particular organization use the requestor's personal information in one or more particular ways;
determining, based at least partially on the determined type of each data subject access request, a workflow that is to be used to process each request;
facilitating the processing of each of the plurality of data subject access requests via the workflow;
providing a data subject access request compliance portal;
receiving an audit request, via the data subject access request compliance portal, to audit compliance, by the particular entity with one or more data subject access request requirements, the audit request comprising one or more request parameters;
perform the audit based on the one or more request parameters;
generate a report of one or more results of the audit; and
provide the report to a privacy officer associated with the particular entity. 2. The privacy management computer system of claim 1, wherein the workflow is a workflow for validating the identity of an individual before the system facilitates completion of the data subject access request. 3. The privacy management computer system of claim 1, wherein the workflow is a workflow that specifies, based at least partially on the determined type of each data subject access request, that the system will facilitate at least a partial manual processing of at least a particular data subject access request of the plurality of data subject access requests. 4. The privacy management computer system of claim 1, wherein:
the one or more data subject access request requirements comprise a respective time constraint for responding to each of the plurality of data subject access requests. 5. The privacy management computer system of claim 4, wherein:
the one or more request parameters comprise one or more parameters related to a timing of the processing of each of the plurality of data subject access requests; and
performing the audit comprises analyzing the timing of the processing of each of the plurality of data subject access requests and comprising the analysis to each respective time constraint. 6. The privacy management computer system of claim 1, wherein:
the one or more request parameters comprise one or more parameters related to a particular group of data subjects; and
performing the audit comprises:
analyzing the plurality of data subject access requestors to identify one or more members of the particular group of data subjects;
identifying particular associated data subject access requests of the plurality of data subject access requests that are associated with the one or more members of the particular group of data subjects; and
analyzing the particular associated data subject access request to determine a compliance level with the one or more data subject access request requirements. 7. The privacy management computer system of claim 1, wherein performing the audit comprises:
analyzing the plurality of data subject access requestors to identify a particular group associated with at least a particular number of the plurality of data subject access requestors; and
generate the report to include the identified particular group. 8. A computer-implemented data processing method for receiving and facilitating the processing of data subject access requests and subsequently auditing a plurality of processed data subject access requests, the method comprising:
receiving, by at least one computer processor, a data subject access request from a data subject access requestor;
automatically determining, by at least one computer processor, a type of the data subject access request, the determined type of data subject access request being selected from a group consisting of: (1) a request to delete personal data of the requestor that is being stored by a particular organization; (2) a request to provide, to the requestor, personal data of the requestor that is being stored by the particular organization; (3) a request to update personal data of the requestor that is being stored by the particular organization; and (4) a request to opt out of having the particular organization use the requestor's personal information in one or more particular ways;
determining, by at least one processor, based at least partially on the determined type of data subject access request, a workflow that is to be used to process the request;
after determining the workflow, facilitating, by at least one processor, the processing of the request via the computer-implemented workflow;
providing a data subject access request compliance portal;
receiving an audit request, via the data subject access request compliance portal, to audit compliance, by the particular organization with one or more data subject access request requirements, the audit request comprising one or more request parameters;
perform the audit based on the one or more request parameters;
generate a report of one or more results of the audit; and
provide the report to a privacy officer associated with the particular organization. 8. A computer-implemented data processing method for receiving and facilitating the processing of data subject access requests and subsequently auditing a plurality of processed data subject access requests, the method comprising:
receiving, by at least one computer processor, a data subject access request from a data subject access requestor;
automatically determining, by at least one computer processor, a type of the data subject access request, the determined type of data subject access request being selected from a group consisting of: (1) a request to delete personal data of the requestor that is being stored by a particular organization; (2) a request to provide, to the requestor, personal data of the requestor that is being stored by the particular organization; (3) a request to update personal data of the requestor that is being stored by the particular organization; and (4) a request to opt out of having the particular organization use the requestor's personal information in one or more particular ways;
determining, by at least one processor, based at least partially on the determined type of data subject access request, a workflow that is to be used to process the request;
after determining the workflow, facilitating, by at least one processor, the processing of the request via the computer-implemented workflow;
providing a data subject access request compliance portal;
receiving an audit request, via the data subject access request compliance portal, to audit compliance, by the particular organization with one or more data subject access request requirements, the audit request comprising one or more request parameters;
perform the audit based on the one or more request parameters;
generate a report of one or more results of the audit; and
provide the report to a privacy officer associated with the particular organization. 9. The computer-implemented data processing method of claim 8, wherein the computer-implemented workflow is a workflow for validating the identity of an individual. 10. The computer-implemented data processing method of claim 9, wherein the computer-implemented method further comprises analyzing a timing of the plurality of processed data subject access requests. 11. The computer-implemented data processing method of claim 10, wherein the one or more data subject access request requirements comprise a respective time constraint for responding to each of the plurality of processed data subject access requests. 12. The computer-implemented data processing method of claim 11, wherein performing the audit comprises analyzing the timing of the processing of each of the plurality of processed data subject access requests and comprising the analysis to each respective time constraint. 13. The computer-implemented data processing method of claim 11, wherein performing the audit comprises analyzing each of the plurality of processed data subject access requests to identify whether any particular data subject access request of the plurality of processed data subject access requests utilized one or more time extensions during processing. 14. The computer-implemented data processing method of claim 13, further comprising:
in response to determining that a particular data subject access request of the plurality of processed data subject access requests utilized the one or more time extensions during processing, modifying the report of one or more results of the audit. 15. The computer-implemented data processing method of claim 8, wherein performing the audit comprises determining, for each of the plurality of processed data subject access requests, whether the processing was substantially automatic or required at least a partial manual processing. 16. The computer-implemented data processing method of claim 8, the method further comprising:
receiving a complaint from the data subject access requestor regarding the processing of the data subject access request; and
in response to receiving the complaint, modifying the report of one or more results of the audit. 17. The computer-implemented data processing method of claim 16, wherein the workflow is a workflow that specifies, based at least partially on the determined type of data subject access request, that the system will facilitate at least a partial manual processing of the data subject access request. 18. The computer-implemented data processing method of claim 17, wherein the workflow is a computer-implemented workflow for automatically deleting at least substantially all of the personal data for the requestor on one or more computer systems of the particular organization. 19. The computer-implemented data processing method of claim 18, wherein performing the audit comprises scanning the one or more computer systems of the particular organization using one or more intelligent identity scanning means to identify at least one piece of personal data associated with the requestor that is stored on the one or more computer systems of the particular organization. 20. The computer-implemented data processing method of claim 8, wherein performing the audit comprises analyzing a plurality of open data subject access requests to determine, for each of the plurality of open data subject access requests, a number of days remaining before a respective deadline for responding.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318780B2,US10318780B2,Encoding and decoding data in two-dimensional symbology,2015-12-14,"method, designated, first, reading, second, encoding, ending, orientation, capturing, implemented, symbology, examples, series, symbols, positioned, data, represented, segment, herein, computer, each, image, decoding, techniques, binary, representing, between, line, differs, extracting, comprise, comprises, string, symbol, indicator, described, starting, disclosed, further, from, dimensional, example","Examples of techniques for encoding data in a 2D symbology are disclosed. In one example described herein, a computer-implemented method comprises capturing an image of the 2D symbology. The 2D symbology comprises a series of data symbols representing a binary string. Each of the data symbols comprises a line segment, and each data symbol in the series of data symbols are positioned in an end-to-end orientation starting at a starting indicator designated by a first symbol and ending at an ending indicator designated by a second symbol. The first symbol differs from the second symbol, and the series of data symbols comprise 0-bit symbols represented by a first data symbol and 1-bit symbols represented by a second data symbol. The method further comprises extracting the binary string from the 2D symbology by reading each of the data symbols between the starting indicator and the ending indicator.","1. A computer-implemented method for decoding data from a two-dimensional (2D) symbology, the method comprising:
capturing an image of the 2D symbology, wherein the 2D symbology comprises a series of data symbols representing a binary string, wherein each of the data symbols comprises a line segment, wherein each data symbol in the series of data symbols are positioned in an end-to-end orientation starting at a starting indicator designated by a first symbol and ending at an ending indicator designated by a second symbol, wherein the first symbol differs from the second symbol, and wherein the series of data symbols comprise 0-bit symbols represented by a first data symbol and 1-bit symbols represented by a second data symbol; and
extracting the binary string from the 2D symbology by reading each of the data symbols between the starting indicator and the ending indicator. 1. A computer-implemented method for decoding data from a two-dimensional (2D) symbology, the method comprising:
capturing an image of the 2D symbology, wherein the 2D symbology comprises a series of data symbols representing a binary string, wherein each of the data symbols comprises a line segment, wherein each data symbol in the series of data symbols are positioned in an end-to-end orientation starting at a starting indicator designated by a first symbol and ending at an ending indicator designated by a second symbol, wherein the first symbol differs from the second symbol, and wherein the series of data symbols comprise 0-bit symbols represented by a first data symbol and 1-bit symbols represented by a second data symbol; and
extracting the binary string from the 2D symbology by reading each of the data symbols between the starting indicator and the ending indicator. 2. The computer-implemented method of claim 1, further comprising recognizing at least one of the starting indicator and the ending indicator, wherein the recognizing further comprises recognizing a connector pair comprising a first connector symbol and a second connector symbol. 3. The computer-implemented method of claim 2, wherein the first connector symbol indicates the end of a first segment and the second connector symbol indicates the start of a second segment. 4. The computer-implemented method of claim 2, wherein the recognizing further comprises recognizing additional connector pairs, each of the connector pairs comprising two connector symbols, wherein the two connector symbols are the same. 5. The computer-implemented method of claim 1, further comprising recognizing an operational symbol. 6. The computer-implemented method of claim 5, wherein the extracting further comprises applying the operational symbol. 7. The computer-implemented method of claim 1, wherein the 2D symbology forms a human-recognizable layout. 8. A system for decoding data from a two-dimensional (2D) symbology, the system comprising:
a processor in communication with one or more types of memory, the processor configured to:
capture an image of the 2D symbology, wherein the 2D symbology comprises a series of data symbols representing a binary string, wherein each of the data symbols comprises a line segment, wherein each data symbol in the series of data symbols are positioned in an end-to-end orientation starting at a starting indicator designated by a first symbol and ending at an ending indicator designated by a second symbol, wherein the first symbol differs from the second symbol, and wherein the series of data symbols comprise 0-bit symbols represented by a first data symbol and 1-bit symbols represented by a second data symbol; and
extract the binary string from the 2D symbology by reading each of the data symbols between the starting indicator and the ending indicator. 8. A system for decoding data from a two-dimensional (2D) symbology, the system comprising:
a processor in communication with one or more types of memory, the processor configured to:
capture an image of the 2D symbology, wherein the 2D symbology comprises a series of data symbols representing a binary string, wherein each of the data symbols comprises a line segment, wherein each data symbol in the series of data symbols are positioned in an end-to-end orientation starting at a starting indicator designated by a first symbol and ending at an ending indicator designated by a second symbol, wherein the first symbol differs from the second symbol, and wherein the series of data symbols comprise 0-bit symbols represented by a first data symbol and 1-bit symbols represented by a second data symbol; and
extract the binary string from the 2D symbology by reading each of the data symbols between the starting indicator and the ending indicator. 9. The system of claim 8, wherein the processor is further configured to recognize at least one of the starting indicator and the ending indicator, wherein the recognizing further comprises recognizing a connector pair comprising a first connector symbol and a second connector symbol. 10. The system of claim 9, wherein the first connector symbol indicates the end of a first segment and the second connector symbol indicates the start of a second segment. 11. The system of claim 9, wherein the recognizing further comprises recognizing additional connector pairs, each of the connector pairs comprising two connector symbols, wherein the two connector symbols are the same. 12. The system of claim 8, wherein the processor is further configured to recognize an operational symbol. 13. The system of claim 12, wherein the extracting further comprises applying the operational symbol. 14. The system of claim 13, wherein the 2D symbology forms a human-recognizable layout. 15. A computer program product for decoding data from a two-dimensional (2D) symbology, the computer program product comprising:
a non-transitory storage medium readable by a processing circuit and storing instructions for execution by the processing circuit for performing a method comprising:
capturing an image of the 2D symbology, wherein the 2D symbology comprises a series of data symbols representing a binary string, wherein each of the data symbols comprises a line segment, wherein each data symbol in the series of data symbols are positioned in an end-to-end orientation starting at a starting indicator designated by a first symbol and ending at an ending indicator designated by a second symbol, wherein the first symbol differs from the second symbol, and wherein the series of data symbols comprise 0-bit symbols represented by a first data symbol and 1-bit symbols represented by a second data symbol; and
extracting the binary string from the 2D symbology by reading each of the data symbols between the starting indicator and the ending indicator. 15. A computer program product for decoding data from a two-dimensional (2D) symbology, the computer program product comprising:
a non-transitory storage medium readable by a processing circuit and storing instructions for execution by the processing circuit for performing a method comprising:
capturing an image of the 2D symbology, wherein the 2D symbology comprises a series of data symbols representing a binary string, wherein each of the data symbols comprises a line segment, wherein each data symbol in the series of data symbols are positioned in an end-to-end orientation starting at a starting indicator designated by a first symbol and ending at an ending indicator designated by a second symbol, wherein the first symbol differs from the second symbol, and wherein the series of data symbols comprise 0-bit symbols represented by a first data symbol and 1-bit symbols represented by a second data symbol; and
extracting the binary string from the 2D symbology by reading each of the data symbols between the starting indicator and the ending indicator. 16. The computer program product of claim 15, wherein the method further comprises recognizing at least one of the starting indicator and the ending indicator, wherein the recognizing further comprises recognizing a connector pair comprising a first connector symbol and a second connector symbol. 17. The computer program product of claim 16, wherein the first connector symbol indicates the end of a first segment and the second connector symbol indicates the start of a second segment. 18. The computer program product of claim 16, wherein the recognizing further comprises recognizing additional connector pairs, each of the connector pairs comprising two connector symbols, wherein the two connector symbols are the same. 19. The computer program product of claim 15, wherein the method further comprises recognizing an operational symbol. 20. The computer program product of claim 19, wherein the extracting further comprises applying the operational symbol.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318779B2,US10318779B2,"Systems and methods for robust protection of item authentication, tracking and tracing against tag duplication",2017-04-28,"duplication, second, arrival, waypoint, validating, detecting, protection, transforming, private, presence, authenticity, comprising, systems, path, data, tracing, chainend, value, with, into, obtaining, validation, entry, authentication, methods, item, using, robust, storing, transaction, record, against, replaced, comprise, hash, chain, public, store, least, facilitating, chainstart, along, supply, from, generating, causing, first, tracking","Systems and methods for facilitating tag authenticity validation. The methods comprise: detecting a tag's arrival/presence at a waypoint along a supply chain path; obtaining a ChainStart Value and a first ChainEnd Value from the tag; generating a public key and a private key; transforming the first ChainEnd Value into a second ChainEnd Value using the private key; storing in a data store at least a hash of the ChainStart Value, a hash of the second ChainEnd Value, and the public key as a transaction record entry; causing the first ChainEnd Value of the tag to be replaced with the second ChainEnd Value; and validating the tag's authenticity at a second waypoint along the supply chain path using at least the ChainStart Value, the second ChainEnd Value, and the transaction record entry comprising the hash of the ChainStart Value, a hash of the first ChainEnd Value and the public key.","1. A method for facilitating tag authenticity validation, comprising:
detecting a tag's arrival or presence at a waypoint along a supply chain path;
obtaining, by a processor, a ChainStart Value and a first ChainEnd Value from the tag;
generating, by the processor, a first key pair comprising a public key and a private key;
transforming, by the processor, the first ChainEnd Value into a second ChainEnd Value using the private key;
storing in a data store at least a hash of the ChainStart Value, a hash of the second ChainEnd Value, and the public key as a transaction record entry;
causing the first ChainEnd Value of the tag to be replaced with the second ChainEnd Value; and
validating the tag's authenticity at a second waypoint along the supply chain path using at least the tag's ChainStart Value, the tag's second ChainEnd Value, and a first transaction record entry comprising the hash of the ChainStart Value, a hash of the second ChainEnd Value and the public key. 1. A method for facilitating tag authenticity validation, comprising:
detecting a tag's arrival or presence at a waypoint along a supply chain path;
obtaining, by a processor, a ChainStart Value and a first ChainEnd Value from the tag;
generating, by the processor, a first key pair comprising a public key and a private key;
transforming, by the processor, the first ChainEnd Value into a second ChainEnd Value using the private key;
storing in a data store at least a hash of the ChainStart Value, a hash of the second ChainEnd Value, and the public key as a transaction record entry;
causing the first ChainEnd Value of the tag to be replaced with the second ChainEnd Value; and
validating the tag's authenticity at a second waypoint along the supply chain path using at least the tag's ChainStart Value, the tag's second ChainEnd Value, and a first transaction record entry comprising the hash of the ChainStart Value, a hash of the second ChainEnd Value and the public key. 2. The method according to claim 1, wherein the processor is disposed in at least one of the tag, a tag reader/writer device, or a computing device remote from the tag and the tag reader/writer device. 3. The method according to claim 1, wherein the second ChainEnd Value comprises the first ChainEnd Value signed with the private key. 4. The method according to claim 3, wherein the validating comprises using the public key to un-sign the second ChainEnd Value to obtain a previous ChainEnd Value. 5. The method according to claim 4, wherein the tag's authenticity is incrementally validated when the previous ChainEnd Value matches the first ChainEnd Value contained in the another transaction record entry. 6. The method according to claim 5, wherein the method is repeated until no further matches are found among one or more transaction record entries creating a chain of linked transaction records. 7. The method according to claim 1, wherein the first waypoint is a first intermediary waypoint along the supply chain path. 8. The method according to claim 7, further comprising:
detecting the tag's arrival or presence at a second intermediary waypoint along the supply chain path;
obtaining the ChainStart Value and the second ChainEnd Value from the tag;
generating a second key pair comprising a public key and a private key;
transforming the second ChainEnd Value into a third ChainEnd Value using the private key of the second key pair;
storing in the data store at least the hash of the ChainStart Value, a hash of the third ChainEnd Value, and the public key of the second key pair as a second another transaction record entry; and
causing the second ChainEnd Value of the tag to be replaced with the third ChainEnd Value. 9. The method according to claim 8, wherein the third ChainEnd Value and the second another transaction record entry are also used to validate the tag's authenticity. 10. The method according to claim 1, wherein the transaction record entry further comprises a transaction block and a hash of the transaction block signed by the first private key. 11. The method according to claim 1, wherein the tag is a wired or wireless communications enabled tag, a printed barcode, an etched barcode, or an allocation of memory in a digital device. 12. A system, comprising:
a processor; and
a non-transitory computer-readable storage medium comprising programming instructions that are configured to cause the processor to implement a method for facilitating tag authenticity validation, wherein the programming instructions comprise instructions to:
detect a tag's arrival or presence at a waypoint along a supply chain path;
obtain a ChainStart Value and a first ChainEnd Value from the tag;
generate a first key pair comprising a public key and a private key;
transform the first ChainEnd Value into a second ChainEnd Value using the private key;
store in a data store at least a hash of the ChainStart Value, a hash of the second ChainEnd Value, and the public key as a transaction record entry;
cause the first ChainEnd Value of the tag to be replaced with the second ChainEnd Value; and
validate the tag's authenticity using at least the tag's ChainStart Value, the tag's second ChainEnd Value, and a first transaction record entry comprising the hash of the ChainStart Value, a hash of the second ChainEnd Value and the public key. 12. A system, comprising:
a processor; and
a non-transitory computer-readable storage medium comprising programming instructions that are configured to cause the processor to implement a method for facilitating tag authenticity validation, wherein the programming instructions comprise instructions to:
detect a tag's arrival or presence at a waypoint along a supply chain path;
obtain a ChainStart Value and a first ChainEnd Value from the tag;
generate a first key pair comprising a public key and a private key;
transform the first ChainEnd Value into a second ChainEnd Value using the private key;
store in a data store at least a hash of the ChainStart Value, a hash of the second ChainEnd Value, and the public key as a transaction record entry;
cause the first ChainEnd Value of the tag to be replaced with the second ChainEnd Value; and
validate the tag's authenticity using at least the tag's ChainStart Value, the tag's second ChainEnd Value, and a first transaction record entry comprising the hash of the ChainStart Value, a hash of the second ChainEnd Value and the public key. 13. The system according to claim 12, wherein the processor is disposed in at least one of the tag, a tag reader/writer device, or a computing device remote from the tag and the tag reader/writer device. 14. The system according to claim 12, wherein the second ChainEnd Value comprises the first ChainEnd Value signed with the private key. 15. The system according to claim 14, wherein the tag's authenticity is incrementally validated by using the public key to un-sign the second ChainEnd Value to obtain a previous ChainEnd Value. 16. The system according to claim 15, wherein the tag's authenticity is validated when the previous ChainEnd Value matches the first ChainEnd Value contained in the another transaction record entry. 17. The system according to claim 16, wherein the method is repeated until no further matches are found among one or more transaction record entries creating a chain of linked transaction records. 18. The system according to claim 12, wherein the first waypoint is a first intermediary waypoint along the supply chain path. 19. The system according to claim 18, wherein the programming instructions comprise instructions to:
detect the tag's arrival or presence at a second intermediary waypoint along the supply chain path;
obtain the ChainStart Value and the second ChainEnd Value from the tag;
generate a second key pair comprising a public key and a private key;
transform the second ChainEnd Value into a third ChainEnd Value using the private key of the second key pair;
store in the data store at least the hash of the ChainStart Value, a hash of the third ChainEnd Value, and the public key of the second pair as a second another transaction record entry; and
cause the second ChainEnd Value of the tag to be replaced with the third ChainEnd Value. 20. The system according to claim 19, wherein the third ChainEnd Value and the second another transaction record entry are also used to validate the tag's authenticity. 21. The system according to claim 12, wherein the transaction record entry further comprises a transaction block and a hash of the transaction block signed by the first private key. 22. The system according to claim 12, wherein the tag is a wired or wireless communications enabled tag, a printed barcode, an etched barcode, or an allocation of memory in a digital device.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318784B2,US10318784B2,Touch panel-sensing chip package module complex and a manufacturing method thereof,2015-06-29,"method, module, invention, this, other, surface, opposite, thereof, wherein, bonded, formed, surrounded, package, cavity, each, with, sensing, provides, layer, touch, scale, bottom, chip, panel, manufacturing, sidewall, having, adjacent, comprising, color, complex, first, wall","This invention provides a touch panel-sensing chip package module complex, comprising: a touch panel with a first top surface and a first bottom surface opposite to each other, wherein the first bottom surface having a first cavity with a bottom wall surrounded by a sidewall; a color layer formed on the bottom wall and the first bottom surface adjacent to the cavity; and a chip scale sensing chip package module bonded to the cavity by the color layer formed on the bottom wall of the cavity.","1. A touch panel-sensing chip package module complex, comprising:
a touch panel with a first top surface and a first bottom surface opposite to each other, wherein the first bottom surface has a first cavity surrounded by a first sidewall;
a first color layer on the first bottom surface adjacent to the first cavity;
a second color layer exposed by the first cavity, wherein when viewed from above in a direction perpendicular to the first top surface of the touch panel, an outer edge of the first color layer surrounds the second color layer, and an inner edge of first color layer is overlapped by the second color layer; and
a chip scale sensing chip package module placed in the first cavity by bonded to the second color layer, wherein the chip scale sensing chip package module comprises:
a chip scale sensing chip package, comprising:
a sensing chip with a second top surface and a second bottom surface opposite to each other, and the sensing chip comprising a sensing device and a plurality of conductive pads formed near the second top surface, and a conductive structure formed near the second bottom surface electrically connected to the conductive pads by a re-distribution layer; and
a cap layer overlay on the second top surface of the sensing chip; and

a circuit board formed under the chip scale sensing chip package and electrically connected to the chip scale sensing chip package by the conductive structure. 1. A touch panel-sensing chip package module complex, comprising:
a touch panel with a first top surface and a first bottom surface opposite to each other, wherein the first bottom surface has a first cavity surrounded by a first sidewall;
a first color layer on the first bottom surface adjacent to the first cavity;
a second color layer exposed by the first cavity, wherein when viewed from above in a direction perpendicular to the first top surface of the touch panel, an outer edge of the first color layer surrounds the second color layer, and an inner edge of first color layer is overlapped by the second color layer; and
a chip scale sensing chip package module placed in the first cavity by bonded to the second color layer, wherein the chip scale sensing chip package module comprises:
a chip scale sensing chip package, comprising:
a sensing chip with a second top surface and a second bottom surface opposite to each other, and the sensing chip comprising a sensing device and a plurality of conductive pads formed near the second top surface, and a conductive structure formed near the second bottom surface electrically connected to the conductive pads by a re-distribution layer; and
a cap layer overlay on the second top surface of the sensing chip; and

a circuit board formed under the chip scale sensing chip package and electrically connected to the chip scale sensing chip package by the conductive structure. 2. The touch panel-sensing chip package module complex as claimed in claim 1, wherein the sensing device is a biometric identification device. 3. The touch panel-sensing chip package module complex as claimed in claim 2, wherein the biometric identification device comprises a fingerprint identification device. 4. The touch panel-sensing chip package module complex as claimed in claim 1, wherein the conductive structure comprises solder balls, solder bumps or conductive pillars. 5. The touch panel-sensing chip package module complex as claimed in claim 1, wherein the circuit board is a rigid-flexible print circuit board. 6. The touch panel-sensing chip package module complex as claimed in claim 5, wherein the rigid-flexible print circuit board further comprises a connector. 7. The touch panel-sensing chip package module complex as claimed in claim 1, further comprising an adhesive layer sandwiched between the second color layer and the chip scale sensing chip package module. 8. The touch panel-sensing chip package module complex as claimed in claim 1, wherein the first top surface has a second cavity with a bottom wall surrounded by a second sidewall, and when viewed from above in the direction perpendicular to the first top surface of the touch panel, the cross-sectional area of the second cavity is greater than that of the first cavity, whereby the first cavity and the second cavity are passed through by an opening, wherein the touch panel-sensing chip package module complex further comprises:
a touch substrate with a third top surface and a third bottom surface opposite to each other, whereby the touch substrate is bonded to the bottom wall of the second cavity, wherein the chip scale sensing chip package module is bonded to the third bottom surface, and the second color layer is between the third bottom surface and the chip scale sensing chip package module and between the bottom wall and the third bottom surface. 9. The touch panel-sensing chip package module complex as claimed in claim 8, wherein the sensing device is a biometric identification device. 10. The touch panel-sensing chip package module complex as claimed in claim 9, wherein the biometric identification device comprises a fingerprint identification device. 11. The touch panel-sensing chip package module complex as claimed in claim 8, wherein the conductive structure comprises solder balls, solder bumps or conductive pillars. 12. The touch panel-sensing chip package module complex as claimed in claim 8, wherein the circuit board is a rigid-flexible print circuit board. 13. The touch panel-sensing chip package module complex as claimed in claim 12, wherein the rigid-flexible print circuit board further comprises a connector. 14. The touch panel-sensing chip package module complex as claimed in claim 8, further comprising a metal holding ring surrounding between an edge of the touch substrate and the sidewall of the second cavity. 15. The touch panel-sensing chip package module complex as claimed in claim 8, further comprising a protective substrate capped on the first top surface of the touch panel and the third top surface of the touch substrate. 16. The touch panel-sensing chip package module complex as claimed in claim 8, further comprising an adhesive layer sandwiched between the second color layer and the chip scale sensing chip package module.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318782B2,US10318782B2,"Image processing device, image processing system, and image processing method",2015-09-07,"method, obtains, associates, second, generated, capturing, device, directions, positions, images, imaging, third, output, devices, position, object, taken, plurality, with, range, surroundings, image, vehicle, running, clipped, includes, common, indicating, include, processing, mounted, detection, rectangle, information, having, target, detected, system, which, from, among, background, first, street","An image processing device obtains first images capturing a street, on which a target vehicle is running, from a plurality of directions, and position information indicating positions at which the first images are taken, and associates a background object in a second image with a background object in a third image. The second image is an image which the target object is detected from and the third image is an image which is taken by an imaging device having a common imaging range with the second image among imaging devices mounted on the target vehicle. An output image is generated to include the position information indicating a position at which the second image is taken, the image of the detection rectangle which is clipped from the second image and includes the target object and the background object, and the first image of surroundings of the target vehicle.","1. An image processing device comprising:
hardware circuitry configured to:
obtain first images capturing a street, on which a target vehicle is running, from a plurality of directions, and position information indicating imaging positions at which the first images are taken;
detect a target object from the first images;
associate an image of a background object included in a second image with an image of the background object included in a third image, the first images including the second image and the third image, the second image being an image from which the target object is detected, the third image being an image which is taken by an imaging device having a common imaging range with the second image among imaging devices installed on the target vehicle;
clip, from the second image, an image of a detection rectangle which includes the target object and the background object; and
generate an output image including the position information indicating a position at which the second image is taken, the image of the detection rectangle, and one of the first images. 1. An image processing device comprising:
hardware circuitry configured to:
obtain first images capturing a street, on which a target vehicle is running, from a plurality of directions, and position information indicating imaging positions at which the first images are taken;
detect a target object from the first images;
associate an image of a background object included in a second image with an image of the background object included in a third image, the first images including the second image and the third image, the second image being an image from which the target object is detected, the third image being an image which is taken by an imaging device having a common imaging range with the second image among imaging devices installed on the target vehicle;
clip, from the second image, an image of a detection rectangle which includes the target object and the background object; and
generate an output image including the position information indicating a position at which the second image is taken, the image of the detection rectangle, and one of the first images. 2. The device according to claim 1, wherein the detection rectangle is certain percentage of a smallest rectangle including the target object entirely and the background object. 3. The image processing device according to claim 1, wherein the hardware circuitry is configured to divide a certain area around the target object in the second image as well as divide the third image using at least either color information or edge information, wherein the hardware circuitry is configured to associate, using at least either color information, or texture information, or area size, areas formed by the dividing of the second image with areas formed by the dividing of the third image, and set an area among mutually-associated areas which has a highest degree of similarity as a background object area. 4. The image processing device according to claim 1, wherein the hardware circuitry is configured to detect an image of the background object from a certain area around the detected target in the second image, wherein
the hardware circuitry is configured to search the third image for the image of the background object, and set an area in the third image which has a highest degree of similarity with the detected image of the background object as a background object area. 5. The image processing device according to claim 1, wherein the hardware circuitry is configured to detect an image of the background object and attribute information of the background object from a certain area around the target object in the second image as well as from the third image, wherein the hardware circuitry is configured to associate images of the background object, which are detected from the second image and the third image, using at least one of color information, texture information, and the attribute information, and set a detection rectangle of the associated images of the background object as a background object area. 6. The image processing device according to claim 1, wherein the hardware circuitry is configured to obtain the first images obtained by an imaging device mounted in a vicinity of the street on which the target vehicle is running and obtained by imaging devices mounted on mobile objects including the running target vehicle, and obtain the position information indicating imaging positions of the first images. 7. The image processing device according to claim 1, wherein, when the hardware circuitry associates a single background object captured within a certain area around the target object and when height and width of a smallest rectangle including an image of the background object associated is smaller than a certain percentage of height and width of the target object, the hardware circuitry is configured to clip a rectangle including the smallest rectangle entirely and the target object entirely as the detection rectangle from the second image. 8. The image processing device according to claim 1, wherein, when the hardware circuitry associates a single background object captured within a certain area around the target object and when height and width of a smallest rectangle including an image of the background object associated is greater than a certain percentage of height and width of the target object, the hardware circuitry is configured to determine whether or not the image of the background object associated represents an image of an object present on road surface, and
the hardware circuitry is configured to, when the image of the background object associated is determined to represent an image of an object present on road surface, clip a rectangle including
an area formed by expansion in horizontal direction of the smallest rectangle, which includes the image of the background object, in a certain percentage from one side of the smallest rectangle that is close in horizontal direction to an in-image position of the target object toward a direction from the smallest rectangle to the target object, and
the target object entirely,

as the detection rectangle from the second image. 9. The image processing device according to claim 1, wherein, when the hardware circuitry associates a single background object captured within a certain area around the target object and when height and width of a smallest rectangle including an image of the background object associated is greater than a certain percentage of height and width of the target object, the hardware circuitry is configured to determine whether or not the image of the background object associated represents an image of an object present on road surface, and
the hardware circuitry is configured to, when the image of the background object associated is determined not to represent an image of an object present on road surface, clip a rectangle including
an area formed by expansion in vertical direction of the smallest rectangle, which includes the image of the background object, in a certain percentage from one side of the smallest rectangle that is close in vertical direction to an in-image position of the target object toward a direction from the smallest rectangle to the target object, and
the target object entirely,

as the detection rectangle from the second image. 10. The image processing device according to claim 7, wherein, when the hardware circuitry associates a plurality of background objects captured within a certain area around the target object, the hardware circuitry is configured to identify, as a single background object, an image of a background object closest to the target object from among the associated images of a plurality of the background objects. 11. The image processing device according to claim 8, wherein, when the hardware circuitry associates a plurality of background objects captured within a certain area around the target object, the hardware circuitry is configured to identify, as a single background object, an image of a background object closest to the target object from among the associated images of a plurality of the background objects. 12. The image processing device according to claim 9, wherein, when the hardware circuitry associates a plurality of background objects captured within a certain area around the target object, the hardware circuitry is configured to identify, as a single background object, an image of background object closest to the target object from among the associated images of a plurality of the background objects. 13. The image processing device according to claim 1, wherein the hardware circuitry is configured to perform wireless communication, and
the hardware circuitry is configured to obtain the first images and the position information using wireless communication. 14. The image processing device according to claim 13, wherein, the hardware circuitry is configured to:
obtain first ones of the first images formed by imaging performed by an imaging device which is mounted on the target vehicle and obtain the position information corresponding to the first ones of the first images,
obtain second ones of the first images formed by imaging performed by an imaging device which is mounted in a surrounding environment of the target vehicle and obtain the position information corresponding to the second ones of the first images using wireless communication, and
obtain third ones of the first images formed by imaging by an imaging device which is mounted on a vehicle different from the target vehicle and obtain the position information corresponding to the third ones of the first images using wireless communication. 15. The image processing device according to claim 13, wherein the hardware circuitry is configured to
perform wireless communication with a server device,
obtain first ones of the first images formed by imaging performed by an imaging device which is mounted on the target vehicle and obtain the position information corresponding to the first ones of the first images, and
obtain second ones of the first images and the position information corresponding to the second ones of the first images from the server device using wireless communication. 16. An image processing system comprising:
the image processing device according to claim 13;
a first communication device; and
a second communication device, wherein
the first communication device is configured to send first ones of the first images taken by an imaging device installed around a street and send the position information corresponding to the first ones of the first images,
the second communication device is configured to send second ones of the first images taken by an imaging device mounted on a vehicle different from the target vehicle and send the position information corresponding to the second ones of the first images, and
the hardware circuitry is configured to;
obtain third ones of the first images formed by imaging performed an imaging device mounted on the target vehicle and obtain the position information corresponding to the first images, and
obtain, via wireless communication, first images and position information sent by at least one of the first communication device and the second communication device. 17. An image processing system comprising:
the image processing device according to claim 13;
a first communication device;
a second communication device; and
a server device, wherein
the first communication device is configured to send first ones of the first images taken by an imaging device installed around the street and send the position information corresponding to the first ones of the first images,
the second communication device is configured to send second ones of ends the first images taken by an imaging device mounted on a vehicle different from the target vehicle and send the position information corresponding to the second ones of the first images,
the server device is configured to obtain first images and tie-position information sent by the first communication device as well as the second communication device, and
the hardware circuitry is configured to;
obtain third ones of the first images formed by imaging performed an imaging device mounted on the target vehicle and obtain the position information corresponding to the third ones of the first images, and
obtain, from the server device via the wireless communication, first images and position information sent by at least one of the first communication device and the second communication device. 18. An image processing method comprising:
obtaining first images capturing a street, on which a target vehicle is running, from a plurality of directions, and position information indicating imaging positions at which the first images are taken;
detecting a target object from the first images;
associating an image of a background object included in a second image with an image of the background object included in a third image, the first images including the second image and the third image, the second image being an image from which the target object is detected, the third image being an image which is taken by an imaging device having a common imaging range with the second image among imaging devices installed on the target vehicle;
clipping, from the second image, an image of a detection rectangle which includes the target object and the background object; and
generating an output image including the position information indicating a position at which the second image is taken, the image of the detection rectangle, and one of the first images. 18. An image processing method comprising:
obtaining first images capturing a street, on which a target vehicle is running, from a plurality of directions, and position information indicating imaging positions at which the first images are taken;
detecting a target object from the first images;
associating an image of a background object included in a second image with an image of the background object included in a third image, the first images including the second image and the third image, the second image being an image from which the target object is detected, the third image being an image which is taken by an imaging device having a common imaging range with the second image among imaging devices installed on the target vehicle;
clipping, from the second image, an image of a detection rectangle which includes the target object and the background object; and
generating an output image including the position information indicating a position at which the second image is taken, the image of the detection rectangle, and one of the first images.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318781B2,US10318781B2,"Two dimensional barcode with dynamic environmental data system, method, and apparatus",2015-03-30,"method, during, reading, product, encoding, digital, coded, recovered, embedded, employed, property, error, within, feature, sensed, physical, records, together, systems, data, augmented, nature, status, combining, chemistry, barcode, with, when, dynamic, state, environmental, methods, apparatus, condition, labeled, utilizing, specified, causes, correction, occurs, sensor, information, change, exposed, system, which, been, preprinted, dimensional, color, modules, biological","Methods, systems, and apparatus for combining preprinted information together with coded sensor information within a two-dimensional barcode. The sensor information may be of an environmental, physical or biological nature, and records a change in status of the environmental or biological condition to which the labeled product has been exposed. A sensor dye chemistry is employed and when a specified condition of the sensed property occurs it causes a change in the color state of sensor dye modules embedded within the sensor-augmented two-dimensional barcode, encoding sensor digital information. Sensor information is recovered utilizing the error-correction feature during barcode reading.","1. A sensor-augmented two-dimensional barcode, comprising:
a substrate;
a two-dimensional error-correcting barcode symbol provided on the substrate, the bar code symbol further comprising a plurality of modules, each module having one of a first color state or a second color state;
a first layer provided on the substrate in a permanent color state; and
a second layer provided on the substrate in a sensor dye module pattern, the sensor dye module pattern containing sensor digital information, the second layer further comprising a sensor dye having a chemistry that is configured, responsive to the occurrence of an environmental, physical or biological condition, to undergo a chemical or physical state change causing a change in the color state of the sensor dye, thereby changing the color state of a subset of the plurality of modules. 1. A sensor-augmented two-dimensional barcode, comprising:
a substrate;
a two-dimensional error-correcting barcode symbol provided on the substrate, the bar code symbol further comprising a plurality of modules, each module having one of a first color state or a second color state;
a first layer provided on the substrate in a permanent color state; and
a second layer provided on the substrate in a sensor dye module pattern, the sensor dye module pattern containing sensor digital information, the second layer further comprising a sensor dye having a chemistry that is configured, responsive to the occurrence of an environmental, physical or biological condition, to undergo a chemical or physical state change causing a change in the color state of the sensor dye, thereby changing the color state of a subset of the plurality of modules. 2. The sensor-augmented two-dimensional barcode symbol of claim 1 wherein the environmental condition is selected from the group consisting of time, temperature, time-temperature product, light, humidity, gas vapor and nuclear radiation. 3. The sensor augmented two-dimensional barcode of claim 2, wherein, the sensor dye permanently changes color state when the environmental condition crosses a threshold value. 4. The sensor-augmented two-dimensional barcode of claim 1, wherein the first layer forms a readable barcode symbol in the symbology of the two-dimensional barcode, and wherein the two-dimensional barcode is readable both prior to and after the color state of a subset of the plurality of modules is changed. 5. The sensor-augmented two-dimensional barcode claim 1 wherein the two-dimensional error-correcting barcode symbol is from the symbology group consisting of Data Matrix, QR Code, Aztec Code, MaxiCode, PDF417 and Dot Code symbologies. 6. The sensor-augmented two-dimensional barcode symbol of claim 1 wherein the two-dimensional error-correcting barcode symbol utilizes Reed-Solomon error correction. 7. The sensor-augmented two-dimensional barcode symbol of claim 1 wherein the sensor dye is initially in a black, white or transparent color state when unactivated and changes to a different color state upon activation. 8. The sensor-augmented two-dimensional barcode symbol of claim 7 wherein the sensor dye permanently changes color state when the specified condition of a sensed property of the environmental, physical, or biological condition is above or below a threshold value. 9. The sensor-augmented two-dimensional barcode symbol of claim 1 wherein the specified condition of a sensed property is a detection of the presence of a biological organism, biological agent or biological toxin. 10. The sensor-augmented two-dimensional barcode of claim 9, wherein a colorimetric immunoassay detects the presence of the biological organism, biological agent or biological toxin. 11. The sensor-augmented two-dimensional barcode symbol of claim 1 wherein the second layer provides sensor digital information. 12. The sensor-augmented two-dimensional barcode symbol of claim 11, wherein the sensor digital information is encoded in an invariant bitmap of the two-dimensional symbol. 13. The sensor-augmented two-dimensional barcode symbol of claim 12, wherein the sensor digital information is encoded as binary encoded sensor data. 14. The sensor-augmented two-dimensional barcode symbol of claim 13, wherein the binary encoded sensor data is in an error correcting code. 15. The sensor augmented two-dimensional barcode of claim 14, wherein the error correcting code is chosen from the group consisting of Hamming Codes, Bose-Chaudhuri-Hocquenghem Codes, Golay Codes, Simplex Codes, Reed-Muller Codes, Fire Codes, Convolutional Codes, and Reed-Solomon Codes. 16. The sensor-augmented two-dimensional barcode symbol of claim 1 wherein the sensor digital information encoded in the sensor dye module pattern is a visual pattern or image. 17. The sensor augmented two-dimensional barcode of claim 1, wherein the modules are square, rectangular, or circular. 18. The sensor augmented two-dimensional barcode of claim 1, wherein the second layer is overprinted on the first layer. 19. The sensor-augmented two-dimensional barcode of claim 1, wherein the sensor dye module pattern has a fixed position, both before and after the change in the color state of the subset of the plurality of modules. 20. The sensor-augmented two-dimensional barcode of claim 1, wherein the modules are at least one of square shaped, rectangular shaped, and circular shaped. 21. An article of manufacture, comprising:
pharmaceutical, biological, or food product;
a container holding the pharmaceutical, biological, or food product; and
a sensor-augmented two-dimensional barcode symbol provided on or in the container, wherein the sensor-augmented two-dimensional barcode symbol includes:
a plurality of modules, each module having one of a first color state or a second color state,
a first layer in a permanent color state, and
a second layer in a sensor dye module pattern, the sensor dye module pattern containing sensor digital information, the second layer further comprising a sensor dye having a chemistry that is configured, responsive to the occurrence of an environmental, physical or biological condition, to undergo a chemical or physical state change causing a change in the color state of the sensor dye, thereby changing the color state of a subset of the plurality of modules. 21. An article of manufacture, comprising:
pharmaceutical, biological, or food product;
a container holding the pharmaceutical, biological, or food product; and
a sensor-augmented two-dimensional barcode symbol provided on or in the container, wherein the sensor-augmented two-dimensional barcode symbol includes:
a plurality of modules, each module having one of a first color state or a second color state,
a first layer in a permanent color state, and
a second layer in a sensor dye module pattern, the sensor dye module pattern containing sensor digital information, the second layer further comprising a sensor dye having a chemistry that is configured, responsive to the occurrence of an environmental, physical or biological condition, to undergo a chemical or physical state change causing a change in the color state of the sensor dye, thereby changing the color state of a subset of the plurality of modules. 22. The article of manufacture of claim 21, wherein the pharmaceutical, biological, or food product is a vaccine, and wherein the sensor-augmented two-dimensional barcode symbol is applied to the outside surface of the container. 23. The article of manufacture of claim 21, wherein the container is a vaccine vial.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318785B2,US10318785B2,"Creation of virtual intersection points on a touchscreen to permit static, non swiping fingerprint user authentication",2016-04-13,"method, greater, intersection, valley, node, detail, creation, detect, than, array, intersections, spacing, providing, improve, permit, swiping, used, when, static, techniques, authentication, operating, fingerprint, user, touch, resolution, also, touchscreen, much, even, grid, ridge, moving, finger, points, virtual, dimensional","A method for operating a two-dimensional touch array by providing virtual grid intersections. The techniques may be used to improve the array resolution. It may also be used to detect fingerprint ridge and valley detail even when the finger is not moving, and when the array node spacing is much greater than the ridge and valley spacing.","1. A method of operating a touch-sensitive grid of transmit and receive electrodes comprising:
for each particular scan in a sequence of scans,
applying a first excitation signal to a first transmit electrode;
applying a second excitation signal to a second transmit electrode, the second excitation signal scaled by an amplitude factor associated with the particular scan in the sequence of scans;
such that an electric field generated adjacent the array in turn depends upon both the first excitation signal and the second excitation signal;
detecting a first response signal from a first receive electrode;
detecting a second response signal from a second receive electrode; and
combining the first and second response signals to provide a corresponding combined response signal for each particular scan,

such that the resulting sequence of combined response signals resulting from the is electric fields generated as a result of the sequence of scans corresponds to responses received at a series of virtual intersection points located between at least two physical intersection points of the grid. 1. A method of operating a touch-sensitive grid of transmit and receive electrodes comprising:
for each particular scan in a sequence of scans,
applying a first excitation signal to a first transmit electrode;
applying a second excitation signal to a second transmit electrode, the second excitation signal scaled by an amplitude factor associated with the particular scan in the sequence of scans;
such that an electric field generated adjacent the array in turn depends upon both the first excitation signal and the second excitation signal;
detecting a first response signal from a first receive electrode;
detecting a second response signal from a second receive electrode; and
combining the first and second response signals to provide a corresponding combined response signal for each particular scan,

such that the resulting sequence of combined response signals resulting from the is electric fields generated as a result of the sequence of scans corresponds to responses received at a series of virtual intersection points located between at least two physical intersection points of the grid. 2. The method of claim 1 wherein the second excitation signal is out of phase with the first excitation signal. 3. The method of claim 1 wherein the first excitation signal and second excitation signal are simultaneously applied to the respective first and second transmit electrodes. 4. The method of claim 1 wherein the grid is a two-dimensional array comprising a set of parallel transmit electrodes located along a first axis in a first plane, and a set of parallel receive electrodes located along a second axis in a second plane, and the intersection points of the grid are located adjacent where the transmit and receive electrodes cross. 5. The method of claim 1 wherein the grid is a mutual capacitive sensor array. 6. The method of claim 1 wherein the grid is a sparse grid such that the spacing between adjacent receive and transmit electrodes is a least ten times greater than a spacing between a ridge and valley of a fingerprint. 7. The method of claim 1 wherein the corresponding combined response signals depend on (A+B×coeff)−abs(A−B×coeff) where A is the first excitation signal and B is the second excitation signal, and coeff is the amplitude factor applied to signal B for the particular scan.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318787B2,US10318787B2,"Fingerprint photocurrent detection unit, fingerprint identifier, driving method and display device",2016-09-18,"method, reading, disclosure, device, configured, display, present, convert, detect, identifier, accordance, acquired, acquire, with, into, driving, provides, fingerprint, includes, unit, line, photocurrent, frequency, about, detection, information, signal, square, coupled, conversion, circuit, wave","The present disclosure provides a fingerprint photocurrent detection unit, a fingerprint identifier, a driving method and a display device. The fingerprint photocurrent detection unit includes: a conversion circuit coupled to a fingerprint photocurrent reading line and configured to convert a fingerprint photocurrent acquired by the fingerprint photocurrent reading line into a square wave signal; and a detection circuit coupled to the conversion circuit and configured to detect the square wave signal and acquire information about a fingerprint photocurrent in accordance with a frequency of the square wave signal.","1. A fingerprint photocurrent detection unit, comprising:
a fingerprint photocurrent reading line;
a conversion circuit coupled to the fingerprint photocurrent reading line and configured to convert a fingerprint photocurrent acquired by the fingerprint photocurrent reading line into a square wave signal; and
a detection circuit coupled to the conversion circuit and configured to detect the square wave signal and acquire information about the fingerprint photocurrent in accordance with a frequency of the square wave signal,
wherein the conversion circuit comprises a detection control module, a first switching module, a second switching module, a phase inversion module and a storage module;
a control end of the first switching module is coupled to the detection control module and a first node, a first end of the first switching module is coupled to a high level line, and a second end of the first switching module is coupled to a control end of the second switching module; wherein the first switching module is configured to be turned on in the case that a potential at the first node is a first level, and turned off in the case that the potential at the first node is a second level;
a first end of the storage module is coupled to the second end of the first switching module, and a second end of the storage module is coupled to a low level line;
the control end of the second switching module is further coupled to the fingerprint photocurrent reading line and a second node, a first end of the second switching module is coupled to an output end of the phase inversion module, and a second end of the second switching module is coupled to an input end of the phase inversion module; wherein the second switching module is configured to be turned on in the case that a potential at the second node is a low level, and turned off in the case that the potential at the second node is a high level;
the output end of the phase inversion module is further coupled to a third node, and the phase inversion module is configured to invert a phase of a level applied to its input end and output a signal with the inverted phase;
the detection control module is coupled to the first node and the output end of the phase inversion module, and configured to, in the case that a detection operation starts and the signal from the output end of the phase inversion module is at a rising or falling edge, control the potential at the first node to be the first level, in the case that the potential at the first node is maintained as the first level for a predetermined time period, control the potential at the first node to jump to the second level, and in the case that the detection operation starts, control the potential at the third node to be the high level or the low level; and
the detection circuit is coupled to at least one of the output end of the phase inversion module and the first node, and further configured to detect at least one of a frequency of a waveform of a potential at the output end of the phase inversion module and a frequency of a waveform of the potential at the first node, and acquire the information about the fingerprint photocurrent in accordance with the frequency of the waveform of the potential at the output end of the phase inversion module and the frequency of the waveform of the potential at the first node. 1. A fingerprint photocurrent detection unit, comprising:
a fingerprint photocurrent reading line;
a conversion circuit coupled to the fingerprint photocurrent reading line and configured to convert a fingerprint photocurrent acquired by the fingerprint photocurrent reading line into a square wave signal; and
a detection circuit coupled to the conversion circuit and configured to detect the square wave signal and acquire information about the fingerprint photocurrent in accordance with a frequency of the square wave signal,
wherein the conversion circuit comprises a detection control module, a first switching module, a second switching module, a phase inversion module and a storage module;
a control end of the first switching module is coupled to the detection control module and a first node, a first end of the first switching module is coupled to a high level line, and a second end of the first switching module is coupled to a control end of the second switching module; wherein the first switching module is configured to be turned on in the case that a potential at the first node is a first level, and turned off in the case that the potential at the first node is a second level;
a first end of the storage module is coupled to the second end of the first switching module, and a second end of the storage module is coupled to a low level line;
the control end of the second switching module is further coupled to the fingerprint photocurrent reading line and a second node, a first end of the second switching module is coupled to an output end of the phase inversion module, and a second end of the second switching module is coupled to an input end of the phase inversion module; wherein the second switching module is configured to be turned on in the case that a potential at the second node is a low level, and turned off in the case that the potential at the second node is a high level;
the output end of the phase inversion module is further coupled to a third node, and the phase inversion module is configured to invert a phase of a level applied to its input end and output a signal with the inverted phase;
the detection control module is coupled to the first node and the output end of the phase inversion module, and configured to, in the case that a detection operation starts and the signal from the output end of the phase inversion module is at a rising or falling edge, control the potential at the first node to be the first level, in the case that the potential at the first node is maintained as the first level for a predetermined time period, control the potential at the first node to jump to the second level, and in the case that the detection operation starts, control the potential at the third node to be the high level or the low level; and
the detection circuit is coupled to at least one of the output end of the phase inversion module and the first node, and further configured to detect at least one of a frequency of a waveform of a potential at the output end of the phase inversion module and a frequency of a waveform of the potential at the first node, and acquire the information about the fingerprint photocurrent in accordance with the frequency of the waveform of the potential at the output end of the phase inversion module and the frequency of the waveform of the potential at the first node. 2. The fingerprint photocurrent detection unit according to claim 1, wherein the first switching module comprises a first switching transistor, a gate electrode of which is coupled to the detection control module and the first node, a first electrode of which is coupled to the high level line, and a second electrode of which is coupled to the second node;
the second switching module comprises a second switching transistor, a, gate electrode of which is coupled to the fingerprint photocurrent reading line and the second node, a first electrode of which is coupled to the output end of the phase inversion module, and a second electrode of which is coupled to the input end of the phase inversion module;
the storage module comprises a storage capacitor, a first end of which is coupled to the second node, and a second end of which is coupled to the low level line;
the phase inversion module comprises A phase inverters coupled in series to each other, where A is an odd number and a positive integer;
an output end of an ath phase inverter is coupled to an input end of an (a+1)th phase inverter, where a is an integer greater than or equal to 1 and smaller than A; and
an input end of a first phase inverter is the input end of the phase inversion module, and an output end of an Ath phase inverter is the output end of the phase inversion module. 3. The fingerprint photocurrent detection unit according to claim 2, wherein the first switching transistor is an n-type transistor, the second switching transistor is a p-type transistor, the first level is a high level, and the second level is a low level. 4. The fingerprint photocurrent detection unit according to claim 2, wherein the first switching transistor and the second switching transistor are p-type transistors, the first level is a low level, and the second level is a high level. 5. The fingerprint photocurrent detection unit according to claim 2, wherein the detection control module comprises an edge trigger and a signal controller;
an edge triggering end of the edge trigger is coupled to the output end of the phase inversion module, and an output end of the edge trigger is coupled to the first node;
the edge trigger is configured to, in the case that the detection operation starts, control the potential at the third node to be the high level or low level, and in the case that the signal from the output end of the phase inversion module is at the rising edge or falling edge, control the potential at the first node to be the first level; and
the signal controller is coupled to the first node, and configured to, in the case that the detection operation starts, control the potential at the first node to be the first level, and in the case that the potential at the first node is maintained as the first level for the predetermined time period, control the potential at the first node to jump to the second level. 6. A method for driving the fingerprint photocurrent detection unit according to claim 1, comprising steps of:
at a charging stage, controlling, by a detection control module, a potential at the first node to be a first level and a potential at a third node to be a high level or a low level, and controlling, by a first switching module, a high level line to be electrically coupled to a second node to charge a storage module and pull up a potential at the second node to the high level; and
at a discharging stage, controlling, by the detection control module, the potential at the first node to jump to a second level, controlling, by the first switching module, the high level line to be electrically decoupled from the second node to enable a fingerprint photocurrent to flow from the second node, through a fingerprint photocurrent reading line and toward a reversely-biased photosensitive diode of a fingerprint touch unit and pull down the potential at the second node gradually until the second switching module is turned on, controlling, by a phase conversion module, the potential at the third node to jump to the low level or high level, controlling, by the detection control module, the potential at the first node to be reset as the first level, and acquiring, by a detection circuit, corresponding information about a fingerprint photocurrent in accordance with at least one of a frequency of a waveform of a potential at an output end of the phase inversion module and a frequency of a waveform of the potential at the first node. 7. A fingerprint identifier, comprising the fingerprint photocurrent detection unit according to claim 1, wherein the conversion circuit of the fingerprint photocurrent detection unit is coupled to the fingerprint photocurrent reading lines. 8. The fingerprint identifier according to claim 7, wherein the fingerprint photocurrent reading lines are arranged in m columns, and the fingerprint identifier further comprises fingerprint sensing units arranged in n rows and m columns, where n and m are both positive integers;
each fingerprint sensing unit comprises a reading control transistor and a photosensitive diode;
an anode of the photosensitive diode is coupled to a low level output end, and a cathode thereof is coupled to a first electrode of the reading control transistor; and
gate electrodes of the reading control transistors of the fingerprint sensing units in each row are coupled to a corresponding gate line, and second electrodes of the reading control transistors of the fingerprint sensing units in each column are coupled to a corresponding fingerprint photocurrent reading line. 9. The fingerprint identifier according to claim 8, further comprising a multiplexer, an input end of which is coupled to the fingerprint photocurrent reading lines arranged in m columns, an output end of which is coupled to a control end of a second switching module of the fingerprint photocurrent detection unit, and a control end of which is coupled to a multiplex control line, wherein the multiplexer is configured to control the fingerprint photocurrent reading lines arranged in m columns to be electrically coupled to the control end of the second switching module in a time-division manner under the control of a multiplex control signal from the multiplex control line. 10. A method for driving the fingerprint identifier according to claim 9, comprising:
a connection step of controlling, by a multiplexer, fingerprint photocurrent reading lines arranged in m columns to be electrically coupled to a control end of a second switching module in a time-division manner under the control of a multiplex control signal; and
a fingerprint photocurrent detection step of detecting, by a fingerprint photocurrent detection unit, information about a fingerprint photocurrent from the fingerprint photocurrent reading line coupled to the fingerprint photocurrent detection unit. 11. A display device, comprising a display substrate and the fingerprint identifier according to claim 7 arranged on the display substrate. 12. The display device according to claim 11, wherein the display substrate is a Low Temperature Poly-Silicon (LTPS) display substrate, and the fingerprint identifier is arranged on the display substrate through an LTPS process. 13. The display device according to claim 11, further comprising N groups fingerprint photocurrent reading lines arranged at an active display region of the display substrate, wherein
each group of fingerprint photocurrent reading lines comprise the fingerprint photocurrent reading lines arranged in m columns, where m and N are both positive integers; and
there are N fingerprint identifiers, and each fingerprint identifier is coupled to the fingerprint photocurrent reading lines arranged in m columns. 14. The fingerprint identifier according to claim 7, wherein the first switching module comprises a first switching transistor, a gate electrode of which is coupled to the detection control module and the first node, a first electrode of which is coupled to the high level line, and a second electrode of which is coupled to the second node;
the second switching module comprises a second switching transistor, a, gate electrode of which is coupled to the fingerprint photocurrent reading line and the second node, a first electrode of which is coupled to the output end of the phase inversion module, and a second electrode of which is coupled to the input end of the phase inversion module;
the storage module comprises a storage capacitor, a first end of which is coupled to the second node, and a second end of which is coupled to the low level line;
the phase inversion module comprises A phase inverters coupled in series to each other, where A is an odd number and a positive integer;
an output end of an ath phase inverter is coupled to an input end of an (a+1)th phase inverter, where a is an integer greater than or equal to 1 and smaller than A; and
an input end of a first phase inverter is the input end of the phase inversion module, and an output end of an Ath phase inverter is the output end of the phase inversion module. 15. The fingerprint identifier according to claim 14, wherein the first switching transistor is an n-type transistor, the second switching transistor is a p-type transistor, the first level is a high level, and the second level is a low level. 16. The fingerprint identifier according to claim 14, wherein the first switching transistor and the second switching transistor are p-type transistors, the first level is a low level, and the second level is a high level. 17. The fingerprint identifier according to claim 14, wherein the detection control circuit comprises an edge trigger and a signal controller;
an edge triggering end of the edge trigger is coupled to the output end of the phase inversion module, and an output end of the edge trigger is coupled to the first node;
the edge trigger is configured to, in the case that the detection operation starts, control the potential at the third node to be the high level or low level, and in the case that the signal from the output end of the phase inversion circuit is at the rising edge or falling edge, control the potential at the first node to be the first level; and
the signal controller is coupled to the first node, and configured to, in the case that the detection operation starts, control the potential at the first node to be the first level, and in the case that the potential at the first node is maintained as the first level for the predetermined time period, control the potential at the first node to jump to the second level.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318783B2,US10318783B2,Mobile terminal,,"invention, second, surface, present, including, button, covering, capable, input, case, through, wherein, formed, accommodated, side, body, fixing, inputting, includes, relates, terminal, unit, being, provided, rear, externally, sensor, inner, function, exposed, coupled, recess, outwardly, finger, scan, which, member, mobile, first","The present invention relates to a mobile terminal, which includes a case covering a rear surface of a terminal body, a rear input unit provided at an inner side of the case, externally exposed through the case, and including a first button unit for inputting a first function, and a second button unit for inputting a second function, wherein the first button unit is externally exposed through the second button unit, and a fixing member provided at an inner side of the rear input unit, and capable of fixing the first button unit by being accommodated in a recess outwardly formed in the second button unit, wherein the fixing member is coupled to the second button unit and a finger scan sensor is provided on the first button unit.","1. A mobile terminal, comprising:
a case covering a rear surface of a terminal body;
a rear input unit provided at an inner side of the case, externally exposed through the case, and including a first button unit for inputting a first function, and a second button unit for inputting a second function,
wherein the first button unit is externally exposed through the second button unit; and
a fixing member provided at an inner side of the rear input unit, and capable of fixing the first button unit by being accommodated in a recess outwardly formed in the second button unit,
a first flexible printed circuit board provided on a rear surface of the first button unit and capable of controlling a finger scan sensor connected to the first flexible printed circuit board;
a second flexible printed circuit board provided in the fixing member spaced apart from the first flexible printed circuit board;
wherein the fixing member is coupled to the second button unit and a finger scan sensor is provided on the first button unit,
wherein the fixing member is elastic to transfer the external force to first and second dome switches to perform the first and second functions in response to the first and second button units being pushed, and
wherein the fixing member has a groove formed at one side of the fixing member and a connecting portion of the first flexible printed circuit board connecting to a connector externally extends through a groove. 1. A mobile terminal, comprising:
a case covering a rear surface of a terminal body;
a rear input unit provided at an inner side of the case, externally exposed through the case, and including a first button unit for inputting a first function, and a second button unit for inputting a second function,
wherein the first button unit is externally exposed through the second button unit; and
a fixing member provided at an inner side of the rear input unit, and capable of fixing the first button unit by being accommodated in a recess outwardly formed in the second button unit,
a first flexible printed circuit board provided on a rear surface of the first button unit and capable of controlling a finger scan sensor connected to the first flexible printed circuit board;
a second flexible printed circuit board provided in the fixing member spaced apart from the first flexible printed circuit board;
wherein the fixing member is coupled to the second button unit and a finger scan sensor is provided on the first button unit,
wherein the fixing member is elastic to transfer the external force to first and second dome switches to perform the first and second functions in response to the first and second button units being pushed, and
wherein the fixing member has a groove formed at one side of the fixing member and a connecting portion of the first flexible printed circuit board connecting to a connector externally extends through a groove. 2. The terminal of claim 1, wherein
the second flexible printed circuit board having first and second dome switches to perform the first and second functions in response to the first and second button units being pushed. 3. The terminal of claim 2, wherein the fixing member is provided with slits dividing points corresponding to the first and second button units. 4. The terminal of claim 2, further comprising:
a decoration member provided at an inner side of the case, externally exposed through the case, and having a through hole, through which the rear input unit is externally exposed; and
a supporting member provided at an inner side of the decoration member to cover the inner side of the decoration member,
wherein the supporting member is arranged to be flush with an inner side surface of the case. 5. The terminal of claim 4, wherein the case is provided with a plurality of hooks protruding inwardly, and the supporting member supports the decoration member by virtue of the hooks. 6. The terminal of claim 5, wherein a plurality of guides are formed at an edge of the decoration member in a protruding manner, and guide holes are formed at the supporting member at positions corresponding to the guides such that the guides are inserted therein when coupling the decoration member and the supporting member to each other. 7. The terminal of claim 4, wherein the supporting member is made of a metal material. 8. The terminal of claim 4, wherein the decoration member and the second button unit are provided therein with grooves, respectively, through which the first flexible printed circuit board is inserted. 9. The terminal of claim 1, wherein the fixing member is made of rubber with elasticity. 10. A mobile terminal, comprising:
a case covering a rear surface of a terminal body;
a rear input unit provided at an inner side of the case, externally exposed through the case, and including a first button unit for inputting a first function, and a second button unit for inputting a second function, wherein the first button unit is externally exposed through the second button unit;
first and second dome switches provided at an inner side of the rear input unit, and located at positions corresponding to the first and second button units so as to perform the first and second functions in response to the first and second button units being pushed; and
a flexible printed circuit board having the first and second dome switches,
wherein the flexible printed circuit board comprises a first flexible printed circuit board having the first dome switch, and a second flexible printed circuit board having the second dome switches,
wherein the first flexible printed circuit board is located at an inner space of the second flexible printed circuit board and is arranged on the same plane as the second flexible printed circuit board,
wherein the mobile terminal further comprises a fixing member provided at an inner side of the flexible printed circuit board and capable of supporting the flexible printed circuit board, and
wherein the first and second flexible printed circuit boards are formed separate from each other, the first dome switch is formed toward the fixing member, and the second dome switches are formed toward the second button unit. 10. A mobile terminal, comprising:
a case covering a rear surface of a terminal body;
a rear input unit provided at an inner side of the case, externally exposed through the case, and including a first button unit for inputting a first function, and a second button unit for inputting a second function, wherein the first button unit is externally exposed through the second button unit;
first and second dome switches provided at an inner side of the rear input unit, and located at positions corresponding to the first and second button units so as to perform the first and second functions in response to the first and second button units being pushed; and
a flexible printed circuit board having the first and second dome switches,
wherein the flexible printed circuit board comprises a first flexible printed circuit board having the first dome switch, and a second flexible printed circuit board having the second dome switches,
wherein the first flexible printed circuit board is located at an inner space of the second flexible printed circuit board and is arranged on the same plane as the second flexible printed circuit board,
wherein the mobile terminal further comprises a fixing member provided at an inner side of the flexible printed circuit board and capable of supporting the flexible printed circuit board, and
wherein the first and second flexible printed circuit boards are formed separate from each other, the first dome switch is formed toward the fixing member, and the second dome switches are formed toward the second button unit. 11. The terminal of claim 10, wherein the second button unit comprises a finger scan sensor. 12. The terminal of claim 10, wherein the first and second flexible printed circuit boards are integrally formed with each other, and
wherein the first and second dome switches protrude toward the supporting member. 13. The terminal of claim 12, wherein the first and second flexible printed circuit boards are electrically connected to each other, and a connector is formed at one side of the flexible printed circuit board and connected to a main printed circuit board. 14. The terminal of claim 10, wherein the first flexible printed circuit board is connected with a first connector, and the second flexible printed circuit board is connected with a second connector, the first and second connectors being electrically connected to a main printed circuit board.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318788B2,US10318788B2,"Fingerprint identification detection circuit, touch screen and display device",,"thereby, precision, second, device, screen, light, display, polycrystalline, voltage, case, wherein, intensity, improve, oxide, switch, follower, using, metal, fingerprint, there, include, touch, transistor, photocurrent, detection, both, current, identification, even, little, diode, disclosed, silicon, leaking, temperature, detected, photosensitive, which, from, circuit, first","A fingerprint identification detection circuit, a touch screen and a display device is disclosed. The fingerprint identification detection circuit can include a photosensitive diode, a first switch transistor, a second switch transistor and a voltage follower transistor, wherein the first switch transistor can include a metal oxide transistor, and the second switch transistor and the voltage follower transistor both can include a low-temperature polycrystalline silicon transistor. There is little current leaking from the first switch transistor by using the metal oxide transistor as the first switch transistor. Even in the case of low light intensity, photocurrent can be detected, which may thereby improve the detection precision of the fingerprint identification detection circuit.","1. A fingerprint identification detection circuit, comprising a photosensitive diode, a first switch transistor, a voltage follower a second switch transistor and a voltage follower transistor, wherein
a gate of the first switch transistor is connected with a reset control terminal, a first electrode of the first switch transistor is connected with a first reference voltage terminal, and a second electrode of the first switch transistor is connected with a first node;
a gate of the second switch transistor is connected with a scanning signal terminal of the fingerprint identification detection circuit, a first electrode of the second switch transistor is connected with a second electrode of the voltage follower transistor, and a second electrode of the second switch transistor is connected with a signal output terminal of the fingerprint identification detection circuit;
an input terminal of the voltage follower is connected with the first node, the voltage follower is configured to output a voltage at the first node via its output terminal a gate of the voltage follower transistor is connected with the first node, and a first electrode of the voltage follower transistor is connected with a second reference voltage terminal; an anode of the photosensitive diode is connected with a third reference voltage terminal, and a cathode of the photosensitive diode is connected with the first node; and
the first switch transistor comprises a metal oxide transistor, and the second switch transistor and the voltage follower transistor both comprise a low-temperature polycrystalline silicon transistor. 1. A fingerprint identification detection circuit, comprising a photosensitive diode, a first switch transistor, a voltage follower a second switch transistor and a voltage follower transistor, wherein
a gate of the first switch transistor is connected with a reset control terminal, a first electrode of the first switch transistor is connected with a first reference voltage terminal, and a second electrode of the first switch transistor is connected with a first node;
a gate of the second switch transistor is connected with a scanning signal terminal of the fingerprint identification detection circuit, a first electrode of the second switch transistor is connected with a second electrode of the voltage follower transistor, and a second electrode of the second switch transistor is connected with a signal output terminal of the fingerprint identification detection circuit;
an input terminal of the voltage follower is connected with the first node, the voltage follower is configured to output a voltage at the first node via its output terminal a gate of the voltage follower transistor is connected with the first node, and a first electrode of the voltage follower transistor is connected with a second reference voltage terminal; an anode of the photosensitive diode is connected with a third reference voltage terminal, and a cathode of the photosensitive diode is connected with the first node; and
the first switch transistor comprises a metal oxide transistor, and the second switch transistor and the voltage follower transistor both comprise a low-temperature polycrystalline silicon transistor. 2. The fingerprint identification detection circuit according to claim 1, further comprising a third switch transistor, wherein
the third switch transistor comprise a metal oxide transistor;
a gate of the third switch transistor is connected with a compensation signal terminal, a first electrode of the third switch transistor is connected with the second electrode of the first switch transistor, a second electrode of the third switch transistor is connected with the first node, and the first and second electrodes of the third switch transistor are connected to each other; and
the potential of the reset control terminal is inverted from the potential of the compensation signal terminal. 3. The fingerprint identification detection circuit according to claim 2, wherein the ratio between the aspect ratio of the first switch transistor and the aspect ratio of the third switch transistor is 2:1. 4. The fingerprint identification detection circuit according to claim 2, wherein it further comprises an inverter a terminal of which is connected to the reset control terminal, and the compensation signal terminal is connected to the other terminal of the inverter. 5. The fingerprint identification detection circuit according to claim 2, wherein films of the same function in the first switch transistor and the third switch transistor can be arranged in a same layer. 6. A touch screen, comprising a plurality of fingerprint identification detection circuits according to claim 1. 7. A display device, comprising the touch screen according to claim 6. 8. The fingerprint identification detection circuit according to claim 1, further comprising an output switching circuit,
wherein an input terminal of the output switching circuit is connected with the output terminal of the voltage follower,
a control terminal of the output switching circuit is connected with a scanning signal terminal of the fingerprint identification detection circuit and receives therefrom a scanning signal for controlling the switch-on or switch-off of the output switching circuit, and
an output terminal of the output switching circuit is connected with a signal output terminal of the fingerprint identification detection circuit. 9. The fingerprint identification detection circuit according to claim 8, wherein the voltage follower comprises a voltage follower transistor, a gate of the voltage follower transistor acts as the input terminal of the voltage follower, a first electrode of the voltage follower transistor is connected with a second reference voltage terminal, and the second electrode of the voltage follower transistor acts as the output terminal of the voltage follower. 10. The fingerprint identification detection circuit according to claim 9, wherein films of the same function in the second switch transistor and the voltage follower transistor can be arranged in a same layer. 11. The fingerprint identification detection circuit according to claim 9, wherein the first electrode of the second switch transistor and the second electrode of the voltage follower transistor can be formed of a same electrode. 12. The fingerprint identification detection circuit according to claim 9, wherein the gate of the first switch transistor and the gate of the second switch transistor can be arranged in a same layer; and/or
the first electrode of the first switch transistor and the first electrode of the second switch transistor can be arranged in a same layer. 13. The fingerprint identification detection circuit according to claim 9, wherein the first reference voltage terminal and the second reference voltage terminal are the same voltage terminal. 14. The fingerprint identification detection circuit according to claim 9, wherein the output switching circuit comprises a second switch transistor, a gate of the second switch transistor acts as the control terminal of the output switching circuit, a first electrode of the second switch transistor acts as the input terminal of the output switching circuit, and a second electrode of the second switch transistor acts as the output terminal of the output switching circuit. 15. The fingerprint identification detection circuit according to claim 14, wherein the first switch transistor is located at a side of the second switch transistor away from the substrate. 16. The fingerprint identification detection circuit according to claim 14, wherein the second switch transistor and the voltage follower transistor both comprise a low-temperature polycrystalline silicon transistor. 17. The touch screen according to claim 6, wherein the plurality of fingerprint identification detection circuits are arranged in rows and columns, a first scanning line connected with the reset control terminal of the fingerprint identification detection circuit in each row respectively, a second scanning line connected with the scanning signal terminal of the fingerprint identification detection circuit in each row respectively, and a readout line connected with the signal output terminal of the fingerprint identification detection circuit in each column respectively. 18. The touch screen according to claim 17, further comprising a current source and a signal processing circuit, wherein a first terminal of the current source is connected with the signal output terminal of the fingerprint identification detection circuit and the signal processing circuit through the readout line, and a second terminal of the current source is connected with a fourth reference voltage terminal; and
the signal processing circuit is configured to identify fingerprints according to a signal outputted from the signal output terminal of the fingerprint identification detection circuit. 19. The touch screen according to claim 17, wherein the first scanning line and the second scanning line Scan are respectively located on two sides of the fingerprint identification detection circuits in each row. 20. The touch screen according to claim 17, further comprising a gate line for each row in the row direction, wherein the gate line is re-used as the first scanning line or the second scanning line.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318789B2,US10318789B2,Display device and manufacturing method thereof,,"method, device, display, defined, recognition, surface, thereof, groove, faces, cover, with, fingerprint, includes, that, sensor, panel, manufacturing, least, glass","A display device includes a display panel, a fingerprint recognition sensor on the display panel, and a cover glass on the fingerprint recognition sensor. The cover glass is defined with at least one groove on a surface that faces the display panel.","1. A display device, comprising:
a display panel;
a fingerprint recognition sensor on the display panel; and
a cover glass on the fingerprint recognition sensor,
wherein:
the cover glass is defined with at least one groove on a surface that faces the display panel; and
the groove has an arch-shaped cross-section having a continuous curvature such that a thickness of the cover glass decreases in a gradual manner from an edge of the groove to a central portion of the groove. 1. A display device, comprising:
a display panel;
a fingerprint recognition sensor on the display panel; and
a cover glass on the fingerprint recognition sensor,
wherein:
the cover glass is defined with at least one groove on a surface that faces the display panel; and
the groove has an arch-shaped cross-section having a continuous curvature such that a thickness of the cover glass decreases in a gradual manner from an edge of the groove to a central portion of the groove. 2. The display device of claim 1, wherein the fingerprint recognition sensor is accommodated in the groove. 3. The display device of claim 1, wherein the cover glass has a greater residual stress within the groove. 4. The display device of claim 3, wherein the groove of the cover glass has a residual stress in a range of 1 MPa to 10 MPa. 5. The display device of claim 1, wherein the cover glass has a minimum thickness at a central portion of the groove, and the minimum thickness is from 5 percent (%) to 95% of a thickness of the cover glass. 6. The display device of claim 5, wherein the minimum thickness is from 5% to 30% of the thickness of the cover glass. 7. The display device of claim 2, further comprising an adhesive member between the groove and the fingerprint recognition sensor. 8. The display device of claim 7, further comprising a high dielectric material between the groove and the adhesive member. 9. The display device of claim 8, wherein the high dielectric material comprises at least one material selected from the group consisting of low melting glass (LMG), ZnO, Al2O3, Ta2O5, TiO2, ZrO2, La2O3, and Y2O3. 10. The display device of claim 8, wherein the high dielectric material has a thickness in a range of 10 μm to 100 μm. 11. A method of manufacturing a display device, the method comprising:
defining a groove in a cover glass;
accommodating a fingerprint recognition sensor in the groove; and
coupling the cover glass and a display panel,
wherein:
the defining of the groove in the cover glass comprises:
contacting a heat source with a portion of the cover glass to be defined with the groove;
moving the heat source at a predetermined speed to chamfer the cover glass; and
cooling the cover glass rapidly;

the defining of the groove in the cover glass further comprises disposing a high dielectric material in the groove; and
the high dielectric material is LMG comprising material selected from the group consisting of selenium, potassium, arsenic, and sulfur. 11. A method of manufacturing a display device, the method comprising:
defining a groove in a cover glass;
accommodating a fingerprint recognition sensor in the groove; and
coupling the cover glass and a display panel,
wherein:
the defining of the groove in the cover glass comprises:
contacting a heat source with a portion of the cover glass to be defined with the groove;
moving the heat source at a predetermined speed to chamfer the cover glass; and
cooling the cover glass rapidly;

the defining of the groove in the cover glass further comprises disposing a high dielectric material in the groove; and
the high dielectric material is LMG comprising material selected from the group consisting of selenium, potassium, arsenic, and sulfur. 12. The method of claim 11, wherein the cover glass has a greater residual stress within the groove. 13. The method of claim 12, wherein the groove of the cover glass has a residual stress in a range of 1 Mpa to 10 Mpa. 14. The method of claim 11, wherein the cover glass has a minimum thickness at a central portion of the groove, and the minimum thickness is in a range from 5% to 95% of a thickness of the cover glass. 15. The method of claim 14, wherein the minimum thickness is in a range from 5% to 30% of the thickness of the cover glass. 16. The method of claim 11, wherein the heat source has a temperature in a range from 1200° C. to 1600° C. 17. The method of claim 11, wherein the heat source contacts the cover glass at a pressure is in a range from 100 kgf to 500 kgf. 18. The method of claim 11, wherein the groove has an arch-shaped cross-section. 19. The method of claim 11, wherein the high dielectric material is selected from the group consisting of low melting glass (LMG), ZnO, Al2O3, Ta2O5, TiO2, ZrO2, La2O3, and Y2O3. 20. The method of claim 11, wherein the high dielectric material is disposed in the groove to a thickness in a range of 10 μm to 100 μm. 21. The method of claim 11, wherein the fingerprint recognition sensor is accommodated in the groove with an adhesive member. 22. The display device of claim 8, wherein the high dielectric material is LMG comprising material selected from the group consisting of selenium, potassium, arsenic, and sulfur. 23. The display device of claim 1, wherein the groove has a maximum depth at the central portion of the groove, and the maximum depth is greater than a thickness of the fingerprint recognition sensor.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318786B2,US10318786B2,Integration of touch screen and fingerprint sensor assembly,2014-07-07,"module, integrated, opening, device, screen, this, display, overall, lower, positioned, within, fitting, directly, toughened, close, together, systems, through, support, devices, hardened, have, mechanical, cover, with, upper, techniques, layer, additional, fingerprint, includes, include, that, touch, provided, sensor, strength, location, also, detection, integrating, underneath, does, provide, least, layers, assembly, glass, mobile, bonded, integration","Devices, systems, and techniques are provided for integrating a touch screen with a fingerprint detection module as an assembly for a mobile device. The integrated touch screen/fingerprint sensor assembly includes at least two glass layers bonded together to provide overall mechanical strength for the touch screen display. The upper glass layer is a toughened/hardened cover glass that does not have an opening for fitting the fingerprint detection module through this glass layer. The lower glass layer is a support glass layer underneath the hardened cover glass to provide additional mechanical strength and to include an opening at a location close to an end of the support glass layer. The integrated touch screen/fingerprint sensor assembly also includes a fingerprint detection module positioned within the opening of the support glass layer and directly underneath the hardened cover glass.","1. A device having a touch screen, comprising:
a touch screen including one or more touch sensitive layers which define a touch sensitive area to provide touch sensing operations of the device;
a top transparent cover on top of the touch screen as an interface to the touch screen to receive touches of one or more fingers for touch sensing operations of the device; and
a fingerprint sensor module located under the top transparent cover to detect a fingerprint of a finger at or above the top transparent cover and structured to include (1) one or more light sources to emit light to illuminate an area at or above the top transparent cover where a finger is present, (2) one or more photodetectors to detect returned light from the finger and (3) a signal processing module to process information in the detected returned light to monitor a human bio-feature signal to determine whether the finger is from a human,
wherein the top transparent cover includes a hardened cover glass having a spatially contiguous glass surface to fully cover the touch sensitive area and the fingerprint sensor module, and a support glass layer directly underneath the hardened cover glass and in contact with the one or more touch sensitive layers, wherein the support glass layer includes an opening through the support glass layer in which the fingerprint sensor module is positioned to be underneath the hardened cover glass to sense a finger or an object through the hardened cover glass. 1. A device having a touch screen, comprising:
a touch screen including one or more touch sensitive layers which define a touch sensitive area to provide touch sensing operations of the device;
a top transparent cover on top of the touch screen as an interface to the touch screen to receive touches of one or more fingers for touch sensing operations of the device; and
a fingerprint sensor module located under the top transparent cover to detect a fingerprint of a finger at or above the top transparent cover and structured to include (1) one or more light sources to emit light to illuminate an area at or above the top transparent cover where a finger is present, (2) one or more photodetectors to detect returned light from the finger and (3) a signal processing module to process information in the detected returned light to monitor a human bio-feature signal to determine whether the finger is from a human,
wherein the top transparent cover includes a hardened cover glass having a spatially contiguous glass surface to fully cover the touch sensitive area and the fingerprint sensor module, and a support glass layer directly underneath the hardened cover glass and in contact with the one or more touch sensitive layers, wherein the support glass layer includes an opening through the support glass layer in which the fingerprint sensor module is positioned to be underneath the hardened cover glass to sense a finger or an object through the hardened cover glass. 2. The device of claim 1, wherein the device includes one or more of a mobile phone, a tablet computer, a portable computer, a wireless device, a laptop, a game machine, or a multimedia device. 3. The device of claim 1, wherein the hardened cover glass is partially etched to form a space for placing the fingerprint detection module. 4. The device of claim 1, wherein the human bio-feature signal indicates a an oxygen saturation level in blood. 5. The device of claim 1, wherein the human bio-feature signal indicates a heartbeat. 6. The device of claim 1, wherein the one or more light sources in the fingerprint detection module include one or more LED lights. 7. The device of claim 1, wherein the touch screen comprises a fingerprint sensor mark located directly above the fingerprint detection module to indicate the position of the fingerprint detection module. 8. The device of claim 1, wherein the touch screen comprises a colored layer sandwiched between the hardened cover glass and the support glass layer, wherein the fingerprint detection module is attached to an area of the colored layer exposed by the opening of the support glass layer. 9. The device of claim 8, wherein the colored layer has a color or a pattern, or both, the color and the pattern to make the fingerprint detection module invisible from a user looking at the touch screen assembly from above the hardened cover glass. 10. The device of claim 1, wherein the touch screen further comprises an adhesive layer for bonding the fingerprint detection module to the colored layer. 11. The device of claim 1, wherein a boundary around the fingerprint detection module is not in contact with a sidewall of the opening. 12. The device of claim 1, wherein the touch screen further comprises a transparent conducting film (TCF) layer on a bottom surface of the support glass layer. 13. The device of claim 1, wherein the TCF layer includes an indium tin oxide (ITO) layer. 14. The device of claim 1, wherein the fingerprint detection module further comprises:
a printed circuit board having integrated circuits; and
a fingerprint sensor chip for collecting fingerprint data, wherein a first side of the fingerprint sensor chip is affixed to the printed circuit board and a second side of the fingerprint sensor chip is attached to an underside of the colored layer. 15. The device of claim 14, wherein the fingerprint detection module further includes a flexible printed circuit on an underside of the printed circuit board for coupling fingerprint sensor signals collected by the fingerprint sensor chip to an electronic device.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318790B2,US10318790B2,Code fingerprint-based processor malfunction detection,2016-01-04,"made, determination, present, determined, occurred, based, representation, while, sequence, response, determine, malfunction, computer, continues, techniques, corresponds, currently, fingerprint, includes, that, occurs, type, executing, detection, processor, determining, software, whether, system, according, relate, monitoring, code, behavior","Techniques relate to fingerprint-based processor malfunction detection. A determination is made whether a fingerprint is present in software that is currently executing on the processor of the computer system. The fingerprint includes a representation of a sequence of behavior that occurs on the processor while the software is executing. The fingerprint corresponds to a type of malfunction. In response to determining that the fingerprint is not present in the software currently executing on the processor, monitoring of the software executing on the processor to determine whether the fingerprint is present continues. In response to determining that the fingerprint is present in the software executing on the processor, it is determined that the malfunction has occurred according to a type of the fingerprint that is present.","1. A computer implemented method for fingerprint-based processor malfunction detection, the method comprising:
recognizing that a predefined prefix is present at a beginning of an instruction; and
in response to recognizing that the predefined prefix is present at the beginning of the instruction, determining whether a fingerprint is present in software that is currently executing on a processor, the fingerprint comprising a representation of a sequence of behavior that occurs on the processor while the software is executing. 1. A computer implemented method for fingerprint-based processor malfunction detection, the method comprising:
recognizing that a predefined prefix is present at a beginning of an instruction; and
in response to recognizing that the predefined prefix is present at the beginning of the instruction, determining whether a fingerprint is present in software that is currently executing on a processor, the fingerprint comprising a representation of a sequence of behavior that occurs on the processor while the software is executing. 2. The method of claim 1, wherein the predefined prefix is in a prefix field. 3. The method of claim 2, wherein the prefix field is an additional field added at the beginning of the instruction. 4. The method of claim 1, wherein the predefined prefix is an instruction prefix. 5. The method of claim 1, wherein the fingerprint corresponds to a type of malfunction. 6. The method of claim 5, further comprising in response to determining that the fingerprint is not present in the software currently executing on the processor, continuing to monitor the software executing on the processor to determine whether the fingerprint is present. 7. The method of claim 5, further comprising in response to determining that the fingerprint is present in the software executing on the processor, determining that the malfunction has occurred according to a type of the fingerprint that is present. 8. The method of claim 1, further comprising comparing a value in a fingerprint register to the fingerprint in order to determine whether the fingerprint is present in the fingerprint register. 9. The method claim 1, wherein recognizing that the predefined prefix is present at the beginning of the instruction causes output of the software currently executing on the processor to be placed into a fingerprint register, such that a value in the fingerprint register is compared to the fingerprint in order to determine whether the fingerprint is present in the fingerprint register. 10. A computer implemented method for fingerprint-based processor malfunction detection, the method comprising:
recognizing that a predefined suffix is present at an end of an instruction; and
in response to recognizing that the predefined suffix is present at the end of the instruction, determining whether a fingerprint is present in software that is currently executing on a processor, the fingerprint comprising a representation of a sequence of behavior that occurs on the processor while the software is executing. 10. A computer implemented method for fingerprint-based processor malfunction detection, the method comprising:
recognizing that a predefined suffix is present at an end of an instruction; and
in response to recognizing that the predefined suffix is present at the end of the instruction, determining whether a fingerprint is present in software that is currently executing on a processor, the fingerprint comprising a representation of a sequence of behavior that occurs on the processor while the software is executing. 11. The method of claim 10, wherein the predefined suffix is in a suffix field. 12. The method of claim 11, wherein the suffix field is an additional field added at the end of the instruction. 13. The method of claim 10, wherein the predefined suffix is an instruction suffix. 14. The method of claim 10, wherein the fingerprint corresponds to a type of malfunction. 15. The method of claim 14, further comprising in response to determining that the fingerprint is not present in the software currently executing on the processor, continuing to monitor the software executing on the processor to determine whether the fingerprint is present. 16. The method of claim 14, further comprising in response to determining that the fingerprint is present in the software executing on the processor, determining that the malfunction has occurred according to a type of the fingerprint that is present. 17. The method of claim 10, further comprising comparing a value in a fingerprint register to the fingerprint in order to determine whether the fingerprint is present in the fingerprint register. 18. The method claim 10, wherein recognizing that the predefined suffix is present at the end of the instruction causes output of the software currently executing on the processor to be placed into a fingerprint register, such that a value in the fingerprint register is compared to the fingerprint in order to determine whether the fingerprint is present in the fingerprint register. 19. A computer implemented method for fingerprint-based processor malfunction detection, the method comprising:
recognizing that an additional field is present in an instruction; and
in response to recognizing that the additional is present in the instruction, determining whether a fingerprint is present in software that is currently executing on a processor, the fingerprint comprising a representation of a sequence of behavior that occurs on the processor while the software is executing. 19. A computer implemented method for fingerprint-based processor malfunction detection, the method comprising:
recognizing that an additional field is present in an instruction; and
in response to recognizing that the additional is present in the instruction, determining whether a fingerprint is present in software that is currently executing on a processor, the fingerprint comprising a representation of a sequence of behavior that occurs on the processor while the software is executing. 20. The method of claim 19, wherein the additional field has been added to the beginning or end of the instruction.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318793B2,US10318793B2,Method for detecting fraud by pre-recorded image projection,2015-10-27,"method, optical, invention, parameters, during, device, enrollment, candidate, images, detecting, reference, said, comparing, acquired, step, sequence, biometric, view, steps, result, with, preliminary, estimating, image, provides, same, recorded, authentication, fraud, geometrical, acquisition, individual, verification, comparison, projection, determining, having, whether, least, from, comprising","The invention provides a method for detecting fraud ( 220 ) during biometric authentication of an individual comprising the acquisition ( 210 ), by an optical device, of a sequence of candidate images comprising at least two candidate images of an individual, with view to comparing said sequence with a reference image sequence comprising at least one reference image of an individual acquired during a preliminary enrollment step ( 100 ) by the same optical device, the method comprising the steps of:
         estimating ( 221 ), from the sequence of candidate images, the geometrical parameters of the optical device having acquired said sequence of candidate images on the individual, and   comparing ( 222 ) said geometrical parameters with the geometrical parameters of the optical device having acquired the reference image sequence, and   from the result of the comparison, determining ( 223, 224 ) whether the verification image sequence was acquired from at least one pre-recorded image of the individual.","1. A method for detecting fraud during biometric authentication, on the basis of a preregistered reference image sequence comprising at least one reference image of an individual acquired by an optical device during a preliminary enrollment step, the method comprising the steps of:
acquiring, with the same optical device, a sequence of candidate images comprising at least two candidate images of the individual,
estimating, from the sequence of candidate images, geometrical parameters of the optical device having acquired said sequence of candidate images on the individual, including a parameter equal to the ratio between the focal distance of the optical device having acquired said sequence and the size of the pixels of the sensor of said optical device,
comparing said geometrical parameters with geometrical parameters of the optical device having acquired the reference image sequence, including comparing the previously estimated parameter with the parameter equal to the ratio between the focal distance of the optical device having acquired the reference image sequence and the size of the pixels of the sensor of said optical device, and
from the result of the comparison, determining whether the sequence of candidate images was acquired from at least one pre-recorded image of the individual,
wherein each image sequence is a video comprising a plurality of images, and the images of each sequence are acquired for different relative positions of the individual with respect to the optical device. 1. A method for detecting fraud during biometric authentication, on the basis of a preregistered reference image sequence comprising at least one reference image of an individual acquired by an optical device during a preliminary enrollment step, the method comprising the steps of:
acquiring, with the same optical device, a sequence of candidate images comprising at least two candidate images of the individual,
estimating, from the sequence of candidate images, geometrical parameters of the optical device having acquired said sequence of candidate images on the individual, including a parameter equal to the ratio between the focal distance of the optical device having acquired said sequence and the size of the pixels of the sensor of said optical device,
comparing said geometrical parameters with geometrical parameters of the optical device having acquired the reference image sequence, including comparing the previously estimated parameter with the parameter equal to the ratio between the focal distance of the optical device having acquired the reference image sequence and the size of the pixels of the sensor of said optical device, and
from the result of the comparison, determining whether the sequence of candidate images was acquired from at least one pre-recorded image of the individual,
wherein each image sequence is a video comprising a plurality of images, and the images of each sequence are acquired for different relative positions of the individual with respect to the optical device. 2. The fraud detection method according to claim 1, wherein, the estimated geometrical parameters are the same as those of the optical device having acquired the reference image sequence in the absence of fraud, and correspond to the geometrical parameters of an equivalent optical device having acquired the sequence of candidate images directly on the individual in the case of fraud by re-projection of a pre-recorded image, and wherein, if the estimated geometrical parameters are different from the geometrical parameters of the optical device having acquired the reference image sequence, the candidate image sequence is considered as having been acquired from a pre-recorded image, and the method comprises a fraud detection step. 3. The fraud detection method according to claim 1, wherein the optical device comprises a sensor and optics, and the geometrical parameters of an optical device comprise the focal distance of the optics, the size of the pixels of the sensor, and the position of the projection of the centre of the optics on the sensor. 4. The fraud detection method according to claim 1, wherein each optical device used for acquiring the sequence of candidate images and the reference image sequence is modeled as a pinhole camera such that the projection of a point P on an optical sensor of the optical device is provided by the relationship:
  p=K[Rt]P  

wherein K is the matrix of the intrinsic geometrical properties of the optical device, R is a matrix of rotation and t is a translation vector,
and the estimation of the geometrical parameters of the optical device having acquired the sequence of candidate images comprises the estimation of the parameters of the matrix Keq of an equivalent optical device having acquired said images directly on the individual, and the comparison with the parameters of the matrix K0 of the optical device having acquired the candidate image sequence. 5. The fraud detection method according to claim 1, further comprising a preliminary enrollment step wherein an individual is enrolled by acquiring, by means of the optical device, a reference image sequence of the individual comprising at least one image of the individual, and the optical device used for enrollment being the same as the optical device used for acquisition of the sequence of candidate images during biometric authentication. 6. A fraud detection system comprising an optical device for acquiring images and a processing unit comprising a processor and a memory, wherein the processor is configured to execute the following steps:
sending an instruction to the optical device to acquire a sequence of candidate images comprising at least two candidate images,
estimating, from the sequence of candidate images, geometrical parameters of the optical device having acquired said sequence of candidate images, including a parameter equal to the ratio between the focal distance of the optical device having acquired said sequence and the size of the pixels of the sensor of said optical device,
comparing said geometrical parameters with geometrical parameters of an optical device having acquired a preregistered reference image sequence, including comparing the previously estimated parameter with the parameter equal to the ratio between the focal distance of the optical device having acquired the reference image sequence and the size of the pixels of the sensor of said optical device, and
from the result of the comparison, determining whether the sequence of candidate images was acquired from at least one pre-recorded image,
wherein each image sequence is a video comprising a plurality of images, and the images of each sequence are acquired for different relative positions of an individual with respect to the optical device. 6. A fraud detection system comprising an optical device for acquiring images and a processing unit comprising a processor and a memory, wherein the processor is configured to execute the following steps:
sending an instruction to the optical device to acquire a sequence of candidate images comprising at least two candidate images,
estimating, from the sequence of candidate images, geometrical parameters of the optical device having acquired said sequence of candidate images, including a parameter equal to the ratio between the focal distance of the optical device having acquired said sequence and the size of the pixels of the sensor of said optical device,
comparing said geometrical parameters with geometrical parameters of an optical device having acquired a preregistered reference image sequence, including comparing the previously estimated parameter with the parameter equal to the ratio between the focal distance of the optical device having acquired the reference image sequence and the size of the pixels of the sensor of said optical device, and
from the result of the comparison, determining whether the sequence of candidate images was acquired from at least one pre-recorded image,
wherein each image sequence is a video comprising a plurality of images, and the images of each sequence are acquired for different relative positions of an individual with respect to the optical device. 7. The fraud detection system according to claim 6, said system being a personal computer, a mobile telephone or a digital tablet. 8. A non-transitory computer-readable storage medium comprising code instructions, when applied by a processor, for executing a method comprising steps of:
acquiring, with an optical device, a sequence of candidate images comprising at least two candidate images,
estimating, from the sequence of candidate images, geometrical parameters of the optical device having acquired said sequence of images, including a parameter equal to the ratio between the focal distance of the optical device having acquired said sequence and the size of the pixels of the sensor of said optical device,
comparing said geometrical parameters with reference geometrical parameters, including comparing the previously estimated parameter with the parameter equal to the ratio between the focal distance of the optical device having acquired the reference image sequence and the size of the pixels of the sensor of said optical device, and
from the result of the comparison, determining whether the sequence of candidate images was acquired from at least one pre-recorded image, and determining whether an individual having used the sequence of candidate images for a biometric authentication has attempted fraud,
wherein each image sequence is a video comprising a plurality of images, and the images of each sequence are acquired for different relative positions of the individual with respect to the optical device. 8. A non-transitory computer-readable storage medium comprising code instructions, when applied by a processor, for executing a method comprising steps of:
acquiring, with an optical device, a sequence of candidate images comprising at least two candidate images,
estimating, from the sequence of candidate images, geometrical parameters of the optical device having acquired said sequence of images, including a parameter equal to the ratio between the focal distance of the optical device having acquired said sequence and the size of the pixels of the sensor of said optical device,
comparing said geometrical parameters with reference geometrical parameters, including comparing the previously estimated parameter with the parameter equal to the ratio between the focal distance of the optical device having acquired the reference image sequence and the size of the pixels of the sensor of said optical device, and
from the result of the comparison, determining whether the sequence of candidate images was acquired from at least one pre-recorded image, and determining whether an individual having used the sequence of candidate images for a biometric authentication has attempted fraud,
wherein each image sequence is a video comprising a plurality of images, and the images of each sequence are acquired for different relative positions of the individual with respect to the optical device.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318792B2,US10318792B2,Target pickup device and target detection method,2014-04-08,"method, invention, part, determination, device, video, excluded, present, captured, determined, detector, data, portion, mask, captures, excluding, image, performs, area, includes, that, unit, processing, characteristic, entirety, obtained, determines, detection, target, pickup, which, from, among","An image pickup device of the present invention includes an image pickup unit that captures a video image, a mask area determination unit that determines a mask area, for which processing for detection of a characteristic part of a target is excluded, among a portion or the entirety of an area of image data of the video image captured by the image pickup unit, and a characteristic part detector that performs the detection processing of the characteristic part of the target on a detection target area obtained by excluding the mask area determined by the mask area determination unit from the portion or the entirety of the area of the image data of the video image captured by the image pickup unit.","1. An image pickup device comprising:
an image pickup unit, which, in operation, captures a video image;
a mask area determination unit, which, in operation, determines a mask area, for which processing for detection of a characteristic part of a target is excluded, among a portion or entirety of an area of image data of the video image captured by the image pickup unit; and
a characteristic part detector, which, in operation, performs processing for detection of the characteristic part of the target on a detection target area obtained by excluding the mask area determined by the mask area determination unit from the portion or the entirety of the area of the image data of the video image captured by the image pickup unit,
the mask area determination unit including:
a target detector, which, in operation, detects the target in a portion or the entirety of the area of the image data of the video image captured by the image pickup unit,
a line statistical processor, which, in operation, generates a frequency distribution indicating a detection rate on a virtual line of the target detected by the target detector for each of a plurality of virtual lines which are set for the entire area of the image data, and
a mask area extractor, which, in operation, extracts the mask area from the portion or the entirety of the area of the image data based on the frequency distribution for each of the plurality of virtual lines generated by the line statistical processor. 1. An image pickup device comprising:
an image pickup unit, which, in operation, captures a video image;
a mask area determination unit, which, in operation, determines a mask area, for which processing for detection of a characteristic part of a target is excluded, among a portion or entirety of an area of image data of the video image captured by the image pickup unit; and
a characteristic part detector, which, in operation, performs processing for detection of the characteristic part of the target on a detection target area obtained by excluding the mask area determined by the mask area determination unit from the portion or the entirety of the area of the image data of the video image captured by the image pickup unit,
the mask area determination unit including:
a target detector, which, in operation, detects the target in a portion or the entirety of the area of the image data of the video image captured by the image pickup unit,
a line statistical processor, which, in operation, generates a frequency distribution indicating a detection rate on a virtual line of the target detected by the target detector for each of a plurality of virtual lines which are set for the entire area of the image data, and
a mask area extractor, which, in operation, extracts the mask area from the portion or the entirety of the area of the image data based on the frequency distribution for each of the plurality of virtual lines generated by the line statistical processor. 2. The image pickup device of claim 1,
wherein the mask area determination unit further includes a mask area renderer, which, in operation, renders the frequency distribution for each of the plurality of virtual lines generated by the line statistical processor in the image data. 3. The image pickup device of claim 2,
wherein the mask area renderer renders the mask area rendered according to the frequency distribution for each of the plurality of virtual lines according to the detection rate on the virtual line of the target designated by a predetermined change operation. 4. The image pickup device of claim 2,
wherein the mask area renderer renders a plurality of normal distribution curves indicating the frequency distribution for each of the plurality of virtual lines generated by the line statistical processor in the image data and displays the normal distribution curves on a display unit. 5. The image pickup device of claim 2,
wherein the mask area renderer renders the frequency distribution indicating the detection rate of the target detected by the target detector to be classified and identified using colors using the frequency distribution for each of the plurality of virtual lines generated by the line statistical processor and displays the frequency distribution on the display unit. 6. The image pickup device of claim 1,
wherein the line statistical processor increments a frequency indicating the detection rate of the target in an intersecting point position of a motion vector of a center of gravity of the area of a predetermined shape including the target of which the motion is detected by the target detector and the virtual line and generates the frequency distribution. 7. The image pickup device of claim 1,
wherein the line statistical processor increments a frequency indicating the detection rate of the target in an intersecting point position of the virtual line close to a center of gravity of an area of a predetermined shape including the target of which the motion is detected by the target detector and the predetermined shape and generates the frequency distribution. 8. The image pickup device of claim 1,
wherein the line statistical processor increments a frequency indicating the detection rate of the target in an intersecting point position of a motion vector of a center of gravity of an area of a predetermined shape including the target detected by the target detector and the virtual line and generates the frequency distribution. 9. The image pickup device of claim 1,
wherein the line statistical processor increments a frequency indicating the detection rate of the target in an intersecting point position of the virtual line close to a center of gravity of an area of a predetermined shape including the target detected by the target detector and the predetermined shape and generates the frequency distribution. 10. The image pickup device of claim 1,
wherein the line statistical processor displays the frequency distribution for each of the plurality of virtual lines generated using the result of the motion detection processing of the target in the target detector on the display unit. 11. The image pickup device of claim 1,
wherein the line statistical processor displays the frequency distribution for each of the plurality of virtual lines generated using the result of the detection processing of the target in the target detector on the display unit. 12. A characteristic part detection method comprising:
capturing a video image;
determining a mask area, for which processing for detection of a characteristic part of a target is excluded, among a portion or entirety of an area of image data of the captured video image;
detecting the target in the portion or entirety of the area of the image data of the captured video image;
generating a frequency distribution indicating a detection rate on a virtual line of the detected target for each of a plurality of virtual lines which are set for the entire area of image data;
extracting the mask area from the portion or entirety of the area of the image data based on the frequency distribution for each of the generated plurality of virtual lines;
performing processing for detection of the characteristic part of the target on a detection target area obtained by excluding the determined mask area from the portion or entirety of the area of the image data of the captured video image. 12. A characteristic part detection method comprising:
capturing a video image;
determining a mask area, for which processing for detection of a characteristic part of a target is excluded, among a portion or entirety of an area of image data of the captured video image;
detecting the target in the portion or entirety of the area of the image data of the captured video image;
generating a frequency distribution indicating a detection rate on a virtual line of the detected target for each of a plurality of virtual lines which are set for the entire area of image data;
extracting the mask area from the portion or entirety of the area of the image data based on the frequency distribution for each of the generated plurality of virtual lines;
performing processing for detection of the characteristic part of the target on a detection target area obtained by excluding the determined mask area from the portion or entirety of the area of the image data of the captured video image.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318794B2,US10318794B2,Intelligent auto cropping of digital images,2017-04-28,"landmark, utilized, digital, other, facial, machine, images, upon, examples, computing, recognition, single, readable, based, systems, technique, devices, cropping, image, techniques, smart, combination, methods, using, mediums, include, that, automatically, some, more, detection, provide, auto, emotion, disclosed, crop, intelligent, matching","Disclosed in some examples are methods, systems, computing devices, and machine readable mediums that provide for cropping systems that automatically crop digital images using one or more smart cropping techniques. Smart cropping techniques may include: cropping an image based upon emotion detection, cropping based upon facial recognition and matching, and cropping based upon landmark matching. In some examples, a single smart cropping technique may be utilized. In other examples, a combination of the smart cropping techniques may be utilized.","1. A machine-readable storage device, storing instructions, which when executed by a machine, cause the machine to perform operations comprising:
receiving a digital image;
detecting three or more faces in the digital image;
for each particular face in the three or more faces, determining an emotion displayed by the particular face;
clustering the three or more faces into two or more clusters, each particular cluster comprising faces displaying emotions that are the same or are classified as related to other faces in the particular cluster;
selecting a cluster of the two or more clusters; and
cropping the digital image based upon the faces in the selected cluster. 1. A machine-readable storage device, storing instructions, which when executed by a machine, cause the machine to perform operations comprising:
receiving a digital image;
detecting three or more faces in the digital image;
for each particular face in the three or more faces, determining an emotion displayed by the particular face;
clustering the three or more faces into two or more clusters, each particular cluster comprising faces displaying emotions that are the same or are classified as related to other faces in the particular cluster;
selecting a cluster of the two or more clusters; and
cropping the digital image based upon the faces in the selected cluster. 2. The machine-readable storage device of claim 1, wherein the operations of cropping the digital image based upon the faces in the selected cluster comprises operations of cropping the digital image so as not to include a face in a cluster of the two or more clusters that was not selected. 3. The machine-readable storage device of claim 1, wherein the operations further comprise:
for each particular face in the set of three or more faces, searching for a matching face in an image storage corresponding to a user; and
responsive to determining that a face in the set of three or more faces matches a face in an image stored in the image storage of the user, cropping the digital image based additionally upon the matched face. 4. The machine-readable storage device of claim 3, wherein the image storage is a network-based image storage. 5. The machine-readable storage device of claim 3, wherein the image storage is a social networking service. 6. The machine-readable storage device of claim 5, wherein images of the image storage correspond to social media posts of the user or posts of connections of the user on the social networking service. 7. The machine-readable storage device of claim 1, wherein the operations further comprise:
detecting a landmark in the digital image; and
cropping the digital image based additionally upon the landmark. 8. The machine-readable storage device of claim 7, wherein the landmark is detected based upon a comparison of features of the digital image with features of a library of digital images including landmarks. 9. The machine-readable storage device of claim 1, wherein the operations further comprise:
for each particular face in the set of three or more faces, searching for a matching face in an image storage of a user;
detecting a landmark in the digital image; and
cropping the digital image based upon the matching face, the landmark, and the set of faces in the selected cluster. 10. The machine-readable storage device of claim 9, wherein the operations of cropping the digital image based upon the matching face, the landmark, and the faces in the selected cluster comprises the operations of:
producing a first score based upon a first confidence value corresponding to the set of three or more faces in the selected cluster and a first weighting factor;
producing a second score based upon a second confidence value corresponding to the matching face and a second weighting factor;
producing a third score based upon a third confidence value corresponding to the landmark and a third weighting factor; and
cropping the digital image based upon the first, second and third scores. 11. The machine-readable storage device of claim 10, wherein the first, second, and third weighting factors are determined based upon a machine learning algorithm. 12. The machine-readable storage device of claim 11, wherein a rejection by the user of the cropped digital image is used to adjust one or more of the first, second, or third weighting factors. 13. A system for cropping an image, the system comprising:
a processor;
a memory including instructions, which when executed by the processor, cause the system to perform operations comprising:
receiving a digital image;
detecting three or more faces in the digital image;
for each particular face in the three or more faces, determining an emotion displayed by the particular face;
clustering the three or more faces into two or more clusters, each particular cluster comprising faces displaying emotions that are the same or are classified as related to other faces in the particular cluster;
selecting a cluster of the two or more clusters; and
cropping the digital image based upon the faces in the selected cluster. 13. A system for cropping an image, the system comprising:
a processor;
a memory including instructions, which when executed by the processor, cause the system to perform operations comprising:
receiving a digital image;
detecting three or more faces in the digital image;
for each particular face in the three or more faces, determining an emotion displayed by the particular face;
clustering the three or more faces into two or more clusters, each particular cluster comprising faces displaying emotions that are the same or are classified as related to other faces in the particular cluster;
selecting a cluster of the two or more clusters; and
cropping the digital image based upon the faces in the selected cluster. 14. The system of claim 13, wherein the operations of cropping the digital image based upon the faces in the selected cluster comprises operations of cropping the digital image so as not to include a face in a cluster of the two or more clusters that was not selected. 15. The system of claim 13, wherein the operations further comprise:
for each particular face in the set of three or more faces, searching for a matching face in an image storage corresponding to a user; and
responsive to determining that a face in the set of three or more faces matches a face in an image stored in the image storage of the user, cropping the digital image based additionally upon the matched face. 16. The system of claim 13, wherein the operations further comprise:
for each particular face in the set of three or more faces, searching for a matching face in an image storage of a user;
detecting a landmark in the digital image; and
cropping the digital image based upon the matching face, the landmark, and the set of faces in the selected cluster. 17. A method for cropping an image, the method comprising:
receiving a digital image;
detecting three or more faces in the digital image;
for each particular face in the three or more faces, determining an emotion displayed by the particular face;
clustering the three or more faces into two or more clusters, each particular cluster comprising faces displaying emotions that are the same or are classified as related to other faces in the particular cluster;
selecting a cluster of the two or more clusters; and
cropping the digital image based upon the faces in the selected cluster. 17. A method for cropping an image, the method comprising:
receiving a digital image;
detecting three or more faces in the digital image;
for each particular face in the three or more faces, determining an emotion displayed by the particular face;
clustering the three or more faces into two or more clusters, each particular cluster comprising faces displaying emotions that are the same or are classified as related to other faces in the particular cluster;
selecting a cluster of the two or more clusters; and
cropping the digital image based upon the faces in the selected cluster. 18. The method of claim 17, comprising:
for each particular face in the set of three or more faces, searching for a matching face in an image storage corresponding to a user; and
responsive to determining that a face in the set of three or more faces matches a face in an image stored in the image storage of the user, cropping the digital image based additionally upon the matched face. 19. The method of claim 17, comprising:
for each particular face in the set of three or more faces, searching for a matching face in an image storage of a user;
detecting a landmark in the digital image; and
cropping the digital image based upon the matching face, the landmark, and the set of faces in the selected cluster. 20. The method of claim 19, wherein cropping the digital image based upon the matching face, the landmark, and the faces in the selected cluster comprises:
producing a first score based upon a first confidence value corresponding to the set of three or more faces in the selected cluster and a first weighting factor;
producing a second score based upon a second confidence value corresponding to the matching face and a second weighting factor;
producing a third score based upon a third confidence value corresponding to the landmark and a third weighting factor; and
cropping the digital image based upon the first, second and third scores.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318795B2,US10318795B2,Remote camera access,2016-04-29,"camera, device, send, deny, configured, receiving, images, receive, allow, examples, unlocked, including, permissions, face, wirelessly, mode, input, interface, remote, operator, lessee, response, have, started, surroundings, attempting, grant, vehicle, person, restrictions, user, primary, some, more, capture, owner, therewith, disclosed, associated, mobile, access, exterior, operate","A vehicle configured to operate in a remote access mode is disclosed. In some examples, a camera at the exterior of the vehicle can capture one or more images of its surroundings, including the face of a person attempting to access the vehicle. A primary operator (e.g., owner or lessee) of the vehicle can receive the one or more images at a mobile device and send an input, via a user interface of the mobile device, to grant or deny access to the vehicle. In response to wirelessly receiving the input to allow access, the vehicle can be unlocked and started in the remote access mode. In some examples, the remote access mode can have a set of permissions and/or restrictions associated therewith.","1. A vehicle comprising:
one or more cameras;
a wireless transceiver; and
a processor operatively coupled to the one or more cameras and the wireless transceiver, the processor configured for:
capturing one or more images of surroundings of the vehicle at the one or more cameras;
transmitting, using the wireless transceiver, the one or more images to a mobile device;
receiving, at the wireless transceiver, an input from the mobile device, the input comprising one of an indication to provide access to the vehicle and an indication to prevent access to the vehicle; and
in response to receiving the indication to provide access to the vehicle:
starting the vehicle in a remote access mode, and
operating the vehicle with one or more vehicle restrictions associated with the remote access mode. 1. A vehicle comprising:
one or more cameras;
a wireless transceiver; and
a processor operatively coupled to the one or more cameras and the wireless transceiver, the processor configured for:
capturing one or more images of surroundings of the vehicle at the one or more cameras;
transmitting, using the wireless transceiver, the one or more images to a mobile device;
receiving, at the wireless transceiver, an input from the mobile device, the input comprising one of an indication to provide access to the vehicle and an indication to prevent access to the vehicle; and
in response to receiving the indication to provide access to the vehicle:
starting the vehicle in a remote access mode, and
operating the vehicle with one or more vehicle restrictions associated with the remote access mode. 2. The vehicle of claim 1, wherein the processor is further configured for:
in response to receiving the indication to provide access to the vehicle:
unlocking one or more locks coupled to one or more doors of the vehicle; and
initializing a motor of the vehicle; and

in response to receiving the indication to prevent access to the vehicle:
preventing the one or more locks coupled to the one or more doors of the vehicle from being unlocked;
preventing the initialization of the motor of the vehicle; and
preventing the vehicle from operating in the remote access mode. 3. The vehicle of claim 1, wherein the vehicle restrictions associated with the remote access mode includes one or more of a geofence, an approved destination and a speed limit. 4. The vehicle of claim 1, wherein the vehicle further comprises an infotainment system with access to a plurality of files, the plurality of files including one or more of contact files, location files, and audio files stored thereon, and operating the vehicle with the one or more vehicle restrictions includes restricting access to one or more of the plurality of files. 5. The vehicle of claim 1, the processor further configured for:
prior to capturing the one or more images at the one or more cameras, receiving an input to capture the one or more images. 6. The vehicle of claim 5, further comprising:
a touchscreen disposed on an exterior of the vehicle proximate to one of the one or more cameras, wherein the input to capture the one or more images comprises an input detected at the touchscreen. 7. The vehicle of claim 5, wherein the input to capture the one or more images is generated by the mobile device and received by the transceiver. 8. The vehicle of claim 1, wherein the one or more images comprise a video. 9. The vehicle of claim 1, wherein the one or more vehicle restrictions restrict one or more of a vehicle speed, a vehicle location, and a navigation route of the vehicle. 10. The vehicle of claim 1, wherein the one or more vehicle restrictions are selected via one or more restriction inputs received from the mobile device. 11. The vehicle of claim 1, wherein the processor is further configured for:
while operating the vehicle with the one or more vehicle restrictions associated with the remote access mode, monitoring the operation of the vehicle to determine whether the operation of the vehicle violates the one or more vehicle restrictions; and
in accordance with a determination that the operation of the vehicle violates the one or more vehicle restrictions, performing one or more respective actions corresponding to the one or more vehicle restrictions that are violated. 12. The vehicle of claim 11, wherein the one or more vehicle restrictions include a speed limit, and the one or more respective actions corresponding to the speed limit include generating a speed limit violation notification and/or preventing the vehicle from accelerating past the speed limit. 13. The vehicle of claim 11, wherein the one or more vehicle restrictions include a geofence, and the one or more respective actions corresponding to the geofence include generating a geofence violation notification and/or operating the vehicle in an autonomous driving mode to remain within the geofence. 14. The vehicle of claim 1, wherein the processor is further configured for:
while operating in the remote access mode, in accordance with a determination that the mobile device is detected within a threshold distance of the vehicle, exiting the remote access mode. 15. A method comprising:
capturing one or more images at one or more cameras operatively coupled to a vehicle;
transmitting, using a wireless transceiver, the one or more images to a mobile device;
receiving, at the wireless transceiver, an input from the mobile device, the input comprising one of an indication to provide access to the vehicle and an indication to prevent access to the vehicle; and
in response to receiving the indication to provide access to the vehicle:
starting the vehicle in a remote access mode, and
operating the vehicle with one or more vehicle restrictions associated with the remote access mode. 15. A method comprising:
capturing one or more images at one or more cameras operatively coupled to a vehicle;
transmitting, using a wireless transceiver, the one or more images to a mobile device;
receiving, at the wireless transceiver, an input from the mobile device, the input comprising one of an indication to provide access to the vehicle and an indication to prevent access to the vehicle; and
in response to receiving the indication to provide access to the vehicle:
starting the vehicle in a remote access mode, and
operating the vehicle with one or more vehicle restrictions associated with the remote access mode. 16. A vehicle comprising:
one or more cameras;
a wireless transceiver; and
a processor operatively coupled to the one or more cameras and the wireless transceiver, the processor configured for:
receiving, at the wireless transceiver, first one or more images of a first person of a plurality of people authorized to access the vehicle, the first one or more images associated with a first set of vehicle restrictions;
capturing second one or more images of surroundings of the vehicle at the one or more cameras;
determining whether, based on the first one or more images, the second one or more images include the first person authorized to access the vehicle;
in accordance with a determination that the second one or more images include the first person authorized to access the vehicle:
starting the vehicle in a first remote access mode, and
operating the vehicle with the first set of vehicle restrictions; and

in accordance with a determination that the second one or more images do not include a person of the plurality of people authorized to access the vehicle, preventing access to the vehicle. 16. A vehicle comprising:
one or more cameras;
a wireless transceiver; and
a processor operatively coupled to the one or more cameras and the wireless transceiver, the processor configured for:
receiving, at the wireless transceiver, first one or more images of a first person of a plurality of people authorized to access the vehicle, the first one or more images associated with a first set of vehicle restrictions;
capturing second one or more images of surroundings of the vehicle at the one or more cameras;
determining whether, based on the first one or more images, the second one or more images include the first person authorized to access the vehicle;
in accordance with a determination that the second one or more images include the first person authorized to access the vehicle:
starting the vehicle in a first remote access mode, and
operating the vehicle with the first set of vehicle restrictions; and

in accordance with a determination that the second one or more images do not include a person of the plurality of people authorized to access the vehicle, preventing access to the vehicle. 17. The vehicle of claim 16, the processor further configured for:
receiving, at the wireless transceiver, third one or more images of a second person of the plurality of people authorized to access the vehicle, the third one or more images associated with a second set of vehicle restrictions different from the first set of vehicle restrictions;
capturing fourth one or more images of the surroundings of the vehicle at the one or more cameras;
determining whether, based on the third one or more images, the fourth one or more images include the second person authorized to access the vehicle; and
in accordance with a determination that the fourth one or more images include the second person authorized to access the vehicle:
starting the vehicle in a second remote access mode, and
operating the vehicle with the second set of vehicle restrictions.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318796B2,US10318796B2,Age progression of subject facial image,2016-11-10,"gender, progression, subjects, test, facial, images, including, face, manipulating, based, corresponding, data, appearance, shapes, substantially, compiling, shape, form, each, group, facilitate, image, appearances, respective, frame, initial, training, aligned, using, specific, addition, includes, aligning, subject, common, free, selected, given, dictionary, determining, having, generate, least, groups, facilitated","Age progression of a test facial image is facilitated by compiling training data, including a training set(s) having selected initial images of subjects by gender and age-group. In addition, the age progression includes manipulating the training data, including: for a given age-group of a training set, substantially aligning respective face shapes; determining a common frame based on the aligned shapes; substantially aligning respective face appearances to generate a shape-free form corresponding to the face appearance of each subject, using the substantially aligned shapes to generate an age-specific shape-dictionary for each age-group, and a common shape-dictionary for the age-groups of the training set, and using the aligned appearances to generate at least an age-specific appearance-dictionary for each age-group, and a common appearance-dictionary for the age-groups of the training set. The age specific appearance dictionary for each age group and the common appearance dictionary facilitate age progression of the facial image.","1. A computer-implemented method of facilitating age progression of a test subject facial image, the computer-implemented method comprising:
compiling training data, the training data comprising at least one training set including selected facial images of subjects in a corresponding gender and classified into given age-groups;
manipulating the training data, comprising, for a given age-group of a given training set, aligning respective face shapes of subjects; computing a common frame based on the aligned respective face shapes; aligning respective face appearances of subjects, thereby generating a shape-free form corresponding to the respective face appearance of each subject, using the aligned respective face shapes to generate, in respect of a shape aspect, an age-specific shape-dictionary for each given age-group, and a common shape-dictionary for all the given age-groups, of the given training set, and using the aligned respective face appearances to generate, in respect of an appearance aspect, at least an age-specific appearance-dictionary for each given age-group, and a common appearance-dictionary for all the given age-groups, of the given training set;
wherein the age-specific appearance-dictionary for each given age-group and the common appearance-dictionary facilitate age progression of the test subject facial image;
wherein compiling training data comprises: accessing images of different subjects;
mapping a respective detected face in each of the accessed images according to defined landmark coordinates and obtaining a face shape corresponding to each of the respective detected face;
selecting the accessed images according to at least a given criterion, and compiling, in respect of a given gender, at least one training set;
the computer-implemented method further comprises testing the training data, the testing comprising: accessing a training set corresponding in gender to a detected gender of the test subject;
accessing given age-specific dictionaries and common dictionaries, which have been generated for respective shape and appearance aspects, in respect of the given age-groups of the accessed training set;
mapping a detected face of the test subject according to the defined landmark coordinates to obtain a test subject face shape;
aligning the test subject face shape with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face shape in at least a selected age-group of the accessed training set using a corresponding age-specific shape-dictionary and common shape dictionary;
aligning a test subject face appearance with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face appearance in at least the selected age-group of the accessed training set using a corresponding age-specific appearance-dictionary and common appearance dictionary, and combing the progressed aligned face shape and the progressed aligned face appearance of the test subject in at least the selected age-group of the accessed training set; and
wherein the testing the training data further comprises: progressing the aligned test subject face shape in a given age-group of the accessed training set by generating respective coding parameters for the age-specific shape-dictionary and the common shape-dictionary, which have been generated in respect of the given age-group, by solving:
subject to the constraint that:
  y i S =D 0,i S c i,1 S +D i S c i,2 S +e i  

where: S comprises a shape aspect; i comprises an integer allocated to a given age-group of the accessed training set and extends from 1 to K integers for the accessed training set; A, and A, comprise respective weighting parameters; ∥⋅∥1, ∥⋅∥2 respectively comprise the l1 and l2 norms and are respectively defined for a vector x as ∥x∥1=Σi|xi| and ∥x∥2=√{square root over (Σi|xi|2)}, ei comprises a sparse vector accounting for non-Gaussian sparse errors; ci,1 S, ci,2 S comprise coding parameters for respective allocation to the common shape-dictionary D0,i S and given age-specific shape-dictionaries Di S and yi S comprises the aligned face shape of the test subject into a given common frame computed in respect of a given age-group i of the accessed training set. 1. A computer-implemented method of facilitating age progression of a test subject facial image, the computer-implemented method comprising:
compiling training data, the training data comprising at least one training set including selected facial images of subjects in a corresponding gender and classified into given age-groups;
manipulating the training data, comprising, for a given age-group of a given training set, aligning respective face shapes of subjects; computing a common frame based on the aligned respective face shapes; aligning respective face appearances of subjects, thereby generating a shape-free form corresponding to the respective face appearance of each subject, using the aligned respective face shapes to generate, in respect of a shape aspect, an age-specific shape-dictionary for each given age-group, and a common shape-dictionary for all the given age-groups, of the given training set, and using the aligned respective face appearances to generate, in respect of an appearance aspect, at least an age-specific appearance-dictionary for each given age-group, and a common appearance-dictionary for all the given age-groups, of the given training set;
wherein the age-specific appearance-dictionary for each given age-group and the common appearance-dictionary facilitate age progression of the test subject facial image;
wherein compiling training data comprises: accessing images of different subjects;
mapping a respective detected face in each of the accessed images according to defined landmark coordinates and obtaining a face shape corresponding to each of the respective detected face;
selecting the accessed images according to at least a given criterion, and compiling, in respect of a given gender, at least one training set;
the computer-implemented method further comprises testing the training data, the testing comprising: accessing a training set corresponding in gender to a detected gender of the test subject;
accessing given age-specific dictionaries and common dictionaries, which have been generated for respective shape and appearance aspects, in respect of the given age-groups of the accessed training set;
mapping a detected face of the test subject according to the defined landmark coordinates to obtain a test subject face shape;
aligning the test subject face shape with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face shape in at least a selected age-group of the accessed training set using a corresponding age-specific shape-dictionary and common shape dictionary;
aligning a test subject face appearance with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face appearance in at least the selected age-group of the accessed training set using a corresponding age-specific appearance-dictionary and common appearance dictionary, and combing the progressed aligned face shape and the progressed aligned face appearance of the test subject in at least the selected age-group of the accessed training set; and
wherein the testing the training data further comprises: progressing the aligned test subject face shape in a given age-group of the accessed training set by generating respective coding parameters for the age-specific shape-dictionary and the common shape-dictionary, which have been generated in respect of the given age-group, by solving:
subject to the constraint that:
  y i S =D 0,i S c i,1 S +D i S c i,2 S +e i  

where: S comprises a shape aspect; i comprises an integer allocated to a given age-group of the accessed training set and extends from 1 to K integers for the accessed training set; A, and A, comprise respective weighting parameters; ∥⋅∥1, ∥⋅∥2 respectively comprise the l1 and l2 norms and are respectively defined for a vector x as ∥x∥1=Σi|xi| and ∥x∥2=√{square root over (Σi|xi|2)}, ei comprises a sparse vector accounting for non-Gaussian sparse errors; ci,1 S, ci,2 S comprise coding parameters for respective allocation to the common shape-dictionary D0,i S and given age-specific shape-dictionaries Di S and yi S comprises the aligned face shape of the test subject into a given common frame computed in respect of a given age-group i of the accessed training set. 2. The computer-implemented method of claim 1, wherein aligning an appearance of a given subject into a given common frame is performed by warping the detected face of the given subject into the given common frame. 3. The computer-implemented method of claim 1, wherein the testing the training data further comprises: progressing the aligned test subject face appearance in a given age-group of the accessed training set by generating respective coding parameters for the age-specific appearance-dictionary and the common appearance-dictionary, which have been generated in respect of the given age-group, by solving:






min


c

i
,
1

A

,

c

i
,
2

A



⁢


λ
c

⁡

(





c

i
,
1

A



2
2

+




c

i
,
2

A



2
2


)



+


λ
e

⁡

(




e
i



1

)





subject to the constraint that:
  y i A =D 0,i A c i,1 A +D i A c i,2 A +e i  

where: A comprises an appearance aspect; i comprises an integer allocated to a given age-group of the accessed training set and extends from 1 to K integers for the accessed training set; λc and λe comprise respective weighting parameters; ∥⋅∥1,∥⋅∥2 respectively comprise the l1 and l2 norms and are respectively defined for a vector x as ∥x∥1=Σi|xi| and ∥x∥2=√{square root over (Σi|xi|2)}, ei comprises a sparse vector accounting for non-Gaussian sparse errors; ci,1 A, ci,2 A comprise coding parameters for respective allocation to the common appearance—dictionary D0,i A and given age-specific appearance-dictionaries Di A, and yi A comprises the aligned face appearance of the test subject into the given common frame computed in respect of a given age-group i of the accessed training set. 4. The computer-implemented method of claim 1, wherein the testing the training data further comprises: combining the progressed aligned face shape and the aligned progressed face appearance of the test subject in the selected age-group of the accessed training set by warping the aligned progressed face appearance into the aligned progressed face shape. 5. The computer-implemented method of claim 1, wherein the manipulating the training data, comprises generating for a given training set, at least an age-specific dictionary for a given age-group and a common dictionary for all given age-groups, in respect of each of the shape aspect and the appearance aspect, by applying a corresponding error aspect. 6. The computer-implemented method of claim 1, wherein the manipulating the training data comprises:
generating at least two matrices, Xi S∈R2L×N and Xi A∈Rd  i  ×N for respective shape and appearance aspects by using the aligned respective face shapes and the aligned respective face appearances of subjects, where:
S comprises shape information; A comprises appearance information;
i comprises an integer allocated to a given age-group in respect of which a given matrix is formed, and extends from 1 to K integers for a given training set;
L comprises the number of defined landmark coordinates for mapping a detected face, thereby to obtain a corresponding face shape;
N comprises a number of images in each age-group, and di comprises a dimension of a shape-free form corresponding to the face appearance of each subject, and
performing a decomposition of the matrices Xi S and Xi A, respective to a shape aspect and an appearance aspect, into at least three components so as to correspondingly generate, in respect of each aspect, at least an age-specific dictionary for each given age-group, and a common dictionary for all the given age-groups, of a given training set:
  X i A =+D 0,i A +D i A +E i A  i=1, . . . K  
  X i S =D 0,i S +D i S +E i S  i=1, . . . K  


where: D0,i A, Di A, respectively comprise a common appearance-dictionary and a given age-specific appearance-dictionary that are generated in respect of an appearance aspect A for the given training set, D0,i S, Di S, respectively comprise a common shape-dictionary and a given age-specific shape-dictionary that are generated in respect of a shape aspect S for the given training set, and Ei S, Ei A comprise error matrices generated respectively for the shape aspect S and appearance aspect A. 7. The computer-implemented method of claim 1, wherein in the manipulating the training data: an age-specific dictionary generated for an age-group of a given training set comprises age-related information of subjects in that age-group. 8. The computer-implemented method of claim 1, wherein in the manipulating the training data: a common dictionary generated in respect of a given training set comprises information on at least a given facial appearance variation of subjects in all the age-groups of that given training set. 9. The computer-implemented method of claim 1, wherein the compiling the training data comprises determining a pose of a given subject from a corresponding mapped face of the subject. 10. The computer-implemented method of claim 1, wherein the compiling the training data, comprises performing a selection of the accessed images by removing those accessed images comprising at least a non-frontal aspect and at least an occlusion. 11. A system comprising:
a memory;
and one or more processors communicatively coupled with the memory, wherein the system performs a method of facilitating age progression of a test subject facial image, the method comprising:
compiling training data, the training data comprising at least one training set including selected facial images of subjects in a corresponding gender and classified into given age-groups;
manipulating the training data, comprising, for a given age-group of a given training set, aligning respective face shapes of subjects; computing a common frame based on the aligned respective face shapes; aligning respective face appearances of subjects, thereby generating a shape-free form corresponding to the respective face appearance of each subject, using the aligned respective face shapes to generate, in respect of a shape aspect, an age-specific shape-dictionary for each given age-group, and a common shape-dictionary for all the given age-groups, of the given training set, and using the aligned respective face appearances to generate, in respect of an appearance aspect, at least an age-specific appearance-dictionary for each given age-group, and a common appearance-dictionary for all the given age-groups, of the given training set;
wherein the age-specific appearance-dictionary for each given age-group and the common appearance-dictionary facilitate age progression of the test subject facial image;
wherein compiling training data comprises: accessing images of different subjects;
mapping a respective detected face in each of the accessed images according to defined landmark coordinates and obtaining a face shape corresponding to each of the respective detected face;
selecting the accessed images according to at least a given criterion, and compiling, in respect of a given gender, at least one training set;
the computer-implemented method further comprises testing the training data, the testing comprising: accessing a training set corresponding in gender to a detected gender of the test subject;
accessing given age-specific dictionaries and common dictionaries, which have been generated for respective shape and appearance aspects, in respect of the given age-groups of the accessed training set;
mapping a detected face of the test subject according to the defined landmark coordinates to obtain a test subject face shape;
aligning the test subject face shape with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face shape in at least a selected age-group of the accessed training set using a corresponding age-specific shape-dictionary and common shape dictionary;
aligning a test subject face appearance with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face appearance in at least the selected age-group of the accessed training set using a corresponding age-specific appearance-dictionary and common appearance dictionary, and combing the progressed aligned face shape and the progressed aligned face appearance of the test subject in at least the selected age-group of the accessed training set; and
wherein the testing the training data further comprises: progressing the aligned test subject face shape in a given age-group of the accessed training set by generating respective coding parameters for the age-specific shape-dictionary and the common shape-dictionary, which have been generated in respect of the given age-group, by solving:
subject to the constraint that:
  y i S =D 0,i S c i,1 S +D i S c i,2 S +e i  

where: S comprises a shape aspect; i comprises an integer allocated to a given age-group of the accessed training set and extends from 1 to K integers for the accessed training set; λc and λe comprise respective weighting parameters; ∥⋅∥1, ∥⋅∥2 respectively comprise the l1 and l2 norms and are respectively defined for a vector x as ∥x∥1=Σi|xi| and ∥x∥2=√{square root over (Σi|xi|2)}, ei comprises a sparse vector accounting for non-Gaussian sparse errors; ci,1 S, ci,2 S comprise coding parameters for respective allocation to the common shape-dictionary D0,i S and given age-specific shape-dictionaries Di S, and yi S comprises the aligned face shape of the test subject into a given common frame computed in respect of a given age-group i of the accessed training set. 11. A system comprising:
a memory;
and one or more processors communicatively coupled with the memory, wherein the system performs a method of facilitating age progression of a test subject facial image, the method comprising:
compiling training data, the training data comprising at least one training set including selected facial images of subjects in a corresponding gender and classified into given age-groups;
manipulating the training data, comprising, for a given age-group of a given training set, aligning respective face shapes of subjects; computing a common frame based on the aligned respective face shapes; aligning respective face appearances of subjects, thereby generating a shape-free form corresponding to the respective face appearance of each subject, using the aligned respective face shapes to generate, in respect of a shape aspect, an age-specific shape-dictionary for each given age-group, and a common shape-dictionary for all the given age-groups, of the given training set, and using the aligned respective face appearances to generate, in respect of an appearance aspect, at least an age-specific appearance-dictionary for each given age-group, and a common appearance-dictionary for all the given age-groups, of the given training set;
wherein the age-specific appearance-dictionary for each given age-group and the common appearance-dictionary facilitate age progression of the test subject facial image;
wherein compiling training data comprises: accessing images of different subjects;
mapping a respective detected face in each of the accessed images according to defined landmark coordinates and obtaining a face shape corresponding to each of the respective detected face;
selecting the accessed images according to at least a given criterion, and compiling, in respect of a given gender, at least one training set;
the computer-implemented method further comprises testing the training data, the testing comprising: accessing a training set corresponding in gender to a detected gender of the test subject;
accessing given age-specific dictionaries and common dictionaries, which have been generated for respective shape and appearance aspects, in respect of the given age-groups of the accessed training set;
mapping a detected face of the test subject according to the defined landmark coordinates to obtain a test subject face shape;
aligning the test subject face shape with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face shape in at least a selected age-group of the accessed training set using a corresponding age-specific shape-dictionary and common shape dictionary;
aligning a test subject face appearance with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face appearance in at least the selected age-group of the accessed training set using a corresponding age-specific appearance-dictionary and common appearance dictionary, and combing the progressed aligned face shape and the progressed aligned face appearance of the test subject in at least the selected age-group of the accessed training set; and
wherein the testing the training data further comprises: progressing the aligned test subject face shape in a given age-group of the accessed training set by generating respective coding parameters for the age-specific shape-dictionary and the common shape-dictionary, which have been generated in respect of the given age-group, by solving:
subject to the constraint that:
  y i S =D 0,i S c i,1 S +D i S c i,2 S +e i  

where: S comprises a shape aspect; i comprises an integer allocated to a given age-group of the accessed training set and extends from 1 to K integers for the accessed training set; λc and λe comprise respective weighting parameters; ∥⋅∥1, ∥⋅∥2 respectively comprise the l1 and l2 norms and are respectively defined for a vector x as ∥x∥1=Σi|xi| and ∥x∥2=√{square root over (Σi|xi|2)}, ei comprises a sparse vector accounting for non-Gaussian sparse errors; ci,1 S, ci,2 S comprise coding parameters for respective allocation to the common shape-dictionary D0,i S and given age-specific shape-dictionaries Di S, and yi S comprises the aligned face shape of the test subject into a given common frame computed in respect of a given age-group i of the accessed training set. 12. A computer program product for facilitating age progression of a test subject facial image, the computer program product comprising:
a non-transitory computer readable storage medium readable by a processing circuit and storing instructions for execution by the processing circuit for performing a method comprising:
compiling training data, the training data comprising at least one training set including selected facial images of subjects in a corresponding gender and classified into given age-groups;
manipulating the training data, comprising, for a given age-group of a given training set, aligning respective face shapes of subjects; computing a common frame based on the aligned respective face shapes; aligning respective face appearances of subjects, thereby generating a shape-free form corresponding to the respective face appearance of each subject, using the aligned respective face shapes to generate, in respect of a shape aspect, an age-specific shape-dictionary for each given age-group, and a common shape-dictionary for all the given age-groups, of the given training set, and using the aligned respective face appearances to generate, in respect of an appearance aspect, at least an age-specific appearance-dictionary for each given age-group, and a common appearance-dictionary for all the given age-groups, of the given training set;
wherein the age-specific appearance-dictionary for each given age-group and the common appearance-dictionary facilitate age progression of the test subject facial image;
wherein compiling training data comprises: accessing images of different subjects;
mapping a respective detected face in each of the accessed images according to defined landmark coordinates and obtaining a face shape corresponding to each of the respective detected face;
selecting the accessed images according to at least a given criterion, and compiling, in respect of a given gender, at least one training set;
the computer-implemented method further comprises testing the training data, the testing comprising: accessing a training set corresponding in gender to a detected gender of the test subject;
accessing given age-specific dictionaries and common dictionaries, which have been generated for respective shape and appearance aspects, in respect of the given age-groups of the accessed training set;
mapping a detected face of the test subject according to the defined landmark coordinates to obtain a test subject face shape;
aligning the test subject face shape with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face shape in at least a selected age-group of the accessed training set using a corresponding age-specific shape-dictionary and common shape dictionary;
aligning a test subject face appearance with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face appearance in at least the selected age-group of the accessed training set using a corresponding age-specific appearance-dictionary and common appearance dictionary, and combing the progressed aligned face shape and the progressed aligned face appearance of the test subject in at least the selected age-group of the accessed training set; and
wherein the testing the training data further comprises: progressing the aligned test subject face shape in a given age-group of the accessed training set by generating respective coding parameters for the age-specific shape-dictionary and the common shape-dictionary, which have been generated in respect of the given age-group, by solving:
subject to the constraint that:
  y i S =D 0,i S c i,1 S +D i S c i,2 S +e i  

where: S comprises a shape aspect; i comprises an integer allocated to a given age-group of the accessed training set and extends from 1 to K integers for the accessed training set; λc and λe comprise respective weighting parameters; ∥⋅∥1, ∥⋅∥2 respectively comprise the l1 and l2 norms and are respectively defined for a vector x as ∥x∥1=Σi|xi| and ∥x∥2=√{square root over (Σi|xi|2)}, ei comprises a sparse vector accounting for non-Gaussian sparse errors; ci,1 S, ci,2 S comprise coding parameters for respective allocation to the common shape-dictionary D0,j S and given age-specific shape-dictionaries Di S, and yi S comprises the aligned face shape of the test subject into a given common frame computed in respect of a given age-group i of the accessed training set. 12. A computer program product for facilitating age progression of a test subject facial image, the computer program product comprising:
a non-transitory computer readable storage medium readable by a processing circuit and storing instructions for execution by the processing circuit for performing a method comprising:
compiling training data, the training data comprising at least one training set including selected facial images of subjects in a corresponding gender and classified into given age-groups;
manipulating the training data, comprising, for a given age-group of a given training set, aligning respective face shapes of subjects; computing a common frame based on the aligned respective face shapes; aligning respective face appearances of subjects, thereby generating a shape-free form corresponding to the respective face appearance of each subject, using the aligned respective face shapes to generate, in respect of a shape aspect, an age-specific shape-dictionary for each given age-group, and a common shape-dictionary for all the given age-groups, of the given training set, and using the aligned respective face appearances to generate, in respect of an appearance aspect, at least an age-specific appearance-dictionary for each given age-group, and a common appearance-dictionary for all the given age-groups, of the given training set;
wherein the age-specific appearance-dictionary for each given age-group and the common appearance-dictionary facilitate age progression of the test subject facial image;
wherein compiling training data comprises: accessing images of different subjects;
mapping a respective detected face in each of the accessed images according to defined landmark coordinates and obtaining a face shape corresponding to each of the respective detected face;
selecting the accessed images according to at least a given criterion, and compiling, in respect of a given gender, at least one training set;
the computer-implemented method further comprises testing the training data, the testing comprising: accessing a training set corresponding in gender to a detected gender of the test subject;
accessing given age-specific dictionaries and common dictionaries, which have been generated for respective shape and appearance aspects, in respect of the given age-groups of the accessed training set;
mapping a detected face of the test subject according to the defined landmark coordinates to obtain a test subject face shape;
aligning the test subject face shape with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face shape in at least a selected age-group of the accessed training set using a corresponding age-specific shape-dictionary and common shape dictionary;
aligning a test subject face appearance with a common frame respectively computed for each given age-group of the accessed training set;
progressing the aligned test subject face appearance in at least the selected age-group of the accessed training set using a corresponding age-specific appearance-dictionary and common appearance dictionary, and combing the progressed aligned face shape and the progressed aligned face appearance of the test subject in at least the selected age-group of the accessed training set; and
wherein the testing the training data further comprises: progressing the aligned test subject face shape in a given age-group of the accessed training set by generating respective coding parameters for the age-specific shape-dictionary and the common shape-dictionary, which have been generated in respect of the given age-group, by solving:
subject to the constraint that:
  y i S =D 0,i S c i,1 S +D i S c i,2 S +e i  

where: S comprises a shape aspect; i comprises an integer allocated to a given age-group of the accessed training set and extends from 1 to K integers for the accessed training set; λc and λe comprise respective weighting parameters; ∥⋅∥1, ∥⋅∥2 respectively comprise the l1 and l2 norms and are respectively defined for a vector x as ∥x∥1=Σi|xi| and ∥x∥2=√{square root over (Σi|xi|2)}, ei comprises a sparse vector accounting for non-Gaussian sparse errors; ci,1 S, ci,2 S comprise coding parameters for respective allocation to the common shape-dictionary D0,j S and given age-specific shape-dictionaries Di S, and yi S comprises the aligned face shape of the test subject into a given common frame computed in respect of a given age-group i of the accessed training set.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318797B2,US10318797B2,Image processing apparatus and image processing method,,"method, containing, invention, maintained, region, other, configured, present, reduced, face, extraction, features, feature, based, aspects, acquired, regions, attributes, symmetry, used, locate, each, extract, with, image, apparatus, acquisition, unit, processing, dimension, could, location, integrate, symmetrical, comprises, much, possible, least, discloses, represents, which, from, according, accuracy, acquire, integration, located","One of the aspects of the present invention discloses an image processing apparatus and an image processing method. The image processing apparatus comprises: an image acquisition unit configured to acquire an image containing a face; a region location unit configured to locate at least one region which represents attributes of the face in the acquired image; a feature extraction unit configured to extract features from the located regions; and a feature integration unit configured to integrate the features of the located regions which are symmetrical with each other based on symmetry of the face. According to the present invention, the dimension of the features used for image processing could be reduced and the image processing accuracy could be maintained as much as possible.","1. An image processing apparatus, comprising:
an image acquisition unit configured to acquire an image containing a face;
a region location unit configured to locate at least one region which represents attributes of the face in the acquired image;
a feature extraction unit configured to extract features from the located regions; and
a feature integration unit configured to integrate the features of the located regions which are symmetrical with each other based on symmetry of the face. 1. An image processing apparatus, comprising:
an image acquisition unit configured to acquire an image containing a face;
a region location unit configured to locate at least one region which represents attributes of the face in the acquired image;
a feature extraction unit configured to extract features from the located regions; and
a feature integration unit configured to integrate the features of the located regions which are symmetrical with each other based on symmetry of the face. 2. The apparatus according to claim 1, wherein the feature integration unit comprising:
a feature combination unit configured to, as for one located region:
combine the features of the located region with the features of the symmetrical region of the located region, in case a center of the located region is not on a symmetrical line of the face; and
combine the features of first sub-region of the located region with the features of second sub-region of the located region, incase the center of the located region is on the symmetrical line of the face; wherein the located region is divided into the first sub-region and the second sub-region by the symmetrical line of the face. 3. The apparatus according to claim 2, wherein the feature integration unit further comprising:
an occluded region determination unit configured to, as for one located region, determine whether the located region is an occluded region or not;
a feature re-determination unit configured to, as for one occluded region, re-determine the features of the occluded region according to the features of the corresponding located region which are symmetrical with the occluded region based on the symmetry of the face. 4. The apparatus according to claim 3, wherein the feature re-determination unit:
re-determines the features of the occluded region or the features of the symmetrical region of the occluded region based on size of occluded area of the occluded region and size of occluded area of the symmetrical region, in case the center of the occluded region is not on the symmetrical line of the face; and
re-determines the features of the first sub-region or the second sub-region of the occluded region based on size of occluded area of the first sub-region and size of occluded area of the second sub-region, in case the center of the occluded region is on the symmetrical line of the face. 5. The apparatus according to claim 4, wherein the feature re-determination unit:
determines the features of the symmetrical region as the features of the occluded region in case the size of occluded area of the occluded region is larger than that of the symmetrical region; otherwise, determines the features of the occluded region as the features of the symmetrical region; and
determines the features of the first sub-region as the features of the second sub-region in case the size of occluded area of the first sub-region is less than that of the second sub-region; otherwise, determines the features of the second sub-region as the features of the first sub-region. 6. The apparatus according to claim 1, wherein the region location unit comprising:
a feature point detection unit configured to detect feature points in the acquired image; and
a region determination unit configured to determine the regions based on the detected feature points. 7. The apparatus according to claim 1, wherein, as for one located region, the feature extraction unit:
divides the located region into a plurality of blocks with same size of area; and
extracts features from the divided blocks. 8. The apparatus according to claim 1, further comprising:
a face feature obtaining unit configured to link the integrated features to obtain a feature for the face. 9. The apparatus according to claim 8, further comprising:
a facial expression recognition unit configured to recognize facial expression category of the face in the acquired image based on the obtained feature for the face and pre-generated facial expression models. 10. The apparatus according to claim 8, further comprising:
a face recognition unit configured to recognize the face in the acquired image based on the obtained feature for the face and pre-generated face models. 11. An image processing method, comprising:
image acquisition step of acquiring an image containing a face;
region location step of locating at least one region which represents attributes of the face in the acquired image;
feature extraction step of extracting features from the located regions; and
feature integration step of integrating the features of the located regions which are symmetrical with each other based on symmetry of the face. 11. An image processing method, comprising:
image acquisition step of acquiring an image containing a face;
region location step of locating at least one region which represents attributes of the face in the acquired image;
feature extraction step of extracting features from the located regions; and
feature integration step of integrating the features of the located regions which are symmetrical with each other based on symmetry of the face. 12. The method according to claim 11, further comprising:
face feature obtaining step of linking the integrated features to obtain a feature for the face. 13. The method according to claim 12, further comprising:
facial expression recognition step of recognizing facial expression category of the face in the acquired image based on the obtained feature for the face and pre-generated facial expression models. 14. The method according to claim 12, further comprising:
face recognition step of recognizing the face in the acquired image based on the obtained feature for the face and pre-generated face models.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318798B2,US10318798B2,Device and method for detecting non-visible content in a non-contact manner,2015-07-10,"method, optical, during, emit, period, changes, device, visible, light, configured, receive, detecting, surface, present, determined, component, detect, based, temporal, motion, object, determine, where, sensing, source, plural, addition, includes, received, processing, content, record, provided, frequency, also, manner, whether, represents, pattern, toward, contact, points, from, over, time","A method and device are provided for detecting non-visible content of an object in a non-contact manner. The device includes a light source configured to emit light toward a surface of an object over a period of time. The device also includes an optical sensing component configured to receive a pattern of light from the surface of the object, and to record the received pattern at plural points in time. In addition, the device includes a processing component configured to determine temporal changes in the pattern during the plural points in time, and to detect whether motion is present in the object based on determined temporal changes in the pattern, where the motion represents a frequency source of non-visible content in the object.","1. A device for detecting non-visible content of an object in a non-contact manner, the device comprising:
a light source configured to emit light toward a surface of an object over a period of time, wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
an optical sensing component configured to receive a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
a processing component configured to determine temporal changes in the pattern of light during the plural points in time, and to detect whether motion is present in the object based on determined temporal changes in the pattern of light, the motion representing a frequency source of non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time. 1. A device for detecting non-visible content of an object in a non-contact manner, the device comprising:
a light source configured to emit light toward a surface of an object over a period of time, wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
an optical sensing component configured to receive a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
a processing component configured to determine temporal changes in the pattern of light during the plural points in time, and to detect whether motion is present in the object based on determined temporal changes in the pattern of light, the motion representing a frequency source of non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time. 2. The device according to claim 1, wherein the optical sensing component is configured to record, as the pattern, an image of an interference pattern of light received from the surface of the object. 3. The device according to claim 2, wherein the optical sensing component is configured to record the plurality of images of the interference pattern within an image frame at the plural points in time, and
wherein the processing component is configured to determine the temporal changes in the interference pattern by comparing the plurality of images and calculating changes in position of the interference pattern in the image frame between the plurality of images. 4. The device according to claim 3, wherein the processing component is configured to calculate changes in position of the interference pattern by overlapping at least two of the images and determining an amount of displacement of the overlapped images with respect to each other during the plural points in time. 5. The device according to claim 4, wherein the processing component is configured to determine the amount of displacement of the overlapped images based on respective intensities of the images and a relative motion of the respective intensities of the images with respect to each other during the plural points in time. 6. The device according to claim 5, wherein the processing component is configured to determine the amount of displacement of the overlapped images based on an amount of relative motion of the respective intensities with respect to Cartesian coordinates. 7. The device according to claim 3, wherein the processing component is configured to identify a plurality of segments in each of the images, respectively, and to calculate changes in position in the interference pattern by calculating changes in position of corresponding identified segments in the plurality of images. 8. The device according to claim 7, wherein the processing component is configured to associate each of the identified segments with position information indicating a respective position within the image frame in each corresponding image, and to determine whether the corresponding position information for at least one of the identified segments in a first one of the plurality of images changes with respect to the corresponding position information for the at least one of the identified segments in a second one of the plurality of images recorded subsequent to the first one of the plurality of images. 9. The device according to claim 8, wherein the processing component is configured to determine whether the corresponding position information for the at least one of the identified segments in the first one of the plurality of images changes with respect to a predetermined number of the plurality of images. 10. The device according to claim 8, wherein the position information includes Cartesian coordinates within the image frame for the corresponding identified segment. 11. The device according to claim 8, wherein the position information includes a measured position of the corresponding identified segment in relation to a known position of at least one other identified segment in the corresponding image. 12. The device according to claim 8, wherein the processing component is configured to calculate a frequency of change for each of the corresponding identified segments in the plurality of images. 13. The device according to claim 1, wherein the processing component is configured to detect non-visible contents of the object by determining which of the corresponding identified segments in the plurality of images have at least the predetermined rate of change during the plural points in time. 14. The device according to claim 13, wherein the processing component is configured to determine motion of the non-visible content of the object in a first direction and in a second direction transverse to the first direction. 15. The device according to claim 13, wherein the processing component is configured to detect a frequency of vibration in the non-visible content based on the determined rate of change of corresponding segments in the image frame within the plural points in time. 16. The device according to claim 1, wherein the optical sensing component includes a camera configured to record respective images of the pattern received from the first surface at the plural points in time. 17. The device according to claim 16, wherein the optical sensing component is configured to record a first image of the first interference pattern received from the first surface of the object based on the first laser light, to record a second image of the second interference pattern from the second surface of the object based on the second laser light, to record a plurality of the first images of the first interference pattern within a first image frame at the plural points in time, and to record a plurality of the second images of the second interference pattern within a second image frame at the plural points in time. 18. The device according to claim 17, wherein the processing component is configured to:
determine the temporal changes in the first interference pattern by comparing the plurality of first images and calculating changes in position of the first interference pattern in the first image frame between the plurality of first images;
determine the temporal changes in the second interference pattern by comparing the plurality of second images and calculating changes in position of the second interference pattern in the second image frame between the plurality of second images; and
determine whether the temporal changes in the first interference pattern are greater or less than the temporal changes in the second interference pattern. 19. The device according to claim 18, wherein the processing component is configured to identify a plurality of segments in each of the first and second images of the first and second interference patterns, respectively, and to identify segments in the first images which respectively correspond to segments in the second images, and
wherein the processing component is configured to detect temporal changes in the first and second interference patterns by calculating changes in position in the corresponding segments of the plurality of first and second images during the plural points in time. 20. The device according to claim 19, wherein the processing component is configured to:
record position information for each segment identified in each of the first and second images, respectively,
calculate changes in position of the corresponding segments by determining whether position information of the corresponding segments of the plurality of first and second images changes during the plural points in time, and
detect whether motion is present in the object based on the calculated changes in position of the corresponding segments of the plurality of first and second images. 21. The device according to claim 1, wherein the optical sensing component comprises a plurality of sensors each configured to detect a corresponding proximity of a plurality of segments of the pattern with respect to the sensors over the plural points in time, and
wherein the processing component is configured to determine temporal changes in the pattern during the plural points in time by determining whether each segment has a predetermined displacement between the recorded patterns. 22. The device according to claim 1, wherein the optical sensing component includes a light sensitive area configured to receive the pattern of light from the surface of the object,
wherein the optical sensing component is configured to output a signal proportional to changes in the pattern of light incident on the light sensitive area of the detector, and
wherein the processing component is configured to determine temporal changes in the pattern during the plural points in time based on the output signal of the optical sensing component. 23. The device according to claim 21, wherein the processing component is configured to determine temporal changes in the pattern by calculating an amount of change in the output signal of the optical sensing component. 24. The device according to claim 1, wherein the processing component is configured to detect motion within a predetermined frequency range. 25. A method for detecting non-visible content of an object in a non-contact manner, the method comprising:
emitting light from a light source toward a surface of an object over a period of time, wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
receiving, by an optical sensing component, a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
determining, by a processor of a computer processing device, temporal changes in the pattern of light during the plural points in time; and
detecting, by the processor, whether motion is present in the object based on determined temporal changes in the pattern of light, the motion representing a frequency source of non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time. 25. A method for detecting non-visible content of an object in a non-contact manner, the method comprising:
emitting light from a light source toward a surface of an object over a period of time, wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
receiving, by an optical sensing component, a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
determining, by a processor of a computer processing device, temporal changes in the pattern of light during the plural points in time; and
detecting, by the processor, whether motion is present in the object based on determined temporal changes in the pattern of light, the motion representing a frequency source of non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time. 26. The method according to claim 25, comprising:
recording, as the pattern, an image of an interference pattern of light received from the surface of the object; and
recording a plurality of the images of the interference pattern at the plural points in time. 27. The method according to claim 26, wherein the optical sensing component is configured to record the plurality of images of the interference pattern within an image frame at the plural points in time, and
wherein the processing component is configured to determine the temporal changes in the interference pattern by comparing the plurality of images and calculating changes in position of the interference pattern in the image frame between the plurality of images. 28. A non-transitory computer-readable recording medium having a computer program tangibly recorded thereon that, when executed by a processor of a computer processing device, causes the processor to carry out operations for detecting non- visible content of an object in a non-contact manner, the operations comprising:
emitting light toward a surface of an object over a period of time, wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
receiving a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
determining temporal changes in the pattern of light during the plural points in time; and
detecting whether motion is present in the object based on determined temporal changes in the pattern of light, the motion representing a frequency source of non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time. 28. A non-transitory computer-readable recording medium having a computer program tangibly recorded thereon that, when executed by a processor of a computer processing device, causes the processor to carry out operations for detecting non- visible content of an object in a non-contact manner, the operations comprising:
emitting light toward a surface of an object over a period of time, wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
receiving a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
determining temporal changes in the pattern of light during the plural points in time; and
detecting whether motion is present in the object based on determined temporal changes in the pattern of light, the motion representing a frequency source of non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time. 29. A device for detecting non-visible content of an object in a non-contact manner, the device comprising:
a light source configured to emit light toward a surface of an object over a period of time1 wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
an optical sensing component configured to receive a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
a processing component configured to define the pattern recorded at at least one of the plural points in time as a reference pattern, to determine temporal changes in the pattern of light during the plural points in time by comparing the pattern of light during the plural points in time with the reference pattern, and to detect whether motion is present in the object when determined temporal changes in the pattern of light differ a predetermined amount the reference pattern, the motion representing non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time. 29. A device for detecting non-visible content of an object in a non-contact manner, the device comprising:
a light source configured to emit light toward a surface of an object over a period of time1 wherein the light source includes a first laser configured to emit a first laser light toward a first surface of the object, and a second laser light source configured to emit a second laser light at a second surface of the object;
an optical sensing component configured to receive a pattern of light from the surface of the object, and to record the received pattern of light at plural points in time, wherein the received pattern of light includes a first interference pattern received from the first surface of the object based on the first laser light, and a second interference pattern received from the second surface of the object based on the second laser light; and
a processing component configured to define the pattern recorded at at least one of the plural points in time as a reference pattern, to determine temporal changes in the pattern of light during the plural points in time by comparing the pattern of light during the plural points in time with the reference pattern, and to detect whether motion is present in the object when determined temporal changes in the pattern of light differ a predetermined amount the reference pattern, the motion representing non-visible content in the object, wherein
the optical sensing component is configured to record a plurality of images of the received pattern at the plural points in time, and
the processing component is configured to calculate motion of corresponding identified segments in the plurality of images by determining which of the identified segments have at least a predetermined rate of change within the plural points in time.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318799B2,US10318799B2,Method of predicting an interest of a user and a system thereof,,"method, disclosure, second, identifying, receiving, environment, present, around, features, prediction, thereof, extracted, input, assigned, related, thus, each, where, with, assigning, weights, interest, user, correlating, more, inputs, comprises, predicting, sensors, discloses, associated, further, system, increasing, from, among, accuracy, first, behavior","The present disclosure discloses a method of predicting an interest of a user and a system thereof. The method comprises receiving, by the system, a first set of inputs related to the user from a first set of sensors, where one or more features of the user are extracted from the first set of inputs. The method further comprises receiving a second set of inputs related to an environment around the user from a second set of sensors, assigning weights to each input among the first set of inputs, identifying a user behavior associated with each of the one or more features of the user and predicting the interest of the user by correlating the user behavior associated with each of the one or more features of the first set of inputs and the weights assigned to each input among the first set of inputs, thus increasing accuracy of prediction.","1. A method of predicting an interest of a user, the method comprising:
receiving, by an interest prediction system, a first set of inputs related to the user from a first set of sensors, wherein one or more features of the user are extracted from the first set of inputs, wherein the first set of inputs comprises image data and audio data;
receiving, by the interest prediction system, a second set of inputs related to an environment around the user from a second set of sensors, wherein the second set of inputs comprises luminosity data of the image data and audio noise data of the audio data;
assigning, by the interest prediction system, weights to the image data and the audio data among the first set of inputs based on the second set of inputs, a higher weight being assigned to one of the image input or the audio input if the second set of inputs convey that the one of the image input or the audio input is a reliable input among the first set of inputs;
identifying, by the interest prediction system, a user behavior associated with each of the one or more features of the user by comparing the one or more features with one or more predefined features corresponding to the user behavior; and
predicting, by the interest prediction system, the interest of the user by correlating the user behavior associated with each of the one or more features of the first set of inputs and the input assigned with higher weight among the first set of inputs. 1. A method of predicting an interest of a user, the method comprising:
receiving, by an interest prediction system, a first set of inputs related to the user from a first set of sensors, wherein one or more features of the user are extracted from the first set of inputs, wherein the first set of inputs comprises image data and audio data;
receiving, by the interest prediction system, a second set of inputs related to an environment around the user from a second set of sensors, wherein the second set of inputs comprises luminosity data of the image data and audio noise data of the audio data;
assigning, by the interest prediction system, weights to the image data and the audio data among the first set of inputs based on the second set of inputs, a higher weight being assigned to one of the image input or the audio input if the second set of inputs convey that the one of the image input or the audio input is a reliable input among the first set of inputs;
identifying, by the interest prediction system, a user behavior associated with each of the one or more features of the user by comparing the one or more features with one or more predefined features corresponding to the user behavior; and
predicting, by the interest prediction system, the interest of the user by correlating the user behavior associated with each of the one or more features of the first set of inputs and the input assigned with higher weight among the first set of inputs. 2. The method of claim 1, wherein the first set of inputs further comprises at least one of video data, and text data. 3. The method of claim 1, wherein the second set of inputs further comprises at least one of temperature data, humidity data, air composition data, and air quality data. 4. The method of claim 1, wherein the one or more features comprises at least one of body postures of the user, facial parameters of the user and audio parameters of the user. 5. An interest prediction system, comprising:
a processor; and
a memory, communicatively coupled to the processor, which stores processor executable instructions, which, on execution causes the processor to:
receive a first set of inputs related to a user from a first set of sensors, wherein one or more features of the user are extracted from the first set of inputs, wherein the first set of inputs comprises image data and audio data;
receive a second set of inputs related to an environment around the user from a second set of sensors, wherein the second set of inputs comprises luminosity data of the image data and audio noise data of the audio data;
assign weights to the image data and the audio data among the first set of inputs based on the second set of inputs, a higher weight being assigned to one of the image input or the audio input if the second set of inputs convey that the one of the image input or the audio input is a reliable input among the first set of inputs;
identify a user behavior associated with each of the one or more features of the user by comparing the one or more features with one or more predefined features corresponding to the user behavior; and
predict the interest of the user by correlating the user behavior associated with each of the one or more features of the first set of inputs and the input assigned with higher weight among the first set of inputs. 5. An interest prediction system, comprising:
a processor; and
a memory, communicatively coupled to the processor, which stores processor executable instructions, which, on execution causes the processor to:
receive a first set of inputs related to a user from a first set of sensors, wherein one or more features of the user are extracted from the first set of inputs, wherein the first set of inputs comprises image data and audio data;
receive a second set of inputs related to an environment around the user from a second set of sensors, wherein the second set of inputs comprises luminosity data of the image data and audio noise data of the audio data;
assign weights to the image data and the audio data among the first set of inputs based on the second set of inputs, a higher weight being assigned to one of the image input or the audio input if the second set of inputs convey that the one of the image input or the audio input is a reliable input among the first set of inputs;
identify a user behavior associated with each of the one or more features of the user by comparing the one or more features with one or more predefined features corresponding to the user behavior; and
predict the interest of the user by correlating the user behavior associated with each of the one or more features of the first set of inputs and the input assigned with higher weight among the first set of inputs. 6. The interest prediction system of claim 5, wherein the first set of inputs further comprises at least one of video data, and text data. 7. The interest prediction system of claim 5, wherein the first set of sensors comprises at least one of image capturing devices, video capturing devices, audio recording devices and user interface. 8. The interest prediction system of claim 5, wherein the second set of inputs further comprises at least one of temperature data, humidity data, air composition data, and air quality data. 9. The interest prediction system of claim 5, wherein the second set of sensors comprises at least one of temperature sensors, luminosity sensors, humidity sensors, air monitor sensors and sound sensors. 10. The interest prediction system of claim 5, wherein the one or more features comprises at least one of body postures of the user, facial parameters of the user and audio parameters of the user. 11. A non-transitory computer readable medium including instructions stored thereon that when processed by at least one processor cause a device to perform operations comprising:
receiving a first set of inputs related to a user from a first set of sensors, wherein one or more features of the user are extracted from the first set of inputs, wherein the first set of inputs comprises image data and audio data;
receiving a second set of inputs related to an environment around the user from a second set of sensors, wherein the second set of inputs comprises luminosity data of the image data and audio noise data of the audio data;
assigning weights to the image data and the audio data among the first set of inputs based on the second set of inputs, a higher weight being assigned to one of the image input or the audio input if the second set of inputs convey that the one of the image input or the audio input is a reliable input among the first set of inputs;
identifying a user behavior associated with each of the one or more features of the user by comparing the one or more features with one or more predefined features corresponding to the user behavior; and
predicting the interest of the user by correlating the user behavior associated with each of the one or more features of the first set of inputs and the input assigned with higher weight among the first set of inputs. 11. A non-transitory computer readable medium including instructions stored thereon that when processed by at least one processor cause a device to perform operations comprising:
receiving a first set of inputs related to a user from a first set of sensors, wherein one or more features of the user are extracted from the first set of inputs, wherein the first set of inputs comprises image data and audio data;
receiving a second set of inputs related to an environment around the user from a second set of sensors, wherein the second set of inputs comprises luminosity data of the image data and audio noise data of the audio data;
assigning weights to the image data and the audio data among the first set of inputs based on the second set of inputs, a higher weight being assigned to one of the image input or the audio input if the second set of inputs convey that the one of the image input or the audio input is a reliable input among the first set of inputs;
identifying a user behavior associated with each of the one or more features of the user by comparing the one or more features with one or more predefined features corresponding to the user behavior; and
predicting the interest of the user by correlating the user behavior associated with each of the one or more features of the first set of inputs and the input assigned with higher weight among the first set of inputs. 12. The medium of claim 11, wherein the first set of inputs further comprises at least one of video data, and text data. 13. The medium of claim 11, wherein the first set of sensors comprises at least one of image capturing devices, video capturing devices, audio recording devices and user interface. 14. The medium of claim 11, wherein the second set of inputs further comprises at least one of temperature data, humidity data, air composition data, and air quality data. 15. The medium of claim 11, wherein the second set of sensors comprises at least one of temperature sensors, luminosity sensors, humidity sensors, air monitor sensors and sound sensors. 16. The medium of claim 11, wherein the one or more features comprises at least one of body postures of the user, facial parameters of the user and audio parameters of the user.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318800B2,US10318800B2,Gesture detection and recognition method and system,,"method, purpose, region, realizing, classifiers, gesture, generated, changes, frequencies, matching, previous, acquiring, recognition, detect, model, distribution, deleting, based, output, pixel, eliminate, sequence, later, influence, skin, scene, thus, alternating, each, plurality, with, extract, image, different, state, adopting, frame, storing, specific, gradually, that, obtained, acute, gestures, detection, establishing, comprises, manner, under, target, change, disclosed, illumination, system, according, color, acquire, extracting","Disclosed are a gesture detection and recognition method and system. The gesture detection and recognition method comprises: acquiring and storing an image; adopting a plurality of pre-set classifiers for deleting different gestures to detect each frame of the image according to a pre-set sequence in a frame alternating manner to acquire a gesture target; establishing a skin color model based on the pixel distribution of the region of the gesture target; and acquiring gesture frequencies of the gesture target in a previous state and a later state according to the skin color model, and matching the gesture frequencies with a pre-set gesture state, so that a gesture change state is obtained and output. The gesture detection and recognition method and extract a skin color under specific scene, and can gradually eliminate the influence generated by acute illumination changes, thus realizing the purpose of extracting a gesture change state.","1. A gesture detection and recognition method, wherein: the method comprises the steps of:
A1. collecting images and storing the images;
A2. using preset multiple classifiers used for detecting different gestures to detect each frame of the images in a way of alternating every two frames in preset order, to acquire gesture targets;
A3, establishing skin color models based on pixel distribution in target area of the gestures;
A4. acquiring gesture frequency of said gesture target in two consecutive states according to the skin color model, and match said gesture frequency with preset gesture state, to acquire gesture transition state and output. 1. A gesture detection and recognition method, wherein: the method comprises the steps of:
A1. collecting images and storing the images;
A2. using preset multiple classifiers used for detecting different gestures to detect each frame of the images in a way of alternating every two frames in preset order, to acquire gesture targets;
A3, establishing skin color models based on pixel distribution in target area of the gestures;
A4. acquiring gesture frequency of said gesture target in two consecutive states according to the skin color model, and match said gesture frequency with preset gesture state, to acquire gesture transition state and output. 2. The gesture detection and recognition method of claim 1, wherein: said image is preprocessed before performing said step A2. 3. The gesture detection and recognition method of claim 1, wherein: each of said classifiers performs multi-scale target detection of said image through a preset sliding window, to obtain said gesture target. 4. The gesture detection and recognition method of claim 3, wherein: after said gesture target is obtained, said window is expanded by four times to detect said gesture target. 5. The gesture detection and recognition method of claim 1, wherein: said classifiers employ cascade classifiers. 6. A gesture detection and recognition system, wherein: the system comprises:
an acquisition unit for collecting images;
a storage unit connecting to said acquisition unit for storing said images;
a plurality of classifiers for detecting different gestures connecting to said storage unit for detecting each frame of said images in a way of alternating every two frames in preset order, to obtain gesture target;
a skin color modeling unit connecting to said storage unit for establishing skin color models based on pixel distribution in said target area of the gestures;
a decision making unit connecting to a plurality of said classifiers and said skin color modeling units respectively, acquiring said gesture frequency of said gesture targets in two consecutive states according to the skin color model, and match said gesture frequency with preset gesture state, to acquire gesture transition state and output. 6. A gesture detection and recognition system, wherein: the system comprises:
an acquisition unit for collecting images;
a storage unit connecting to said acquisition unit for storing said images;
a plurality of classifiers for detecting different gestures connecting to said storage unit for detecting each frame of said images in a way of alternating every two frames in preset order, to obtain gesture target;
a skin color modeling unit connecting to said storage unit for establishing skin color models based on pixel distribution in said target area of the gestures;
a decision making unit connecting to a plurality of said classifiers and said skin color modeling units respectively, acquiring said gesture frequency of said gesture targets in two consecutive states according to the skin color model, and match said gesture frequency with preset gesture state, to acquire gesture transition state and output. 7. The gesture detection and recognition system of claim 6, wherein: said acquisition unit employs video camera. 8. The gesture detection and recognition system of claim 6, wherein: said classifiers employ cascade classifiers. 9. The gesture detection and recognition system of claim 6, wherein: said classifiers all perform multi-scale target detection of said image through a preset sliding window, to obtain said gesture target. 10. The gesture detection and recognition system of claim 9, wherein: after said classifiers obtain said gesture target, said window will be expanded by four times, to detect said gesture target. 11. A gesture detection and recognition method, comprising the steps of:
providing an acquisition unit and a storage unit;
acquiring and storing a plurality of images;
providing a plurality of pre-set classifiers for detecting different gestures to detect each frame of said plurality of images according to a pre-set sequence in a frame alternating manner and acquiring a gesture target;
providing a skin color modeling unit based upon a pixel distribution of a region of said gesture target; and
providing a decision-making unit that acquires gesture frequencies of said gesture target in one of a previous state and a later state according to said skin color modeling unit. 11. A gesture detection and recognition method, comprising the steps of:
providing an acquisition unit and a storage unit;
acquiring and storing a plurality of images;
providing a plurality of pre-set classifiers for detecting different gestures to detect each frame of said plurality of images according to a pre-set sequence in a frame alternating manner and acquiring a gesture target;
providing a skin color modeling unit based upon a pixel distribution of a region of said gesture target; and
providing a decision-making unit that acquires gesture frequencies of said gesture target in one of a previous state and a later state according to said skin color modeling unit.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318802B2,US10318802B2,Worn banknote identification method and ATM using the same,2016-07-27,"method, region, disclosure, machine, present, reference, bright, distribution, depending, comparing, acquired, regions, reject, difference, dark, value, banknote, into, separately, level, average, image, same, brightness, inserted, values, preset, using, wear, worn, between, relates, divides, automated, pixels, determines, more, identification, acquires, which, teller, particularly, stores","The present disclosure relates to a worn banknote identification method and an ATM (Automated Teller Machine) using the same, and more particularly, to a worn banknote identification method which acquires an image of a banknote inserted into an ATM, divides pixels of the acquired banknote image into a bright region and dark region depending on a brightness distribution of the banknote image, determines the wear level of the banknote by comparing a difference between the average brightness values of the two regions to a preset reference value, and separately stores the inserted banknote into a reject box depending on the wear level of the banknote, and an ATM using the same.","1. A worn banknote identification method which determines the wear level of a banknote inserted into an ATM using an image of the banknote, comprising:
acquiring an image of the banknote through an image sensor;
calculating brightness values of pixels in the acquired banknote image by converting the banknote image into gray scales;
extracting a threshold value for determining the variance of the brightness values of the pixels in the banknote image;
dividing the pixels of the banknote image into a bright region and a dark region, based on the extracted threshold value;
calculating a difference between the average brightness values of the bright region and the dark region by calculating the average brightness value of the bright region and the average brightness value of the dark region; and
determining whether the banknote is a worn banknote, by comparing the calculated difference to a preset reference value, wherein a minimum wear level required to determine whether the banknote is recyclable is preset through a previous scan operation and stored as the reference value, and the banknote is determined to worn when the calculated difference is equal to or less than the reference value. 1. A worn banknote identification method which determines the wear level of a banknote inserted into an ATM using an image of the banknote, comprising:
acquiring an image of the banknote through an image sensor;
calculating brightness values of pixels in the acquired banknote image by converting the banknote image into gray scales;
extracting a threshold value for determining the variance of the brightness values of the pixels in the banknote image;
dividing the pixels of the banknote image into a bright region and a dark region, based on the extracted threshold value;
calculating a difference between the average brightness values of the bright region and the dark region by calculating the average brightness value of the bright region and the average brightness value of the dark region; and
determining whether the banknote is a worn banknote, by comparing the calculated difference to a preset reference value, wherein a minimum wear level required to determine whether the banknote is recyclable is preset through a previous scan operation and stored as the reference value, and the banknote is determined to worn when the calculated difference is equal to or less than the reference value. 2. The worn banknote identification method of claim 1, wherein the calculating of the brightness values of the pixels in the banknote image comprises extracting only brightness signals by removing color signals from the image of the inserted banknote. 3. The worn banknote identification method of claim 1, wherein the calculating of the brightness values of the pixels in the banknote image comprises extracting only brightness values of green data from the image of the inserted banknote. 4. The worn banknote identification method of claim 1, wherein the extracting of the threshold value comprises extracting the threshold value using the Otsu thresholding algorithm. 5. The worn banknote identification method of claim 4, wherein the extracting of the threshold value using the Otsu thresholding algorithm comprises finding a valley in a distribution graph of the brightness values of the pixels and extracting the valley as the threshold value. 6. The worn banknote identification method of claim 1, wherein in the calculating of the difference between the average brightness values of the two regions, Equation 2 below is applied:








δ
t

=



∑
t

L
-
1


⁢

i
⁢

 

⁢


n
i

/


∑
t

L
-
1


⁢

n
i





-


∑
0

t
-
1


⁢

i
⁢

 

⁢


n
i

/


∑
0

t
-
1


⁢

n
i










[

Equation
⁢

 

⁢
2

]






where δtrepresents the difference between the average brightness values of the bright region and the dark region, L represents the number of brightness value levels, t represents the threshold value, i represents a brightness value, and ni represents the number of pixels having the brightness value i.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318801B2,US10318801B2,Image processing apparatus and non-transitory computer readable medium,,"second, document, images, extraction, readable, third, accordance, medium, combining, computer, with, high, image, extracts, area, apparatus, plural, includes, characteristics, that, unit, processing, frequency, areas, transitory, having, whether, background, first, white","An image processing apparatus includes a first extraction unit, a second extraction unit, and a third extraction unit. The first extraction unit extracts a first area in an image that includes plural document images, the first area having low-frequency characteristics. The second extraction unit extracts a second area in the image, the second area having high-frequency characteristics. The third extraction unit extracts areas of the document images by combining the first area and the second area in accordance with whether a background of the image is white.","1. An image processing apparatus comprising:
at least one hardware processor to implement:
controlling a scanner to scan an image;
a first extraction unit that extracts a first area in the image that includes a plurality of document images, the first area having low-frequency characteristics;
a second extraction unit that extracts a second area in the image, the second area having high-frequency characteristics; and
a third extraction unit that extracts areas of the document images by combining the first area and the second area in accordance with whether a background of the image is white. 1. An image processing apparatus comprising:
at least one hardware processor to implement:
controlling a scanner to scan an image;
a first extraction unit that extracts a first area in the image that includes a plurality of document images, the first area having low-frequency characteristics;
a second extraction unit that extracts a second area in the image, the second area having high-frequency characteristics; and
a third extraction unit that extracts areas of the document images by combining the first area and the second area in accordance with whether a background of the image is white. 2. The image processing apparatus according to claim 1, further comprising
a determination unit that determines whether the background is white on a basis of a brightness value of a peripheral area of the image, wherein
the third extraction unit uses a result of determination by the determination unit. 3. The image processing apparatus according to claim 2, wherein
the determination unit determines whether the background of the image is white by using a result of determination as to whether four peripheral areas are each a white area. 4. The image processing apparatus according to claim 2, wherein
in a case where the determination unit determines that the background of the image is not white, the third extraction unit extracts the areas of the document images by using both the first area and the second area. 5. The image processing apparatus according to claim 4, wherein
in a case where the determination unit determines that the background of the image is white, the third extraction unit extracts the areas of the document images by preferentially using the second area. 6. The image processing apparatus according to claim 1, wherein
the first extraction unit extracts the first area by performing a binarization process. 7. The image processing apparatus according to claim 1, wherein
the second extraction unit extracts the second area by performing an edge detection process or a color difference detection process. 8. The image processing apparatus according to claim 2, further comprising
a display that performs display to prompt a user to specify whether the background is white or a color other than white or performs display to prompt the user to provide an instruction for a re-scan in a case where the determination unit fails to determine whether the background is white. 9. A non-transitory computer readable medium storing a program causing a computer to execute a process for image processing, the process comprising:
controlling a scanner to scan an image;
extracting a first area in the image that includes a plurality of document images, the first area having low-frequency characteristics;
extracting a second area in the image, the second area having high-frequency characteristics; and
extracting areas of the document images by combining the first area and the second area in accordance with whether a background of the image is white. 9. A non-transitory computer readable medium storing a program causing a computer to execute a process for image processing, the process comprising:
controlling a scanner to scan an image;
extracting a first area in the image that includes a plurality of document images, the first area having low-frequency characteristics;
extracting a second area in the image, the second area having high-frequency characteristics; and
extracting areas of the document images by combining the first area and the second area in accordance with whether a background of the image is white. 10. The image processing apparatus according to claim 1,
wherein the image comprises a plurality of business cards spaced apart from each other,
wherein the background comprises a background color,
wherein a first one of the business cards comprises a first color,
wherein a second one of the business cards comprises a second color, and
wherein the at least one hardware processor is configured to implement:
determining whether the background color is white from scanning the image;
extracting the first one of the business cards and the second one of the business cards differently depending on whether the background color is white.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318791B2,US10318791B2,Anti-spoofing sensing for rejecting fake fingerprint patterns in under-screen optical sensor module for on-screen fingerprint sensing,2017-07-18,"optical, module, spoofing, screen, light, based, returned, patterns, devices, fingerprints, captures, sensing, oled, anti, models, rejecting, using, fingerprint, that, capacitive, provided, sensor, under, provide, detects, fake, modules",Devices and optical sensor modules are provided for provide on-screen optical sensing of fingerprints by using a under-LCD or OLED screen optical sensor module that captures and detects returned light and anti-spoofing sensing for rejecting fake fingerprint models based on capacitive sensing or optical sensing.,"1. An electronic device capable of detecting a fingerprint by optical sensing, comprising:
a screen that provides touch sensing operations and includes a display panel structure to display images;
a top transparent layer formed over the device screen as an interface for being touched by a user for the touch sensing operations and for transmitting the light from the display structure to display images to a user;
an optical sensor module located below the display panel structure to receive probe light that passes through the screen to detect a fingerprint; and
a capacitive sensor array coupled to the screen to measure capacitance signals as a capacitive anti-spoofing sensor for rejecting a fake fingerprint pattern,
wherein the optical sensor module includes:
an optical sensor array of optical detectors to convert the received light from the top transparent layer and display panel that carries a fingerprint pattern of the user into detector signals representing the fingerprint pattern,
a pinhole layer located between the display panel and the optical sensor array and structured to include a single pinhole that is structured to produce a large optical field of view in collecting the received light and to transmit the collected light towards the optical sensor array, and
a lens located between the pinhole layer and the optical sensor array to receive the transmitted light from the single pinhole and to focus the received light onto the optical sensor array for optical imaging at an enhanced spatial imaging resolution at the optical sensor array in comparison with a lower spatial imaging resolution when using the pinhole to project light onto the optical sensor array without the lens. 1. An electronic device capable of detecting a fingerprint by optical sensing, comprising:
a screen that provides touch sensing operations and includes a display panel structure to display images;
a top transparent layer formed over the device screen as an interface for being touched by a user for the touch sensing operations and for transmitting the light from the display structure to display images to a user;
an optical sensor module located below the display panel structure to receive probe light that passes through the screen to detect a fingerprint; and
a capacitive sensor array coupled to the screen to measure capacitance signals as a capacitive anti-spoofing sensor for rejecting a fake fingerprint pattern,
wherein the optical sensor module includes:
an optical sensor array of optical detectors to convert the received light from the top transparent layer and display panel that carries a fingerprint pattern of the user into detector signals representing the fingerprint pattern,
a pinhole layer located between the display panel and the optical sensor array and structured to include a single pinhole that is structured to produce a large optical field of view in collecting the received light and to transmit the collected light towards the optical sensor array, and
a lens located between the pinhole layer and the optical sensor array to receive the transmitted light from the single pinhole and to focus the received light onto the optical sensor array for optical imaging at an enhanced spatial imaging resolution at the optical sensor array in comparison with a lower spatial imaging resolution when using the pinhole to project light onto the optical sensor array without the lens. 2. The device as in claim 1, wherein
the optical sensor module includes one or more optical filtering layers to transmit probe light for optical sensing of fingerprints while blocking background day light at optical wavelengths longer than the probe light. 3. The device as in claim 1, wherein the screen is a LCD screen. 4. The device as in claim 1, wherein the screen is an OLED screen. 5. The device as in claim 1, wherein
the capacitive sensor array is configured to measure an image contrast in the capacitance signals as a capacitive anti-spoofing sensor for rejecting a fake fingerprint pattern. 6. The device as in claim 1, wherein
the capacitive sensor array is configured to measure an integrated signal strength in the capacitance signals as a capacitive anti-spoofing sensor for rejecting a fake fingerprint pattern. 7. An electronic device capable of detecting a fingerprint by optical sensing, comprising:
a screen that provides touch sensing operations and includes a display panel structure to display images;
a top transparent layer formed over the device screen as an interface for being touched by a user for the touch sensing operations and for transmitting the light from the display structure to display images to a user; and
an optical sensor module located below the display panel structure to receive probe light that passes through the screen and configured (1) to detect a fingerprint by detecting and extracting an image carried by the received probe light and (2) to measure optical absorption signals at different optical wavelengths associated with different optical absorption characteristics by human blood at the different optical wavelengths for rejecting a fake fingerprint pattern free of human blood, wherein the optical sensor module includes an optical sensor array of optical detectors to convert the received light from the top transparent layer and display panel that carries a fingerprint pattern of the user into detector signals representing the fingerprint pattern, a pinhole layer located between the display panel and the optical sensor array and structured to include a single pinhole that is structured to produce a large optical field of view in collecting the received light and to transmit the collected light towards the optical sensor array, and a lens located between the pinhole layer and the optical sensor array to receive the transmitted light from the single pinhole and to focus the received light onto the optical sensor array for optical imaging at an enhanced spatial imaging resolution at the optical sensor array in comparison with a lower spatial imaging resolution when using the pinhole to project light onto the optical sensor array without the lens. 7. An electronic device capable of detecting a fingerprint by optical sensing, comprising:
a screen that provides touch sensing operations and includes a display panel structure to display images;
a top transparent layer formed over the device screen as an interface for being touched by a user for the touch sensing operations and for transmitting the light from the display structure to display images to a user; and
an optical sensor module located below the display panel structure to receive probe light that passes through the screen and configured (1) to detect a fingerprint by detecting and extracting an image carried by the received probe light and (2) to measure optical absorption signals at different optical wavelengths associated with different optical absorption characteristics by human blood at the different optical wavelengths for rejecting a fake fingerprint pattern free of human blood, wherein the optical sensor module includes an optical sensor array of optical detectors to convert the received light from the top transparent layer and display panel that carries a fingerprint pattern of the user into detector signals representing the fingerprint pattern, a pinhole layer located between the display panel and the optical sensor array and structured to include a single pinhole that is structured to produce a large optical field of view in collecting the received light and to transmit the collected light towards the optical sensor array, and a lens located between the pinhole layer and the optical sensor array to receive the transmitted light from the single pinhole and to focus the received light onto the optical sensor array for optical imaging at an enhanced spatial imaging resolution at the optical sensor array in comparison with a lower spatial imaging resolution when using the pinhole to project light onto the optical sensor array without the lens. 8. The device as in claim 7, wherein
the optical sensor module includes one or more optical filtering layers to transmit probe light for optical sensing of fingerprints while blocking background day light at optical wavelengths longer than the probe light. 9. The device as in claim 7, wherein the screen is a LCD screen. 10. The device as in claim 7, wherein the screen is an OLED screen. 11. A method for operating an electronic device that includes an optical sensor module for optically detecting a fingerprint and a capacitive fingerprint senor module for capacitively detecting the fingerprint, comprising:
operating a capacitive fingerprint sensor module having an array of capacitive sensors to obtain capacitive sensor signals representing an input fingerprint pattern of an object;
processing the obtained capacitive sensor signals to obtain a capacitive sensor signal parameter that includes a capacitive signal strength, an image constructed from the obtained capacitive sensor signals or an image contrast of the image constructed from the obtained capacitive sensor signals;
applying the capacitive sensor signal parameter to determine whether the object is a finger of a live user;
operating an optical fingerprint sensor module to optically sense whether the object is a finger of a live user and to obtain an optical image of fingerprint of the object;
combining measurements by both the optical fingerprint sensor module and the capacitive fingerprint sensor module to authenticate whether a measured fingerprint is from a finger of a live user and whether the fingerprint is from an authorized user,
operating the optical fingerprint sensor module to obtain an optical image of the object;
processing the obtained optical image of the object to determine whether the obtained optical image is a composition of two different images; and
denying access when the obtained optical image is a composition of two different images. 11. A method for operating an electronic device that includes an optical sensor module for optically detecting a fingerprint and a capacitive fingerprint senor module for capacitively detecting the fingerprint, comprising:
operating a capacitive fingerprint sensor module having an array of capacitive sensors to obtain capacitive sensor signals representing an input fingerprint pattern of an object;
processing the obtained capacitive sensor signals to obtain a capacitive sensor signal parameter that includes a capacitive signal strength, an image constructed from the obtained capacitive sensor signals or an image contrast of the image constructed from the obtained capacitive sensor signals;
applying the capacitive sensor signal parameter to determine whether the object is a finger of a live user;
operating an optical fingerprint sensor module to optically sense whether the object is a finger of a live user and to obtain an optical image of fingerprint of the object;
combining measurements by both the optical fingerprint sensor module and the capacitive fingerprint sensor module to authenticate whether a measured fingerprint is from a finger of a live user and whether the fingerprint is from an authorized user,
operating the optical fingerprint sensor module to obtain an optical image of the object;
processing the obtained optical image of the object to determine whether the obtained optical image is a composition of two different images; and
denying access when the obtained optical image is a composition of two different images. 12. The method as in claim 11, wherein the operation of the optical fingerprint sensor module to optically sense whether the object is a finger of a live user includes:
illuminating the object with light at two different optical wavelengths at which human blood exhibits different optical absorption characteristics; and
using optical measurements obtained at the two different optical wavelengths to determine whether the object is a finger of a live user based on the different optical absorption characteristics in the optical measurements at the two different optical wavelengths. 13. The method as in claim 11, wherein the operation of the optical fingerprint sensor module to optically sense whether the object is a finger of a live user includes:
operating the optical fingerprint sensor module to capture different fingerprint patterns of the object at different times to monitor time-domain evolution of a pattern deformation that indicates time-domain evolution of a press force from the object to determine whether the object is a finger of a live user. 14. The method as in claim 11, further comprising:
operating the capacitive fingerprint sensor module first before turning on the optical fingerprint sensor module to use the capacitive sensor signal parameter as a trigger for turning the optical fingerprint sensor module. 15. The method as in claim 11, further comprising:
turning on the capacitive fingerprint sensor module for capacitive sensing without turning on the optical fingerprint sensor module; and
using a result of the capacitive sensing as trigger to determine whether or not to turn on the optical fingerprint sensor module for optical sensing. 16. The method as in claim 11, further comprising:
turning on both the capacitive fingerprint sensor module for capacitive sensing and the optical fingerprint sensor module for optical sensing to perform the capacitive sensing and the optical sensing in parallel; and
using results of both the capacitive sensing and optical sensing to determine whether to grant access to the electronic device. 17. A method for operating an electronic device that includes an optical sensor module for optically detecting a fingerprint and a capacitive fingerprint senor module for capacitively detecting the fingerprint, comprising:
operating a capacitive fingerprint sensor module having an array of capacitive sensors to obtain capacitive sensor signals representing an input fingerprint pattern of an object;
processing the obtained capacitive sensor signals to obtain an image of the input fingerprint pattern;
operating an optical fingerprint sensor module to optically sense whether the object is a finger of a live user based on based on different optical absorption characteristics of human blood at different optical wavelengths and to obtain an optical image of the input fingerprint pattern of the object;
combining measurements by both the optical fingerprint sensor module and the capacitive fingerprint sensor module to authenticate whether a measured fingerprint is from a finger of a live user and whether the fingerprint is from an authorized user;
processing the obtained optical image of the object to determine whether the obtained optical image is a composition of two different images; and
denying access when the obtained optical image is a composition of two different images. 17. A method for operating an electronic device that includes an optical sensor module for optically detecting a fingerprint and a capacitive fingerprint senor module for capacitively detecting the fingerprint, comprising:
operating a capacitive fingerprint sensor module having an array of capacitive sensors to obtain capacitive sensor signals representing an input fingerprint pattern of an object;
processing the obtained capacitive sensor signals to obtain an image of the input fingerprint pattern;
operating an optical fingerprint sensor module to optically sense whether the object is a finger of a live user based on based on different optical absorption characteristics of human blood at different optical wavelengths and to obtain an optical image of the input fingerprint pattern of the object;
combining measurements by both the optical fingerprint sensor module and the capacitive fingerprint sensor module to authenticate whether a measured fingerprint is from a finger of a live user and whether the fingerprint is from an authorized user;
processing the obtained optical image of the object to determine whether the obtained optical image is a composition of two different images; and
denying access when the obtained optical image is a composition of two different images. 18. The method as in claim 17, further comprising:
turning on the capacitive fingerprint sensor module for capacitive sensing without turning on the optical fingerprint sensor module; and
using a result of the capacitive sensing as trigger to determine whether or not to turn on the optical fingerprint sensor module for optical sensing. 19. The method as in claim 17, further comprising:
turning on both the capacitive fingerprint sensor module for capacitive sensing and the optical fingerprint sensor module for optical sensing to perform the capacitive sensing and the optical sensing in parallel; and
using results of both the capacitive sensing and optical sensing to determine whether to grant access to the electronic device. 20. The method as in claim 17, further comprising:
processing the obtained capacitive sensor signals to also obtain a capacitive sensor signal parameter that includes a capacitive signal strength, an image constructed from the obtained capacitive sensor signals or an image contrast of the image constructed from the obtained capacitive sensor signals; and
applying the capacitive sensor signal parameter to determine whether the object is a finger of a live user. 21. The method as in claim 17, further comprising:
operating the optical fingerprint sensor module to capture different fingerprint patterns of the object at different times to monitor time-domain evolution of a pattern deformation that indicates time-domain evolution of a press force from the object.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318804B2,US10318804B2,System and method for data extraction and searching,2014-06-30,"method, optical, textual, document, without, quickly, receiving, images, recognition, extraction, scanned, extracted, based, context, systems, data, aspects, well, related, results, subsequently, estate, plurality, with, searching, obtain, searches, methods, real, performed, documents, processing, provided, contextualized, more, information, efficiently, human, least, representations, search, system, minimal, from, character, according","Systems and methods are provided for quickly and efficiently searching and receiving results for real estate-related information without or at least with minimal human processing of real estate-related documents. Optical character recognition on a plurality of scanned document images is performed to obtain a plurality of textual data representations of the real estate-related documents. Data is extracted from the textual data representations, and subsequently contextualized according to a real estate-related context. Aspects of the extracted data as well as the textual data representations are provided as search results based on one or more searches for real estate-related information.","1. A non-transitory computer-readable medium having computer executable program code embodied thereon, the computer executable program code configured to cause a computer system to:
perform image pre-processing on a document image;
perform optical character recognition on the document image;
create a textual data representation of the document image;
apply processing rules to the textual data representation;
perform one or more extractions based on the processing rules to extract metadata from the textual data representation and contextualize the extracted metadata;
perform automated validation of the contextualization of the extracted metadata;
store the extracted metadata and the textual data representation in a full text index database;
transfer the extracted metadata and the textual data representation from the full text index database to a search engine platform, the search engine platform indexing and storing the transferred extracted metadata to allow for searching of the indexed, extracted metadata, the indexed, extracted metadata having been correlated to the textual data representation, wherein the search engine platform allows for the selection of extracted metadata stored in full text index database that is transferred to the search engine platform;
perform additional validation of the extracted metadata upon transfer of the extracted metadata to the search engine platform by correlating at least one of the extracted metadata and the textual data representation with at least one additional reference document relevant to the at least one of the extracted metadata and the textual data representation;
display at the search engine platform a plurality of facets for filtering of search results; and
cause the search engine platform to learn from facets utilized in prior searches how to access data from the full text index database. 1. A non-transitory computer-readable medium having computer executable program code embodied thereon, the computer executable program code configured to cause a computer system to:
perform image pre-processing on a document image;
perform optical character recognition on the document image;
create a textual data representation of the document image;
apply processing rules to the textual data representation;
perform one or more extractions based on the processing rules to extract metadata from the textual data representation and contextualize the extracted metadata;
perform automated validation of the contextualization of the extracted metadata;
store the extracted metadata and the textual data representation in a full text index database;
transfer the extracted metadata and the textual data representation from the full text index database to a search engine platform, the search engine platform indexing and storing the transferred extracted metadata to allow for searching of the indexed, extracted metadata, the indexed, extracted metadata having been correlated to the textual data representation, wherein the search engine platform allows for the selection of extracted metadata stored in full text index database that is transferred to the search engine platform;
perform additional validation of the extracted metadata upon transfer of the extracted metadata to the search engine platform by correlating at least one of the extracted metadata and the textual data representation with at least one additional reference document relevant to the at least one of the extracted metadata and the textual data representation;
display at the search engine platform a plurality of facets for filtering of search results; and
cause the search engine platform to learn from facets utilized in prior searches how to access data from the full text index database. 2. The non-transitory computer-readable medium of claim 1, wherein the computer executable program code is configured to further cause the computer system to receive pre-processing rules feedback for performance of the image pre-processing based upon optical character recognition correction information obtained from correction of the textual data representation subsequent to the performance of the optical character recognition. 3. The non-transitory computer-readable medium of claim 1, wherein the computer executable program code for performing the image pre-processing is configured to cause the computer system to apply a preferred pre-processing profile. 4. The non-transitory computer-readable medium of claim 3, wherein the computer executable program code is configured to further cause the computer system to determine if an output from performance of the optical character recognition meets a desired accuracy threshold. 5. The non-transitory computer-readable medium of claim 4, wherein the computer executable program code is configured to further cause the computer system to output the textual data representation for performance of the one or more extractions upon determining that the output from performance of the optical character recognition meets the desired accuracy threshold. 6. The non-transitory computer-readable medium of claim 4, wherein the computer executable program code is configured to further cause the computer system to apply multiple pre-processing profiles upon which performance of the optical character recognition is based, select the most accurate output from performance of the optical character recognition, and output the textual data representation resulting from the most accurate output from performance of the optical character recognition. 7. The non-transitory computer-readable medium of claim 1, wherein each of the multiple pre-processing profiles comprises at least one combination of image enhancement processing, automatically applied pre-processing, erosion processing, and dilation processing. 8. The non-transitory computer-readable medium of claim 1, wherein the computer executable program code is configured to further cause the computer system to receive research and training feedback for application of the processing rules based upon extraction error correction information obtained from correction of the extracted metadata subsequent to the performance of the one or more extractions. 9. The non-transitory computer-readable medium of claim 1, wherein the one or more extractions comprise at least one of natural language processing extraction, coordinate-based extraction, quadrant-based extraction, and regular expression extraction. 10. The non-transitory computer-readable medium of claim 1, wherein the computer executable program code configured to perform the automated validation of the extracted metadata further causes the computer system to access existing collected data with which to compare the extracted metadata to validate the contextualization of the extracted metadata. 11. The non-transitory computer-readable medium of claim 1, wherein the document image comprises a scanned image of a real estate document. 12. The non-transitory computer-readable medium of claim 11, wherein the extracted metadata comprises at least one of a title of the real estate document, state information associated with the real estate document, county information associated with the real estate document, address of a property relevant to the real estate document, an assessor's parcel number associated with the property, any party name indicated in the real estate document, a document number associated with the real estate document, a recording date associated with the real estate document, any reference document related to the real estate document, and any legal description of the property. 13. The computer program product of claim 1, wherein the computer executable program code is configured to further cause the search engine platform to learn from prior searches conducted on the search engine platform how to access data from the full text index database. 14. The non-transitory computer-readable medium of claim 1, wherein the search engine facets comprise a type of real estate-related document, a county associated property at issue in a real estate-related document, or a date range associated with a real-estate related document. 15. The non-transitory computer-readable medium of claim 14, wherein the search engine facets comprise a type of real estate-related document. 16. The non-transitory computer-readable medium of claim 14, wherein the search engine facets comprise a county associated property at issue in a real estate-related document. 17. A computer-implemented method, comprising:
receiving a plurality of scanned document images representative of real estate-related documents;
performing optical character recognition on the plurality of scanned document images to obtain a plurality of textual data representations of the real estate-related documents;
extracting metadata from the textual data representations of the real estate-related documents, and contextualizing the extracted metadata in a real estate-related context;
performing automated validation of the contextualization of the extracted metadata;
storing the extracted metadata and the textual data representation in a full text index database;
transferring a subset of the extracted metadata and the textual data representation from the full text index database to a search engine platform, wherein the search engine platform selects the subset of extracted metadata stored in the full text index database that is transferred to the search engine platform, the search engine platform indexing and storing the transferred subset of the extracted metadata to allow for searching of the indexed, subset of extracted metadata, the indexed, subset of extracted metadata having been correlated to the textual data representation;
performing validation of the extracted metadata by correlating the extracted metadata with at least one additional reference document relevant to the extracted metadata;
providing a link to the at least one additional reference document and one or more elements of the extracted subset of metadata and the textual data representations as search results based on one or more searches for real estate-related information using the search engine platform;
displaying at the search engine platform a plurality of facets for filtering of the search results; and
causing the search engine platform to learn from facets utilized in prior searches how to access data from the full text index database. 17. A computer-implemented method, comprising:
receiving a plurality of scanned document images representative of real estate-related documents;
performing optical character recognition on the plurality of scanned document images to obtain a plurality of textual data representations of the real estate-related documents;
extracting metadata from the textual data representations of the real estate-related documents, and contextualizing the extracted metadata in a real estate-related context;
performing automated validation of the contextualization of the extracted metadata;
storing the extracted metadata and the textual data representation in a full text index database;
transferring a subset of the extracted metadata and the textual data representation from the full text index database to a search engine platform, wherein the search engine platform selects the subset of extracted metadata stored in the full text index database that is transferred to the search engine platform, the search engine platform indexing and storing the transferred subset of the extracted metadata to allow for searching of the indexed, subset of extracted metadata, the indexed, subset of extracted metadata having been correlated to the textual data representation;
performing validation of the extracted metadata by correlating the extracted metadata with at least one additional reference document relevant to the extracted metadata;
providing a link to the at least one additional reference document and one or more elements of the extracted subset of metadata and the textual data representations as search results based on one or more searches for real estate-related information using the search engine platform;
displaying at the search engine platform a plurality of facets for filtering of the search results; and
causing the search engine platform to learn from facets utilized in prior searches how to access data from the full text index database. 18. The computer-implemented method of claim 17, wherein the plurality of displayed facets comprise a plurality of real estate-related facets, the method further comprising: providing the one or more elements of the extracted subset of metadata and the textual data representations as search results categorized in accordance with the plurality of real-estate related facets. 19. A non-transitory computer-readable medium having computer executable program code embodied thereon, the computer executable program code configured to cause a computer system to:
perform optical character recognition on a plurality of scanned document images to obtain a plurality of textual data representations of the real estate-related documents;
extract metadata from the textual data representations of the real estate-related documents, and contextualize the extracted metadata in a real estate-related context;
store the extracted metadata and the textual data representation in a full text index database;
transfer a subset of the extracted metadata and the textual data representation from the full text index database to a search engine platform, wherein the search engine platform selects the subset of extracted metadata stored in the full text index database that is transferred to the search engine platform, the search engine platform indexing and storing the transferred subset of the extracted metadata to allow for searching of the indexed, subset of extracted metadata, the indexed, subset of extracted metadata having been correlated to the textual data representation;
perform validation of the extracted metadata by correlating the extracted metadata with at least one additional reference document relevant to the extracted metadata;
provide data from the at least one additional reference document and at least one of the extracted metadata and the textual data representations as search results based on one or more searches for real estate-related information performed via the search engine platform; and
display at the search engine platform a plurality of real-estate related facets for filtering of the search results; and
cause the search engine platform to learn from real-estate related facets utilized in prior searches how to access real estate-related data from the full text index database. 19. A non-transitory computer-readable medium having computer executable program code embodied thereon, the computer executable program code configured to cause a computer system to:
perform optical character recognition on a plurality of scanned document images to obtain a plurality of textual data representations of the real estate-related documents;
extract metadata from the textual data representations of the real estate-related documents, and contextualize the extracted metadata in a real estate-related context;
store the extracted metadata and the textual data representation in a full text index database;
transfer a subset of the extracted metadata and the textual data representation from the full text index database to a search engine platform, wherein the search engine platform selects the subset of extracted metadata stored in the full text index database that is transferred to the search engine platform, the search engine platform indexing and storing the transferred subset of the extracted metadata to allow for searching of the indexed, subset of extracted metadata, the indexed, subset of extracted metadata having been correlated to the textual data representation;
perform validation of the extracted metadata by correlating the extracted metadata with at least one additional reference document relevant to the extracted metadata;
provide data from the at least one additional reference document and at least one of the extracted metadata and the textual data representations as search results based on one or more searches for real estate-related information performed via the search engine platform; and
display at the search engine platform a plurality of real-estate related facets for filtering of the search results; and
cause the search engine platform to learn from real-estate related facets utilized in prior searches how to access real estate-related data from the full text index database.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318805B2,US10318805B2,Pattern matching method and apparatus,,"method, first, during, region, determination, second, capturing, wider, matching, surface, captured, calculates, occurred, than, patterns, data, operator, appearance, similarity, partial, steps, evaluation, result, degree, basis, work, when, range, image, apparatus, between, burden, that, unit, placed, processing, obtained, manufacturing, having, target, heavy, sample, pattern, threshold, search, been, would, template, fluctuation, decreases","When the degree of matching between patterns decreases due to a pattern fluctuation or an appearance fluctuation that has occurred during manufacturing steps, a heavy work burden would be placed on an operator. A data processing unit of a pattern matching apparatus calculates a threshold for determination of matching between a first template image and a partial region of a search target image obtained by capturing an image of the surface of a sample, on the basis of a result of evaluation of a similarity between the search target image and a second template image, the second template image having been captured in a wider range than the first template image.","1. A pattern matching method for matching a search target image comprising:
capturing a search target image formed based on an output of a particle detector configured to detect a secondary particle emitted from an irradiation region of a charged particle beam;
selecting a first template image and a second template image, the second template image having a field of view wider than a field of view of the first template image;
overlaying the search target image and the second template image one on top of the other;
determining, using an image processor, a similarity degree comprising a correlation value, of a common region of the overlaid search target image and the second template image;
adjusting, using the image processor, a similarity threshold on the basis of a result of the evaluation of the similarity degree using the first template image, wherein the similarity threshold determines a matching position or a candidate of the matching position from the search target image, and wherein the adjusting of the similarity threshold is performed in accordance with the correlation value such that the similarity threshold decreases as the correlation value decreases;
searching, using the image processor, for the matching position and the candidate of the matching position in the search target image, using the first template image and the adjusted similarity threshold, within the search target image whose similarity degree has been evaluated; and
extracting a region of the search target image having a pattern similarity which exceeds the adjusted similarity threshold. 1. A pattern matching method for matching a search target image comprising:
capturing a search target image formed based on an output of a particle detector configured to detect a secondary particle emitted from an irradiation region of a charged particle beam;
selecting a first template image and a second template image, the second template image having a field of view wider than a field of view of the first template image;
overlaying the search target image and the second template image one on top of the other;
determining, using an image processor, a similarity degree comprising a correlation value, of a common region of the overlaid search target image and the second template image;
adjusting, using the image processor, a similarity threshold on the basis of a result of the evaluation of the similarity degree using the first template image, wherein the similarity threshold determines a matching position or a candidate of the matching position from the search target image, and wherein the adjusting of the similarity threshold is performed in accordance with the correlation value such that the similarity threshold decreases as the correlation value decreases;
searching, using the image processor, for the matching position and the candidate of the matching position in the search target image, using the first template image and the adjusted similarity threshold, within the search target image whose similarity degree has been evaluated; and
extracting a region of the search target image having a pattern similarity which exceeds the adjusted similarity threshold. 2. The pattern matching method according to claim 1, further comprising, when the result of the matching between the first template image and the search target image is below the similarity threshold:
registering, using the image processor, a first template image newly set in or around the search target image, and a new second template image including the newly set first template image or a pattern around the newly set first template image. 3. The pattern matching method according to claim 2, further comprising:
registering, using the image processor, position information on alignment reference position coordinates corresponding to the first template image, and position information on the first and second template images, together with the first and second template images. 4. The pattern matching method according to claim 1, further comprising, when a plurality of regions whose similarities are evaluated to be above the similarity threshold are detected in the search target image:
setting, with the image processor, a region with a highest reliability among the plurality of regions as a matched region on the basis of position information on the second template image and the first template image. 5. A pattern matching apparatus for matching a search target image, the apparatus comprising:
a charged particle beam source configured to irradiate a surface of a sample with a charged particle beam;
a particle detector configured to detect a secondary particle emitted from an irradiation region of the charged particle beam; and
an image processor configured to perform image processing based on an output of the particle detector,
wherein a storage device accessible by the image processor comprises a non-transitory memory storing a first template image for local matching, a second template image for global matching having a field of view wider than the first template image, and a sequence of programmed instructions which, when executed by the image processor, cause the image processor to
capture a search target image formed based on an output of the particle detector;
overlay the search target image and the second template image one on top of the other;
determine a similarity degree comprising a correlation value of a common region of the overlaid search target image and the second template image;
adjust a threshold value used for determining a matching candidate in accordance with the correlation value such that the similarity threshold decreases as the correlation value decreases; and
perform template matching using the first template image and the adjusted threshold value within the search target image whose similarity degree has been evaluated; and
extract a region of the search target image having a pattern similarity which exceeds the adjusted similarity threshold. 5. A pattern matching apparatus for matching a search target image, the apparatus comprising:
a charged particle beam source configured to irradiate a surface of a sample with a charged particle beam;
a particle detector configured to detect a secondary particle emitted from an irradiation region of the charged particle beam; and
an image processor configured to perform image processing based on an output of the particle detector,
wherein a storage device accessible by the image processor comprises a non-transitory memory storing a first template image for local matching, a second template image for global matching having a field of view wider than the first template image, and a sequence of programmed instructions which, when executed by the image processor, cause the image processor to
capture a search target image formed based on an output of the particle detector;
overlay the search target image and the second template image one on top of the other;
determine a similarity degree comprising a correlation value of a common region of the overlaid search target image and the second template image;
adjust a threshold value used for determining a matching candidate in accordance with the correlation value such that the similarity threshold decreases as the correlation value decreases; and
perform template matching using the first template image and the adjusted threshold value within the search target image whose similarity degree has been evaluated; and
extract a region of the search target image having a pattern similarity which exceeds the adjusted similarity threshold. 6. The pattern matching apparatus according to claim 5, wherein
the image processor is further configured to, when the result of matching between the first template image and the search target image is below the similarity threshold, register a first template image newly set in or around the search target image and a new second template image including the newly set first template image or a pattern around the newly set first template image. 7. The pattern matching apparatus according to claim 6, wherein the image processor is further configured to register position information on alignment reference position coordinates corresponding to the first template image, and position information on the first and second template images, together with the first and second template images. 8. The pattern matching apparatus according to claim 5, wherein the image processor is further configured to, when a plurality of regions whose similarities are evaluated to be above the similarity threshold are detected from the search target image, set, as a matched region, a region with a highest reliability among the plurality of regions on the basis of position information on the second template image and the first template image. 9. A non-transitory computer-readable medium, storing program instructions executed on a computer system for performing a computer-implemented method for performing template matching using a template image, which, when executed by the computer system, cause the computer system to perform the computer-implemented method comprising:
capturing a search target image formed based on an output of a particle detector configured to detect a secondary particle emitted from an irradiation region of a charged particle beam;
selecting a first template image and a second template image, the second template image having a field of view wider than a field of view of the first template image;
overlaying the search target image and the second template image one on top of the other;
determining a similarity degree comprising a correlation value of a common region of the overlaid search target image and the second template image;
adjusting a similarity threshold used for determining matching, as the threshold to determine a matching candidate, wherein when performing a search using the first template image, wherein the adjusting of the similarity threshold is performed in accordance with the correlation value such that the similarity threshold decreases as the correlation value decreases;
performing template matching using the first template image and the adjusted threshold within the search target image whose similarity degree has been evaluated; and
extracting a region of the search target image having a pattern similarity which exceeds the adjusted similarity threshold. 9. A non-transitory computer-readable medium, storing program instructions executed on a computer system for performing a computer-implemented method for performing template matching using a template image, which, when executed by the computer system, cause the computer system to perform the computer-implemented method comprising:
capturing a search target image formed based on an output of a particle detector configured to detect a secondary particle emitted from an irradiation region of a charged particle beam;
selecting a first template image and a second template image, the second template image having a field of view wider than a field of view of the first template image;
overlaying the search target image and the second template image one on top of the other;
determining a similarity degree comprising a correlation value of a common region of the overlaid search target image and the second template image;
adjusting a similarity threshold used for determining matching, as the threshold to determine a matching candidate, wherein when performing a search using the first template image, wherein the adjusting of the similarity threshold is performed in accordance with the correlation value such that the similarity threshold decreases as the correlation value decreases;
performing template matching using the first template image and the adjusted threshold within the search target image whose similarity degree has been evaluated; and
extracting a region of the search target image having a pattern similarity which exceeds the adjusted similarity threshold.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318776B2,US10318776B2,Barcode-reading system that uses characteristics of a captured image to verify license entitlement,2016-09-19,"reading, verify, camera, conditional, device, configured, upon, captured, securable, mode, view, application, indication, operation, enhanced, stored, barcode, entitlement, field, image, license, characteristics, enhancement, include, that, memory, includes, accessory, also, processor, enable, determining, provide, uses, system, assembly, executable, mobile",A barcode-reading system for a mobile device may include a camera assembly. The barcode-reading system may include a barcode-reading enhancement accessory and a barcode-reading application. The barcode-reading enhancement accessory may be securable to the mobile device and may be configured to provide an indication of license entitlement in a field of view of the camera assembly. The barcode-reading application may be stored in memory of the mobile device and executable by a processor of the mobile device. The barcode-reading application may also be configured to enable an enhanced mode of operation of the barcode-reading application conditional upon determining that an image captured by the camera assembly includes the indication of license entitlement.,"1. A barcode-reading system for a mobile device, the mobile device comprising a camera assembly, the barcode-reading system comprising:
a barcode-reading enhancement accessory securable to the mobile device, the barcode-reading enhancement accessory being configured to provide an indication of license entitlement in a field of view of the camera assembly; and
a barcode-reading application stored in memory of the mobile device and executable by a processor of the mobile device, the barcode-reading application being configured to enable an enhanced mode of operation of the barcode-reading application conditional upon determining that an image captured by the camera assembly comprises the indication of license entitlement. 1. A barcode-reading system for a mobile device, the mobile device comprising a camera assembly, the barcode-reading system comprising:
a barcode-reading enhancement accessory securable to the mobile device, the barcode-reading enhancement accessory being configured to provide an indication of license entitlement in a field of view of the camera assembly; and
a barcode-reading application stored in memory of the mobile device and executable by a processor of the mobile device, the barcode-reading application being configured to enable an enhanced mode of operation of the barcode-reading application conditional upon determining that an image captured by the camera assembly comprises the indication of license entitlement. 2. The barcode-reading system of claim 1, wherein determining that the image comprises the indication of license entitlement comprises determining that the barcode-reading enhancement accessory has affected the image. 3. The barcode-reading system of claim 2, wherein the barcode-reading enhancement accessory includes an obstruction component obstructing the field of view of the camera assembly when the barcode-reading enhancement accessory is secured to the mobile device, the obstruction providing the indication of license entitlement. 4. The barcode-reading system of claim 3, wherein:
the indication of license entitlement comprises a license identifier;
the license identifier is printed on a section of the barcode-reading enhancement accessory that obstructs the field of view of the camera assembly when the barcode-reading enhancement accessory is secured to the mobile device; and
determining that the image comprises the indication of license entitlement comprises locating the license identifier in the image. 5. The barcode-reading system of claim 3, wherein:
the barcode-reading enhancement accessory comprises a reflective surface; and
the reflective surface is positioned so that when the barcode-reading enhancement accessory is secured to the mobile device, only a portion of the field of view of the camera assembly is folded towards a target area beyond a top side of the mobile device, and at least one other portion of the field of view of the camera assembly is obstructed by the barcode-reading enhancement accessory. 6. The barcode-reading system of claim 5 wherein a license identifier is printed on the portion of the barcode-reading enhancement accessory obstructing the field of view. 7. The barcode-reading system of claim 6, wherein:
the barcode-reading enhancement accessory further comprises a focusing lens; and
the focusing lens is positioned between the camera assembly and a section of the barcode-reading enhancement accessory on which the license identifier is printed when the barcode-reading enhancement accessory is secured to the mobile device. 8. The barcode-reading system of claim 2, wherein the barcode-reading enhancement accessory projects the indication of license entitlement into the field of view such that the indication of license entitlement is present in the captured image. 9. The barcode-reading system of claim 8, wherein:
the indication of license entitlement comprises a targeting pattern; and
the barcode-reading application is configured to enable the enhanced mode of operation conditional upon locating the targeting pattern in the image. 10. The barcode-reading system of claim 9, wherein:
the barcode-reading enhancement accessory comprises a target-generating structure that comprises at least one light source; and
the target-generating structure performs at least one of shaping or filtering the at least one light source to produce the targeting pattern. 11. The barcode-reading system of claim 9, wherein:
the barcode-reading enhancement accessory comprises a target-generating structure; and
the barcode-reading enhancement accessory performs at least one of shaping or filtering a light source of the mobile device to produce the targeting pattern. 12. The barcode-reading system of claim 9, wherein:
the barcode-reading enhancement accessory comprises an aperture that restricts illumination emitted by a light source to produce a shape of the targeting pattern; and
determining that the image comprises the indication of license entitlement comprises locating the shape of the targeting pattern within the image. 13. The barcode-reading system of claim 9, wherein:
the barcode-reading enhancement accessory comprises a filter that passes a limited spectrum of illumination emitted by a light source so that the targeting pattern, when incident on a surface, appears as a color of the limited spectrum passed by the filter; and
determining that the image comprises the indication of license entitlement comprises locating the color of the limited spectrum within the image. 14. The barcode-reading system of claim 8 wherein the indication of license entitlement is a sequential illumination pattern projected into the field of view by the barcode-reading enhancement accessory, the sequential illumination pattern being generated by an illuminator being sequences on and off in a predetermined pattern. 15. A barcode-reading enhancement accessory for a mobile device operating a barcode reading application, the mobile device comprising a camera assembly having a field of view, the barcode-reading enhancement accessory comprising:
an optics module; and
a reflective surface within the optics module, the reflective surface being positioned so as to fold a portion of the field of view of the camera assembly towards a target area beyond a top side of the mobile device when the barcode-reading enhancement accessory is secured to the mobile device;
wherein the barcode-reading enhancement accessory is configured to provide an indication of license entitlement for the barcode reading application in the field of view of the camera assembly. 15. A barcode-reading enhancement accessory for a mobile device operating a barcode reading application, the mobile device comprising a camera assembly having a field of view, the barcode-reading enhancement accessory comprising:
an optics module; and
a reflective surface within the optics module, the reflective surface being positioned so as to fold a portion of the field of view of the camera assembly towards a target area beyond a top side of the mobile device when the barcode-reading enhancement accessory is secured to the mobile device;
wherein the barcode-reading enhancement accessory is configured to provide an indication of license entitlement for the barcode reading application in the field of view of the camera assembly. 16. The barcode-reading enhancement accessory of claim 15, wherein providing the indication of license entitlement in the field of view of the camera assembly affects an image captured by the camera assembly of the mobile device. 17. The barcode-reading enhancement accessory of claim 16, wherein the barcode-reading enhancement accessory includes an obstruction component obstructing the field of view of the camera assembly when the barcode-reading enhancement accessory is secured to the mobile device, the obstruction providing the indication of license entitlement. 18. The barcode-reading enhancement accessory of claim 17, wherein:
the indication of license entitlement comprises a license identifier;
the license identifier is printed on a section of the barcode-reading enhancement accessory that obstructs the field of view of the camera assembly when the barcode-reading enhancement accessory is secured to the mobile device; and
determining that the image comprises the indication of license entitlement comprises locating the license identifier in the image. 19. The barcode-reading enhancement accessory of claim 17, wherein:
wherein the reflective surface is positioned so that when the barcode-reading enhancement accessory is secured to the mobile device, a first portion of the field of view of the camera assembly is folded towards a target area beyond a top side of the mobile device, and a second portion of the field of view of the camera assembly is obstructed by the barcode-reading enhancement accessory. 20. The barcode-reading enhancement accessory of claim 19, wherein a license identifier is printed on a portion of the barcode-reading enhancement accessory obstructing the field of view. 21. The barcode-reading enhancement accessory of claim 20, further comprising:
a focusing lens positioned between the camera assembly and a section of the barcode-reading enhancement accessory on which the license identifier is printed when the barcode-reading enhancement accessory is secured to the mobile device. 22. The barcode-reading enhancement accessory of claim 16, wherein the barcode-reading enhancement accessory projects the indication of license entitlement into the field of view such that the indication of license entitlement is present within the image captured by the camera assembly. 23. The barcode-reading enhancement accessory of claim 22, wherein the indication of license entitlement comprises a targeting pattern. 24. The barcode-reading enhancement accessory of claim 23, wherein:
the barcode-reading enhancement accessory comprises a target-generating structure that comprises at least one light source; and
the target-generating structure performs at least one of shaping or filtering the at least one light source to produce the targeting pattern. 25. The barcode-reading enhancement accessory of claim 23, wherein:
the barcode-reading enhancement accessory comprises a target-generating structure; and
the barcode-reading enhancement accessory performs at least one of shaping or filtering a light source of the mobile device to produce the targeting pattern. 26. The barcode-reading enhancement accessory of claim 23, wherein:
the barcode-reading enhancement accessory comprises an aperture that restricts illumination emitted by a light source to produce a shape of the targeting pattern; and
determining that the image comprises the indication of license entitlement comprises locating the shape of the targeting pattern within the image. 27. The barcode-reading enhancement accessory of claim 23, wherein:
the barcode-reading enhancement accessory comprises a filter that passes a limited spectrum of illumination emitted by a light source so that the targeting pattern, when incident on a surface, appears as a color of the limited spectrum passed by the filter; and
determining that the image comprises the indication of license entitlement comprises locating the color of the limited spectrum within the image. 28. The barcode-reading enhancement accessory of claim 22, wherein the indication of license entitlement is a sequential illumination pattern projected into the field of view by the barcode reading enhancement accessory, the sequential illumination pattern being generated by an illuminator being sequences on and off in a predetermined pattern. 29. A non-transitory computer readable medium storing instructions thereon that, when executed by at least one processor, causes a computing device to:
determine that an image captured by a camera assembly on the computing device comprises an indication of license entitlement, wherein determining that the image comprises the indication of license entitlement comprises detecting that a barcode-reading enhancement accessory secured to the computing device has affected the image captured by the camera assembly; and
in response to determining that the image captured by the camera assembly comprises the indication of license entitlement, activate an enhanced scanning mode of a barcode-reading application of the computing device. 29. A non-transitory computer readable medium storing instructions thereon that, when executed by at least one processor, causes a computing device to:
determine that an image captured by a camera assembly on the computing device comprises an indication of license entitlement, wherein determining that the image comprises the indication of license entitlement comprises detecting that a barcode-reading enhancement accessory secured to the computing device has affected the image captured by the camera assembly; and
in response to determining that the image captured by the camera assembly comprises the indication of license entitlement, activate an enhanced scanning mode of a barcode-reading application of the computing device. 30. The non-transitory computer readable medium of claim 29, wherein determining that the image comprises the indication of license entitlement comprises detecting that the barcode-reading enhancement accessory has obstructed a portion of a field of view of the camera assembly. 31. The non-transitory computer readable medium of claim 30, wherein:
the indication of license entitlement comprises a license identifier; and
determining that the image comprises the indication of license entitlement comprises locating the license identifier in the image. 32. The non-transitory computer readable medium of claim 29, wherein determining that the image comprises the indication of the license entitlement comprises detecting that the barcode-reading enhancement accessory has projected an indication of license entitlement into a field of view of the camera assembly. 33. The non-transitory computer readable medium of claim 32, wherein:
the indication of license entitlement comprises a targeting pattern; and
determining that the image comprises the indication of license entitlement comprises locating the targeting pattern in the image. 34. The non-transitory computer readable medium of claim 29, further comprising instructions thereon that, when executed by the at least one processor, cause the computing device to:
activate a base mode of operation of the barcode-reading application of the computing device prior to determining that the image captured by the camera assembly comprises the indication of license entitlement; and
wherein activating the enhanced scanning mode of the barcode-reading application of the computing device comprises activating one or more additional barcode-reading capabilities of the computing device not operable while operating in the base mode of operation of the barcode-reading application.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318807B2,US10318807B2,"Target searching apparatus, target searching method, and computer readable medium",2016-09-15,"method, closer, amount, region, second, divided, extractor, display, present, including, calculates, than, readable, feature, representative, inside, regions, medium, object, value, computer, each, plurality, into, image, searching, values, extracts, respectively, apparatus, calculator, small, includes, that, more, identification, having, target, outside, which, from","A target searching apparatus includes a display, an object region extractor, a feature amount calculator, a second feature amount calculator, and an object present region extractor. The object region extractor extracts, from a display image, an object region including an identification object. The feature amount calculator calculates in-region and out-region representative values of the image feature amount that respectively are representative values inside and outside the object region in the display image. The second feature amount calculator calculates a representative value of the image feature amount in each of a plurality of small regions into which the object region is divided. The object present region extractor extracts, from the plurality of small regions, one or more small region having the representative value that is closer to the in-region representative value than the out-region representative value, as an object present region in which the identification object is present.","1. A target searching apparatus comprising:
a controller; and
a memory storing instructions that, when executed by the controller, cause the controller to:
provide for display, on a display, a first image captured by a camera;
extract, from the first image displayed on the display, an object region including an identification object that is a possible detection target and is to be identified;
calculate an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
divide the object region into a plurality of small regions;
calculate a representative value of the image feature amount in each of the plurality of small regions;
extract, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjust an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
provide for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object. 1. A target searching apparatus comprising:
a controller; and
a memory storing instructions that, when executed by the controller, cause the controller to:
provide for display, on a display, a first image captured by a camera;
extract, from the first image displayed on the display, an object region including an identification object that is a possible detection target and is to be identified;
calculate an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
divide the object region into a plurality of small regions;
calculate a representative value of the image feature amount in each of the plurality of small regions;
extract, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjust an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
provide for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object. 2. The target searching apparatus according to claim 1, wherein the controller is further configured to:
calculate a zoom-in region, the zoom-in region being located around a centroid of the object present region and including the object present region at a predetermined area rate; and
adjust the angle of view of the camera by setting the zoom-in region as a zoom-in range. 3. The target searching apparatus according to claim 2, wherein the image feature amount is based on one or more of luminance of the first image, a contrast of the first image, and a color of the first image. 4. The target searching apparatus according to claim 2, wherein the controller detects a line-of-sight trajectory of an operator of the target searching apparatus on the first image displayed on the display, and extracts the object region from the first image on a basis of the detected line-of-sight trajectory. 5. The target searching apparatus according to claim 2, wherein the camera is mounted on an aircraft. 6. The target searching apparatus according to claim 5, wherein the display is mounted on the aircraft. 7. The target searching apparatus according to claim 5, wherein the display is provided separately from the aircraft. 8. The target searching apparatus according to claim 1, wherein the image feature amount is based on one or more of luminance of the first image, a contrast of the first image, and a color of the first image. 9. The target searching apparatus according to claim 1, wherein the controller detects a line-of-sight trajectory of an operator of the target searching apparatus on the first image displayed on the display, and extracts the object region from the first image on a basis of the detected line-of-sight trajectory. 10. The target searching apparatus according to claim 1, wherein the camera is mounted on an aircraft. 11. The target searching apparatus according to claim 10, wherein the display is mounted on the aircraft. 12. The target searching apparatus according to claim 10, wherein the display is provided separately from the aircraft. 13. A target searching method comprising:
providing for display, on a display, a first image captured by a camera;
extracting, from the displayed first image, an object region including an identification object that is a possible detection target and is to be identified;
calculating an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
dividing the object region into a plurality of small regions;
calculating a representative value of the image feature amount in each of the plurality of small regions;
extracting, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjusting an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
providing for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object. 13. A target searching method comprising:
providing for display, on a display, a first image captured by a camera;
extracting, from the displayed first image, an object region including an identification object that is a possible detection target and is to be identified;
calculating an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
dividing the object region into a plurality of small regions;
calculating a representative value of the image feature amount in each of the plurality of small regions;
extracting, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjusting an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
providing for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object. 14. A non-transitory computer readable medium having a target searching program that causes, when executed by a computer, the computer to perform a method, the method comprising:
providing for display, on a display, a first image captured by a camera;
extracting, from the displayed first image, an object region including an identification object that is a possible detection target and is to be identified;
calculating an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
dividing the object region into a plurality of small regions;
calculating a representative value of the image feature amount in each of plurality of small regions;
extracting, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjusting an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
providing for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object. 14. A non-transitory computer readable medium having a target searching program that causes, when executed by a computer, the computer to perform a method, the method comprising:
providing for display, on a display, a first image captured by a camera;
extracting, from the displayed first image, an object region including an identification object that is a possible detection target and is to be identified;
calculating an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
dividing the object region into a plurality of small regions;
calculating a representative value of the image feature amount in each of plurality of small regions;
extracting, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjusting an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
providing for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object. 15. A target searching apparatus comprising:
a display configured to display an image captured by a camera; and
circuitry configured to
extract, from a first image displayed on the display, an object region including an identification object that is a possible detection target and is to be identified;
calculate an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
divide the object region into a plurality of small regions;
calculate a representative value of the image feature amount in each of plurality of small regions into which the object region is divided;
extract, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjust an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
provide for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object. 15. A target searching apparatus comprising:
a display configured to display an image captured by a camera; and
circuitry configured to
extract, from a first image displayed on the display, an object region including an identification object that is a possible detection target and is to be identified;
calculate an in-region representative value of an image feature amount and an out-region representative value of the image feature amount in the displayed first image, the in-region representative value of the image feature amount being a representative value of the image feature amount inside the object region in the displayed first image, the out-region representative value of the image feature amount being a representative value of the image feature amount outside the object region in the displayed first image;
divide the object region into a plurality of small regions;
calculate a representative value of the image feature amount in each of plurality of small regions into which the object region is divided;
extract, from the plurality of small regions, one or more small regions as an object present region in which the identification object is present, wherein a difference between the representative value of the image feature amount for each of the extracted one or more small regions and the in-region representative value of the image feature amount is smaller than a difference between the representative value of the image feature amount for each of the extract one or more small regions and the out-region representative value of the image feature amount;
adjust an angle of view of the camera to capture an enlarged view of the identification object present in the extracted object present region; and
provide for display, on the display, a second image captured by the camera using the adjusted angle of view, the second image comprising the enlarged view of the identification object.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318808B2,US10318808B2,Some automated and semi-automated tools for linear feature extraction in two and three dimensions,2014-06-03,"raster, device, images, computes, computing, loads, extraction, features, feature, based, linear, stored, semi, tools, methods, engine, operating, network, that, dimensions, three, some, connected, automated, database, identifies, system, vector, from, comprising","A system for vector extraction comprising a vector extraction engine stored and operating on a network-connected computing device that loads raster images from a database stored and operating on a network-connected computing device, identifies features in the raster images, and computes a vector based on the features, and methods for feature and vector extraction.","1. A system for three-dimensional vector extraction from imagery of surface, comprising:
a vector extraction engine operating on a computing device or network of computing devices;
wherein the vector extraction engine:
builds a cost raster;
builds at least one digital surface model raster based at least in part on the cost raster;
combines the rasters to form a new three-dimensional cost raster;
identifies image features in at least one of the rasters;
computes instantaneous values for the identified features;
calculates a vector based at least in part on the instantaneous values; and
projects the vector onto the new three-dimensional raster. 1. A system for three-dimensional vector extraction from imagery of surface, comprising:
a vector extraction engine operating on a computing device or network of computing devices;
wherein the vector extraction engine:
builds a cost raster;
builds at least one digital surface model raster based at least in part on the cost raster;
combines the rasters to form a new three-dimensional cost raster;
identifies image features in at least one of the rasters;
computes instantaneous values for the identified features;
calculates a vector based at least in part on the instantaneous values; and
projects the vector onto the new three-dimensional raster. 2. A method for three-dimensional vector extraction from image data of surface, comprising the steps of:
building, using a vector extraction engine operating on a computing device or network of computing devices, a cost raster;
building at least one digital surface model raster based at least in part on the cost raster; and
combining the rasters to form a new three-dimensional cost raster;
identifying, using a vector extraction engine stored and operating on a workstation computer, image features in at least one of the rasters;
computing instantaneous values for the identified features;
calculating a vector based at least in part on the instantaneous values; and
projecting the vector onto the three dimensional raster. 2. A method for three-dimensional vector extraction from image data of surface, comprising the steps of:
building, using a vector extraction engine operating on a computing device or network of computing devices, a cost raster;
building at least one digital surface model raster based at least in part on the cost raster; and
combining the rasters to form a new three-dimensional cost raster;
identifying, using a vector extraction engine stored and operating on a workstation computer, image features in at least one of the rasters;
computing instantaneous values for the identified features;
calculating a vector based at least in part on the instantaneous values; and
projecting the vector onto the three dimensional raster.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318806B2,US10318806B2,Method and device for irradiating light for photographing iris,,"method, camera, device, light, configured, irradiating, distance, based, through, lens, have, index, photographing, image, refractive, source, adjustable, ambient, using, iris, includes, provided, capture, change, generate, control, levels, controller, capturer, controlling, motion, active, passing","A method and device for controlling an irradiating light for photographing an iris is provided. The device includes a light source configured to generate light, an active lens configured to have an adjustable refractive index to change light passing through the active lens, an image capturer configured to capture an image of the iris by using a camera, and a controller configured to control the active lens to change the light passing through the active lens based on distance, motion and ambient light levels.","1. A device for photographing an iris, the device comprising:
a light source configured to generate light;
an active lens configured to have an adjustable refractive index to change a light irradiated to the iris;
an image capturer configured to capture an image of the iris by using a camera and the changed light irradiated to the iris; and
at least one processor configured to control the active lens to change the light irradiated to the iris based on at least one detected photographing condition. 1. A device for photographing an iris, the device comprising:
a light source configured to generate light;
an active lens configured to have an adjustable refractive index to change a light irradiated to the iris;
an image capturer configured to capture an image of the iris by using a camera and the changed light irradiated to the iris; and
at least one processor configured to control the active lens to change the light irradiated to the iris based on at least one detected photographing condition. 2. The device of claim 1,
wherein the light source comprises at least one infrared emitting diode (IRED) for generating IR rays used to capture the image of the iris, and
wherein the at least one processor is further configured to perform iris recognition with respect to the captured image of the iris. 3. The device of claim 1, further comprising:
a distance sensor configured to determine a distance between the iris and the device as a detected photographing condition,
wherein the at least one processor is further configured to control the active lens based on the distance determined by the distance sensor. 4. The device of claim 3, further comprising:
a transceiver configured to communicate with an external device,
wherein the at least one processor is further configured to request through the transceiver that the external device generate light if the determined distance is greater than a threshold value. 5. The device of claim 3, wherein the at least one processor is further configured to control the active lens based on the determined distance to control an amount of light arriving at the iris. 6. The device of claim 1, further comprising:
a light amount sensor configured to sense an amount of light around the device as a detected photographing condition,
wherein the at least one processor is further configured to control the active lens based on the amount of the light sensed by the light amount sensor. 7. The device of claim 6, further comprising:
a transceiver configured to communicate with an external device,
wherein the at least one processor is further configured to request through the transceiver that the external device generate light if the sensed amount of light is less than a threshold value. 8. The device of claim 1, further comprising:
a parabolic lens configured to form a radiation plane of light generated by the light source. 9. The device of claim 1, further comprising:
a motion sensor configured to detect motion of the device relative to the iris as a detected photographing condition,
wherein the at least one processor is further configured to control the active lens based on the motion detected by the motion sensor. 10. A method of obtaining an image of an iris, the method comprising:
generating light by using a light source;
controlling an active lens configured to have an adjustable refractive index to change a light irradiated to the iris based on at least one detected photographing condition; and
capturing an image of the iris by using a camera and the changed light irradiated to the iris. 10. A method of obtaining an image of an iris, the method comprising:
generating light by using a light source;
controlling an active lens configured to have an adjustable refractive index to change a light irradiated to the iris based on at least one detected photographing condition; and
capturing an image of the iris by using a camera and the changed light irradiated to the iris. 11. The method of claim 10,
wherein the generation of light comprises generating infrared (IR) rays for capturing the image of the iris, and
wherein the method further comprises performing iris recognition based on the captured image of the iris. 12. The method of claim 10, wherein the controlling of the active lens comprises:
determining a distance between the iris and the device as a detected photographing condition; and
controlling the active lens based on the determined distance. 13. The method of claim 12, wherein the generation of light comprises requesting that an external device generate light if the determined distance is greater than a threshold value. 14. The method of claim 12, further comprising controlling the active lens based on the determined distance to control an amount of light arriving at the iris. 15. The method of claim 10, wherein the controlling of the active lens comprises:
determining an amount of light around the device as a detected photographing condition; and
controlling the active lens based on the determined amount of light. 16. The method of claim 15, wherein the generation of light comprises requesting that an external device generate light if the determined amount of the light is less than a threshold value. 17. The method of claim 10, further comprising:
detecting motion of the device relative to the iris as a detected photographing condition; and
controlling the active lens based on the detected motion. 18. A non-transitory recording medium having recorded thereon a program for controlling at least one processor to perform operations executing the method of claim 10.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318812B2,US10318812B2,Automatic digital image correlation and distribution,2016-06-21,"method, invention, digital, other, facial, receiving, present, recognition, distribution, based, depicting, people, systems, correlation, metadata, data, identity, image, analyzing, automatic, methods, person, includes, location, embodiments, determining, provide, least, contact, further","Embodiments of the present invention provide systems and methods for image correlation and distribution. The method includes receiving an image depicting at least one person, metadata for the image, contact data, facial recognition data, and location data. The method further includes analyzing the image and other data, and determining the identity of people in the image based on the facial recognition data and the location data.","1. A method for image correlation and distribution, the method comprising:
receiving, by one or more processors, an image depicting one or more persons and a plurality of associated data comprising: metadata for the image, a set of user contact data, a set of facial recognition data, and a set of geographic location data, wherein the image depicting one or more persons is received from a social media data feed;
analyzing, by one or more processors, the image and the plurality of associated data;
determining, by one or more processors, an identity of the one or more persons depicted in the image, wherein the identity is determined by the set of facial recognition data and elements of the set of user contact data; and
in response to the set of facial recognition data being unable to determine an identity of a person depicted in the image from among at least two candidate persons, using the set of geographic location data of the at least two candidate persons and the set of facial recognition data in conjunction with the elements of the set of user contact data to determine, by one or more processors, the identity of the person. 1. A method for image correlation and distribution, the method comprising:
receiving, by one or more processors, an image depicting one or more persons and a plurality of associated data comprising: metadata for the image, a set of user contact data, a set of facial recognition data, and a set of geographic location data, wherein the image depicting one or more persons is received from a social media data feed;
analyzing, by one or more processors, the image and the plurality of associated data;
determining, by one or more processors, an identity of the one or more persons depicted in the image, wherein the identity is determined by the set of facial recognition data and elements of the set of user contact data; and
in response to the set of facial recognition data being unable to determine an identity of a person depicted in the image from among at least two candidate persons, using the set of geographic location data of the at least two candidate persons and the set of facial recognition data in conjunction with the elements of the set of user contact data to determine, by one or more processors, the identity of the person. 2. The method of claim 1, further comprising:
automatically distributing, by one or more processors, the image to the one or more people in the image whose identity has been determined, by a preferred image transmission method, wherein the preferred image transmission method is based on the set of user contact data. 3. The method of claim 1, further comprising:
responsive to determining that at least one of the one or more people in the image is unidentified, generating, by one or more processors, an option to add new contact data for the one or more unidentified people; and
responsive to receiving new contact data for the one or more unidentified people, updating, by one or more processors, the set of user contact data with the added new contact data for the one or more unidentified people. 4. The method of claim 3, wherein the image is distributed to only the one or more people in the image: whose identity has been determined, and whose set of contact data has been received. 5. The method of claim 2, further comprising:
receiving, by one or more processors, one or more timing rules detailing predetermined scheduling parameters for the distribution of the image;
storing, by one or more processors, the received one or more timing rules; and
automatically distributing, by one or more processors, the image, based in part on the one or more timing rules. 6. The method of claim 1, wherein the set of user contact data comprises: names, addresses, e-mail addresses, genders, birthdays, phone numbers, instant messaging accounts, social media accounts, facial information used by facial recognition software to identify a person, and image transmission methods. 7. The method of claim 1, wherein the set of geographic location data comprises: GPS data, Wi-Fi data, Bluetooth data, social media data, mobile network data, and device sensor data. 8. A computer program product for image correlation and distribution, the computer program product comprising:
a computer readable tangible storage device, wherein the computer readable tangible storage device is hardware, and program instructions stored on the computer readable tangible storage device, the program instructions comprising:
program instructions to receive an image depicting one or more persons and a plurality of associated data comprising: metadata for the image, a set of user contact data, a set of facial recognition data, and a set of geographic location data, wherein the image depicting one or more persons is received from a social media data feed;
program instructions to analyze the image and the plurality of associated data;
program instructions to determine an identity of the one or more persons depicted in the image, wherein the identity is determined by the set of facial recognition data and elements of the set of user contact data; and
in response to the set of facial recognition data being unable to determine an identity of a person depicted in the image from among at least two candidate persons, program instructions to use the set of geographic location data of the at least two candidate persons and the set of facial recognition data in conjunction with the elements of the set of user contact data to determine the identity of the person. 8. A computer program product for image correlation and distribution, the computer program product comprising:
a computer readable tangible storage device, wherein the computer readable tangible storage device is hardware, and program instructions stored on the computer readable tangible storage device, the program instructions comprising:
program instructions to receive an image depicting one or more persons and a plurality of associated data comprising: metadata for the image, a set of user contact data, a set of facial recognition data, and a set of geographic location data, wherein the image depicting one or more persons is received from a social media data feed;
program instructions to analyze the image and the plurality of associated data;
program instructions to determine an identity of the one or more persons depicted in the image, wherein the identity is determined by the set of facial recognition data and elements of the set of user contact data; and
in response to the set of facial recognition data being unable to determine an identity of a person depicted in the image from among at least two candidate persons, program instructions to use the set of geographic location data of the at least two candidate persons and the set of facial recognition data in conjunction with the elements of the set of user contact data to determine the identity of the person. 9. The computer program product of claim 8, further comprising:
program instructions to automatically distribute the image to the one or more people in the image whose identity has been determined, by a preferred image transmission method, wherein the preferred image transmission method is based on the set of user contact data. 10. The computer program product of claim 8, further comprising:
responsive to determining that at least one of the one or more people in the image is unidentified, program instructions to generate an option to add new contact data for the one or more unidentified people; and
program instructions to, responsive to receiving new contact data for the one or more unidentified people, update the set of user contact data with the added new contact data for the one or more unidentified people. 11. The computer program product of claim 10, wherein the image is distributed to only the one or more people in the image: whose identity has been determined, and whose set of contact data has been received. 12. The computer program product of claim 9, further comprising:
program instructions to receive one or more timing rules detailing predetermined scheduling parameters for the distribution of the image;
program instructions to store the received one or more timing rules; and
program instructions to automatically distribute the image, based in part on the one or more timing rules. 13. The computer program product of claim 8, wherein the set of user contact data comprises: names, addresses, e-mail addresses, genders, birthdays, phone numbers, instant messaging accounts, social media accounts, facial information used by facial recognition software to identify a person, and image transmission methods. 14. The computer program product of claim 8, wherein the set of geographic location data comprises: GPS data, Wi-Fi data, Bluetooth data, social media data, mobile network data, and device sensor data. 15. A computer system for image correlation and distribution, the computer system comprising:
one or more computer processors;
one or more computer readable storage media;
program instructions stored on the one or more computer readable storage media for execution by at least one of the one or more processors, the program instructions comprising:
program instructions to receive an image depicting one or more persons and a plurality of associated data comprising: metadata for the image, a set of user contact data, a set of facial recognition data, and a set of geographic location data, wherein the image depicting one or more persons is received from a social media data feed;
program instructions to analyze the image and the plurality of associated data; and
program instructions to determine an identity of the one or more persons depicted in the image, wherein the identity is determined by the set of facial recognition data and elements of the set of user contact data; and
in response to the set of facial recognition data being unable to determine an identity of a person depicted in the image from among at least two candidate persons, program instructions to use the set of geographic location data of the at least two candidate persons and the set of facial recognition data in conjunction with the elements of the set of user contact data to determine the identity of the person. 15. A computer system for image correlation and distribution, the computer system comprising:
one or more computer processors;
one or more computer readable storage media;
program instructions stored on the one or more computer readable storage media for execution by at least one of the one or more processors, the program instructions comprising:
program instructions to receive an image depicting one or more persons and a plurality of associated data comprising: metadata for the image, a set of user contact data, a set of facial recognition data, and a set of geographic location data, wherein the image depicting one or more persons is received from a social media data feed;
program instructions to analyze the image and the plurality of associated data; and
program instructions to determine an identity of the one or more persons depicted in the image, wherein the identity is determined by the set of facial recognition data and elements of the set of user contact data; and
in response to the set of facial recognition data being unable to determine an identity of a person depicted in the image from among at least two candidate persons, program instructions to use the set of geographic location data of the at least two candidate persons and the set of facial recognition data in conjunction with the elements of the set of user contact data to determine the identity of the person. 16. The computer system of claim 15, further comprising:
program instructions to automatically distribute the image to the one or more people in the image whose identity has been determined, by a preferred image transmission method, wherein the preferred image transmission method is based on the set of user contact data. 17. The computer system of claim 15, further comprising:
responsive to determining that at least one of the one or more people in the image is unidentified, program instructions to generate an option to add new contact data for the one or more unidentified people; and
program instructions to, responsive to receiving new contact data for the one or more unidentified people, update the set of user contact data with the added new contact data for the one or more unidentified people. 18. The computer system of claim 17, wherein the image is distributed to only the one or more people in the image: whose identity has been determined, and whose set of contact data has been received. 19. The computer system of claim 16, further comprising:
program instructions to receive one or more timing rules detailing predetermined scheduling parameters for the distribution of the image;
program instructions to store the received one or more timing rules; and
program instructions to automatically distribute the image, based in part on the one or more timing rules. 20. The computer system of claim 15, wherein the set of geographic location data comprises: GPS data, Wi-Fi data, Bluetooth data, social media data, mobile network data, and device sensor data.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318810B2,US10318810B2,Systems and methods for determining statistics plant populations based on overhead optical measurements,2015-09-18,"method, plant, optical, hardware, disclosure, this, configured, machine, instructions, receive, statistics, convey, images, overhead, readable, based, corresponding, output, systems, remote, devices, crops, related, regions, grown, land, determine, where, sensing, image, area, methods, processors, include, platform, unit, clutter, provided, mounted, more, vegetation, segregate, information, determining, distinguish, count, signals, populations, describes, system, from, measurements, background",This disclosure describes a system and a method for determining statistics of plant populations based on overhead optical measurements. The system may include one or more hardware processors configured by machine-readable instructions to receive output signals provided by one or more remote sensing devices mounted to an overhead platform. The output signals may convey information related to one or more images of a land area where crops are grown. The one or more hardware processors may be configured by machine-readable instructions to distinguish vegetation from background clutter; segregate image regions corresponding to the vegetation from image regions corresponding to the background clutter; and determine a plant count per unit area.,"1. A system configured for determining statistics of plant populations based on overhead optical measurements, the system comprising:
one or more hardware processors configured by machine-readable instructions to:
receive output signals provided by one or more remote sensing devices mounted to an overhead platform, the overhead platform comprising an unmanned aerial vehicle, the output signals conveying information related to one or more images of a land area where crops are grown, the one or more images being spatially resolved, the output signals including one or more channels corresponding to one or more spectral ranges;
numerically combine the output signals such that a contrast between vegetation and background clutter is increased;
segregate image regions corresponding to the vegetation from image regions corresponding to the background clutter based on the one or more channels, the vegetation including one or both of a crop population and a non-crop population, the background clutter including one or more of soil, rock, standing water, man-made materials, or dead vegetation, wherein segregating image regions comprises:
classifying groups of pixels as belonging to a vegetation class or the background clutter based on an adjustable threshold of spectral reflectance combinations;
classifying groups of vegetation pixels as belonging to the crop population if the group of vegetation pixels are positioned statistically within crop rows; and
classifying groups of vegetation pixels as belonging to the non-crop population if the group of vegetation pixels are positioned statistically outside of the crop rows; and

determine a plant count per unit area. 1. A system configured for determining statistics of plant populations based on overhead optical measurements, the system comprising:
one or more hardware processors configured by machine-readable instructions to:
receive output signals provided by one or more remote sensing devices mounted to an overhead platform, the overhead platform comprising an unmanned aerial vehicle, the output signals conveying information related to one or more images of a land area where crops are grown, the one or more images being spatially resolved, the output signals including one or more channels corresponding to one or more spectral ranges;
numerically combine the output signals such that a contrast between vegetation and background clutter is increased;
segregate image regions corresponding to the vegetation from image regions corresponding to the background clutter based on the one or more channels, the vegetation including one or both of a crop population and a non-crop population, the background clutter including one or more of soil, rock, standing water, man-made materials, or dead vegetation, wherein segregating image regions comprises:
classifying groups of pixels as belonging to a vegetation class or the background clutter based on an adjustable threshold of spectral reflectance combinations;
classifying groups of vegetation pixels as belonging to the crop population if the group of vegetation pixels are positioned statistically within crop rows; and
classifying groups of vegetation pixels as belonging to the non-crop population if the group of vegetation pixels are positioned statistically outside of the crop rows; and

determine a plant count per unit area. 2. The system of claim 1, wherein the one or more hardware processors are further configured by machine-readable instructions to:
revise one or more intensity non-uniformities of the one or more images;
revise one or more spatial distortions of the one or more images;
revise one or more intensity values for variations in solar irradiance of the one or more images; or
register one or more pixels from one or more channels to a common pixel space. 3. The system of claim 1, wherein segregating image regions corresponding to the vegetation from image regions corresponding to the background clutter further comprises:
utilizing one or more differing spectral reflectance numerical combinations across one or more wavelength bands. 4. The system of claim 3, wherein the one or more hardware processors are further configured by machine-readable instructions to amplify one or more spatial frequency components corresponding to the numerical combination. 5. The system of claim 1, wherein the one or more hardware processors are further configured by machine readable instructions to, in the image regions corresponding to the vegetation, segregate image regions corresponding to the crop population from image regions corresponding to the non-crop population. 6. The system of claim 5, wherein segregating the crop population from the non-crop population further comprises:
determining a characteristic size of the crop population based on a statistical distribution of the vegetation size; and
segregating one or more contiguous groups of vegetation pixels having a size substantially greater than the characteristic size of the crop population. 7. The system of claim 5, wherein the one or more hardware processors are further configured by machine-readable instructions to determine one or more of an orientation, a spacing, or a location of one or more crop rows. 8. The system of claim 7, wherein the one or more hardware processors are further configured by machine-readable instructions to:
characterize a reference spectral signature of vegetation within the one or more crop rows; and/or determine a reference spectral signature from an external resource; and/or determine a reference spectral signature corresponding to a user selection of a region of interest; and
statistically compare a spectral signature of each pixel to the reference spectral signature; and
assign each pixel as belonging to same class or another class as the reference spectral signature. 9. The system of claim 7, wherein the one or more hardware processors are further configured to:
determine a spacing of one or more crop rows in pixels;
determine the pixel's Ground Sample Dimension using externally provided row spacing and the row spacing in pixels; and
determine an area of land portrayed by the one or more images using the number of pixels in the image and the pixel's Ground Sample Dimension. 10. The system of claim 9, further comprising determining the crop density and the non-crop density by determining a first count corresponding to the crop population and a second count corresponding to the non-crop population, wherein the crop density is determined by dividing the first count by the determined area of the land and wherein the non-crop density is determined by dividing the second count by the determined area of the land. 11. A method for determining statistics of plant populations based on overhead optical measurements, the method comprising:
receiving output signals provided by one or more remote sensing devices mounted to an overhead platform, the overhead platform comprising an unmanned aerial vehicle, the output signals conveying information related to one or more images of a land area where crops are grown, the one or more images being spatially resolved, the output signals including one or more channels corresponding to one or more spectral ranges;
numerically combine the output signals such that a contrast between the vegetation and the background clutter is increased;
segregating image regions corresponding to the vegetation from image regions corresponding to the background clutter based on the one or more channels, the vegetation including one or both of a crop population and a non-crop population, the background clutter including one or more of soil, rock, standing water, man-made materials, or dead vegetation, wherein segregating image regions comprises:
classifying groups of pixels as belonging to a vegetation class or the background clutter based on an adjustable threshold of spectral reflectance combinations;
classifying groups of vegetation pixels as belonging to the crop population if the group of vegetation pixels are positioned statistically within crop rows; and
classifying groups of vegetation pixels as belonging to the non-crop population if the group of vegetation pixels are positioned statistically outside of the crop rows; and

determining a plant count per unit area. 11. A method for determining statistics of plant populations based on overhead optical measurements, the method comprising:
receiving output signals provided by one or more remote sensing devices mounted to an overhead platform, the overhead platform comprising an unmanned aerial vehicle, the output signals conveying information related to one or more images of a land area where crops are grown, the one or more images being spatially resolved, the output signals including one or more channels corresponding to one or more spectral ranges;
numerically combine the output signals such that a contrast between the vegetation and the background clutter is increased;
segregating image regions corresponding to the vegetation from image regions corresponding to the background clutter based on the one or more channels, the vegetation including one or both of a crop population and a non-crop population, the background clutter including one or more of soil, rock, standing water, man-made materials, or dead vegetation, wherein segregating image regions comprises:
classifying groups of pixels as belonging to a vegetation class or the background clutter based on an adjustable threshold of spectral reflectance combinations;
classifying groups of vegetation pixels as belonging to the crop population if the group of vegetation pixels are positioned statistically within crop rows; and
classifying groups of vegetation pixels as belonging to the non-crop population if the group of vegetation pixels are positioned statistically outside of the crop rows; and

determining a plant count per unit area. 12. The method of claim 11, further comprising:
revising one or more intensity non-uniformities of the one or more images;
revising one or more spatial distortions of the one or more images;
revising one or more intensity values for variations in solar irradiance of the one or more images; or
registering one or more pixels from one or more channels to a common pixel space. 13. The method of claim 11, wherein segregating image regions corresponding to the vegetation from image regions corresponding to the background clutter further comprises:
utilizing one or more differing spectral reflectance numerical combinations across one or more wavelength bands. 14. The method of claim 13, further comprising amplifying one or more spatial frequency components corresponding to the numerical combination. 15. The method of claim 11, further comprising segregating image regions corresponding to the crop population from image regions corresponding to the non-crop population in the image regions corresponding to the vegetation. 16. The method of claim 15, wherein segregating the crop population from the non-crop population further comprises:
determining a characteristic size of the crop population based on a statistical distribution of the vegetation size; and
segregating one or more contiguous groups of vegetation pixels having a size substantially greater than the characteristic size of the crop population. 17. The method of claim 15, further comprising determining one or more of an orientation, a spacing, curvature, or a location of one or more crop rows. 18. The method of claim 17, further comprising:
characterizing a reference spectral signature of vegetation within the one or more crop rows; and/or determining a reference spectral signature from an external resource; and/or determining a reference spectral signature corresponding to a user selection of a region of interest; and
statistically comparing a spectral signature of each pixel to the reference spectral signature; and
assigning each pixel as belonging to same class or another class as the reference spectral signature. 19. The method of claim 17, further comprising:
determining a spacing of one or more crop rows in pixels;
determining the pixel's Ground Sample Dimension using externally provided row spacing and the row spacing in pixels; and
determining an area of land portrayed by the one or more images using the number of pixels in the image and the pixel's Ground Sample Dimension. 20. The method of claim 19, further comprising determining the crop density and the non-crop density by determining a first count corresponding to the crop population and a second count corresponding to the non-crop population, wherein the crop density is determined by dividing the first count by the determined area of the land and wherein the non-crop density is determined by dividing the second count by the determined area of the land. 21. A system configured for determining statistics of plant populations based on overhead optical measurements, the system comprising:
one or more hardware processors configured by machine-readable instructions to:
receive output signals provided by one or more remote sensing devices mounted to an overhead platform, the overhead platform comprising an unmanned aerial vehicle, the output signals conveying information related to one or more images of a land area where crops are grown, the one or more images being spatially resolved at a spatial frequency and that are resolved spectrally;
segregate image regions corresponding to the vegetation from image regions corresponding to the background clutter based on image data, the vegetation including one or both of a crop population and a non-crop population, the background clutter including one or more of soil, rock, standing water, man-made materials, or dead vegetation, wherein segregating image regions comprises:
classifying groups of pixels as belonging to a vegetation class or the background clutter based on an adjustable threshold of spectral reflectance combinations;
classifying groups of contiguous vegetation pixels as belonging to a first vegetation class if spatial and spectral characteristics of a given group are proximate to a statistical description of the first vegetation class; and
classifying the groups of contiguous vegetation pixels as belonging to another vegetation class if the spatial and spectral characteristics of a given group are not proximate to a statistical description of the first vegetation class; and

determine a number of groups of contiguous vegetation pixels belonging to the first vegetation class in the one or more images of the land area. 21. A system configured for determining statistics of plant populations based on overhead optical measurements, the system comprising:
one or more hardware processors configured by machine-readable instructions to:
receive output signals provided by one or more remote sensing devices mounted to an overhead platform, the overhead platform comprising an unmanned aerial vehicle, the output signals conveying information related to one or more images of a land area where crops are grown, the one or more images being spatially resolved at a spatial frequency and that are resolved spectrally;
segregate image regions corresponding to the vegetation from image regions corresponding to the background clutter based on image data, the vegetation including one or both of a crop population and a non-crop population, the background clutter including one or more of soil, rock, standing water, man-made materials, or dead vegetation, wherein segregating image regions comprises:
classifying groups of pixels as belonging to a vegetation class or the background clutter based on an adjustable threshold of spectral reflectance combinations;
classifying groups of contiguous vegetation pixels as belonging to a first vegetation class if spatial and spectral characteristics of a given group are proximate to a statistical description of the first vegetation class; and
classifying the groups of contiguous vegetation pixels as belonging to another vegetation class if the spatial and spectral characteristics of a given group are not proximate to a statistical description of the first vegetation class; and

determine a number of groups of contiguous vegetation pixels belonging to the first vegetation class in the one or more images of the land area.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318814B2,US10318814B2,Framework for combining content intelligence modules,2009-10-05,"method, parameters, module, concurrently, intelligence, digital, video, played, assets, framework, including, produce, cache, face, linked, feature, analyze, based, asset, metadata, data, providing, related, results, analyzer, files, combining, result, stored, with, unique, identifiers, histograms, accessed, such, analyzing, microprocessor, running, using, analysis, audio, includes, include, that, content, tracks, identification, media, temporally, generate, associated, immutable, file, which, access, algorithm, modules, sets, frames","A method for analyzing media assets such as video and audio files. The method includes providing access to all the frames of a digital media asset. The method includes, with a microprocessor, running a raw analyzer modules to analyze the asset frames to produce sets of raw analyzer result data that are stored in a data cache in a file associated with the asset. The sets of raw analyzer results are linked to the raw analyzer modules with unique identifiers. The digital media asset is played for the raw analyzer modules, which concurrently analyze the temporally-related frames. The raw analyzer results are stored as data tracks that include metadata for the asset such as immutable parameters including histograms. The method includes using a feature algorithm module to generate an analysis result, such as face identification, for the digital media asset based on the raw analyzer results accessed by the identifiers.","1. A computer-implemented method, comprising:
using a media processing device, accessing, from a data store, a plurality of portions of a digital media asset, wherein the plurality of portions are analyzed based on executing a media asset analysis plugin having a plurality of raw analyzer modules and a feature algorithm module, wherein the plurality of raw analyzer modules and the feature algorithm module are operable as a plug-in combination that supports shared common functionality in the plurality of raw analyzer modules and the feature algorithm module, wherein the plug-in combination comprises code for operating both the plurality of raw analyzer modules and the feature algorithm module in combination for content intelligence processes;
using a microprocessor of the media processing device, the microprocessor operating based on the plug-in combination, executing the plurality of raw analyzer modules to analyze the portions of the digital media asset to produce sets of raw analyzer results data, wherein the sets of raw analyzer results data are accessible by the feature algorithm module based in part on the code of the plug-in combination;
storing in a data cache the sets of raw analyzer result data in a media asset cache file associated with the digital media asset, wherein each of the sets of raw analyzer result data is linked to a particular one of the raw analyzer modules based on data tracks in the media asset cache file;
based on a call to the feature algorithm module, accessing the data cache storing the set of raw analyzer result data in the media asset cache file; and
upon accessing the data cache, executing, using the code of the plug-in combination, the feature algorithm module to generate an analysis result for the digital media asset, based on, at least one of the sets of raw analyzer result data in the data cache, wherein the at least one of the sets of raw analyzer result data is specific to the linked raw analyzer module that generated the raw analyzer result data. 1. A computer-implemented method, comprising:
using a media processing device, accessing, from a data store, a plurality of portions of a digital media asset, wherein the plurality of portions are analyzed based on executing a media asset analysis plugin having a plurality of raw analyzer modules and a feature algorithm module, wherein the plurality of raw analyzer modules and the feature algorithm module are operable as a plug-in combination that supports shared common functionality in the plurality of raw analyzer modules and the feature algorithm module, wherein the plug-in combination comprises code for operating both the plurality of raw analyzer modules and the feature algorithm module in combination for content intelligence processes;
using a microprocessor of the media processing device, the microprocessor operating based on the plug-in combination, executing the plurality of raw analyzer modules to analyze the portions of the digital media asset to produce sets of raw analyzer results data, wherein the sets of raw analyzer results data are accessible by the feature algorithm module based in part on the code of the plug-in combination;
storing in a data cache the sets of raw analyzer result data in a media asset cache file associated with the digital media asset, wherein each of the sets of raw analyzer result data is linked to a particular one of the raw analyzer modules based on data tracks in the media asset cache file;
based on a call to the feature algorithm module, accessing the data cache storing the set of raw analyzer result data in the media asset cache file; and
upon accessing the data cache, executing, using the code of the plug-in combination, the feature algorithm module to generate an analysis result for the digital media asset, based on, at least one of the sets of raw analyzer result data in the data cache, wherein the at least one of the sets of raw analyzer result data is specific to the linked raw analyzer module that generated the raw analyzer result data. 2. The method of claim 1, wherein the digital media asset comprises an audio or video file and the portions comprise temporally-related frames of the audio or video file that include timestamps. 3. The method of claim 2, wherein each of the sets of the raw analyzer result data comprises the data track providing metadata for the frames extracted by an associated one of the raw analyzer modules and associated with the frames of the digital media asset via the timestamps. 4. The method of claim 3, wherein the metadata comprises an immutable property of content in the audio or video file. 5. The method of claim 1, wherein during a raw analyzer phase, the plurality of raw analyzer modules of the plug-in combination operate to compute intermediate results data and during a feature algorithm phase, the feature algorithm module of the plug-in combination operates to run one or more algorithms on the intermediate results data based code of the plug-in combination. 6. The method of claim 1, wherein the at least one of the sets of raw analyzer result data is accessed by the feature algorithm module by providing a unique identifier associated with at least one of the raw analyzer modules that generated the at least one of the sets of raw analyzer result data. 7. The method of claim 1, wherein the plug-in combination includes a new version of code and an older version of code corresponding to a raw analyzer module from the plurality of raw analyzer modules or the feature algorithm module available simultaneously. 8. The method of claim 1, further comprising, with the microprocessor, running an additional feature algorithm module to generate an additional analysis result based on the analysis result of the feature algorithm module and based on at least one of the sets of raw analyzer result data in the data cache. 9. A non-transitory computer-readable storage medium with an executable program stored thereon, wherein the program instructs a microprocessor to perform the following:
playing, using the microprocessor of a media processing device, a media file including time-related frames;
during the playing, a media asset analysis plugin having a plurality of raw analyzer modules and at least two feature algorithm modules, extracts, using the plurality of raw analyzer modules, a plurality of sets of metadata, wherein the plurality of raw analyzer modules and the at least two feature algorithm modules are operable as a plug-in combination that supports shared common functionality in the plurality of raw analyzer modules and the at least two feature algorithm modules, wherein the plug-in combination comprises code for operating both the plurality of raw analyzer modules and the at least two feature algorithm module in combination for content intelligence processes;
storing the sets of metadata in a media asset cache file associated with the media file, wherein each of the sets of metadata is linked to a particular one of the raw analyzer modules based on data tracks in the media asset cache file;
based on a call to one of the at least two feature algorithm modules, accessing at least one of the sets of metadata stored in the media asset cache file;
based on the accessing, generating an analysis result for the media file using a first feature algorithm module to execute the code of the plug-in combination; and
generating a secondary analysis result for the media file, using a second feature algorithm module to execute the code of the plug-in combination, based on the analysis result of the first feature algorithm module and the at least one of the sets of metadata stored in the media asset cache file, wherein the at least one of the sets of metadata is specific to the linked raw analyzer module that generated the at least one of the sets of metadata. 9. A non-transitory computer-readable storage medium with an executable program stored thereon, wherein the program instructs a microprocessor to perform the following:
playing, using the microprocessor of a media processing device, a media file including time-related frames;
during the playing, a media asset analysis plugin having a plurality of raw analyzer modules and at least two feature algorithm modules, extracts, using the plurality of raw analyzer modules, a plurality of sets of metadata, wherein the plurality of raw analyzer modules and the at least two feature algorithm modules are operable as a plug-in combination that supports shared common functionality in the plurality of raw analyzer modules and the at least two feature algorithm modules, wherein the plug-in combination comprises code for operating both the plurality of raw analyzer modules and the at least two feature algorithm module in combination for content intelligence processes;
storing the sets of metadata in a media asset cache file associated with the media file, wherein each of the sets of metadata is linked to a particular one of the raw analyzer modules based on data tracks in the media asset cache file;
based on a call to one of the at least two feature algorithm modules, accessing at least one of the sets of metadata stored in the media asset cache file;
based on the accessing, generating an analysis result for the media file using a first feature algorithm module to execute the code of the plug-in combination; and
generating a secondary analysis result for the media file, using a second feature algorithm module to execute the code of the plug-in combination, based on the analysis result of the first feature algorithm module and the at least one of the sets of metadata stored in the media asset cache file, wherein the at least one of the sets of metadata is specific to the linked raw analyzer module that generated the at least one of the sets of metadata. 10. The computer readable medium of claim 9, wherein at least a portion of the sets of the metadata comprise substantially immutable properties determined for the time-related frames. 11. The computer readable medium of claim 10, wherein the immutable properties comprise histograms for the time-related frames. 12. The computer readable medium of claim 9, wherein the media asset cache file comprises the data tracks associated with a plurality of raw analyzer modules via a unique identifier and wherein the accessing comprises providing one of the unique identifiers and one or more time stamps associated with the time-related frames. 13. The computer readable medium of claim 12, wherein at least a subset of the plurality raw analyzer modules operate concurrently to generate the sets of the metadata for the time-related frames. 14. The computer readable medium of claim 9, wherein the secondary analysis result comprises identification of an object in one or more of the time-related frames based on processing of at least one of the sets of the metadata. 15. A computer system comprising:
a processor;
a media import module run by the processor to provide access to a digital media asset;
a data cache storing a media asset cache file for the digital media asset, wherein the data cache is accessible based on calls to feature algorithm modules associated with the media asset cache file stored in the data cache; and
a plurality of media asset analysis plugins generating analysis results, each of the media asset analysis plugins comprising:
a raw analyzer module run by the processor to create a data track associated with portions of the digital media asset and to store data determined based on an analysis of the portions in the media asset cache file; and
a feature algorithm module for accessing the data track to generate a feature analysis result based on the data determined by one or more raw analyzer modules, wherein the data is specific to the raw analyzer module that generated the data, wherein the raw analyzer module and the feature algorithm module are operable as a plug-in combination that supports shared common functionality in the raw analyzer module and the feature algorithm module, wherein the plug-in combination comprises code for operating both the raw analyzer module and the feature algorithm module in combination for content intelligence processes. 15. A computer system comprising:
a processor;
a media import module run by the processor to provide access to a digital media asset;
a data cache storing a media asset cache file for the digital media asset, wherein the data cache is accessible based on calls to feature algorithm modules associated with the media asset cache file stored in the data cache; and
a plurality of media asset analysis plugins generating analysis results, each of the media asset analysis plugins comprising:
a raw analyzer module run by the processor to create a data track associated with portions of the digital media asset and to store data determined based on an analysis of the portions in the media asset cache file; and
a feature algorithm module for accessing the data track to generate a feature analysis result based on the data determined by one or more raw analyzer modules, wherein the data is specific to the raw analyzer module that generated the data, wherein the raw analyzer module and the feature algorithm module are operable as a plug-in combination that supports shared common functionality in the raw analyzer module and the feature algorithm module, wherein the plug-in combination comprises code for operating both the raw analyzer module and the feature algorithm module in combination for content intelligence processes. 16. The system of claim 15, wherein the digital media asset comprises an audio or video file with time-related frames and wherein the data determined by the raw analyzer module includes properties extracted from the frames and associated with the frames via timestamp data from the audio or video file. 17. The system of claim 16, wherein the data track is associated with the raw analyzer module by a unique identifier and wherein the feature algorithm module accesses the data in the data track by providing a portion of the timestamp data and the unique identifier. 18. The system of claim 17, wherein the data in the data track comprises a substantially immutable property of content of one or more of the frames. 19. The system of claim 15, wherein the media import module plays an entire length of the digital media asset to provide concurrent access to the digital media asset to a plurality of the raw analyzer modules including the raw analyzer module and the one or more raw analyzer modules. 20. The system of claim 18, further comprising a client application run by the processor receiving the feature analysis result as input and wherein the feature algorithm module accesses at least two data tracks associated with the one or more raw analyzer modules provided in a different plug-in combination.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318815B2,US10318815B2,Systems and methods for selecting previews for presentation during media navigation,2015-12-28,"during, video, viewer, confidence, previews, produce, readable, query, preview, based, systems, identify, score, computer, sequential, with, each, navigation, image, respective, frame, methods, interest, selecting, content, selected, provided, some, more, information, transitory, media, least, presentation, associated, search, ranked, items","Systems, methods, and non-transitory computer-readable media can identify a set of media content items based on at least one search query. The set of media content items can be ranked based on information associated with one or more media content items in the set of media content items to produce a ranked set of media content items. The ranked set of media content items can be provided for sequential video presentation. A set of image frame previews can be selected for at least some media content items in the ranked set. Each image frame preview in the set of image frame previews can be selected based on a respective confidence score associated with viewer interest. One or more image frame previews, out of the set of image frame previews, can be provided during media navigation associated with the sequential video presentation.","1. A computer-implemented method comprising:
identifying, by a computing system, a set of media content items based on at least one search query;
ranking, by the computing system, the set of media content items based on information associated with one or more media content items in the set of media content items to produce a ranked set of media content items, wherein the ranked set of media content items includes a plurality of media content items;
providing, by the computing system, the ranked set of media content items for sequential video presentation;
selecting, by the computing system, a set of image frame previews for at least some media content items in the ranked set, each image frame preview in the set of image frame previews being selected based on a respective confidence score associated with viewer interest of a particular viewing user; and
providing, by the computing system, one or more image frame previews, out of the set of image frame previews, during media navigation associated with the sequential video presentation. 1. A computer-implemented method comprising:
identifying, by a computing system, a set of media content items based on at least one search query;
ranking, by the computing system, the set of media content items based on information associated with one or more media content items in the set of media content items to produce a ranked set of media content items, wherein the ranked set of media content items includes a plurality of media content items;
providing, by the computing system, the ranked set of media content items for sequential video presentation;
selecting, by the computing system, a set of image frame previews for at least some media content items in the ranked set, each image frame preview in the set of image frame previews being selected based on a respective confidence score associated with viewer interest of a particular viewing user; and
providing, by the computing system, one or more image frame previews, out of the set of image frame previews, during media navigation associated with the sequential video presentation. 2. The computer-implemented method of claim 1, wherein selecting the set of image frame previews for the at least some media content items in the ranked set further comprises:
determining that a first media content item in the set of media content items is ranked higher than at least a second media content item in the set of media content items;
selecting a first quantity of image frame previews associated with the first media content item to be included in the set of image frame previews; and
selecting a second quantity of image frame previews associated with the second media content item to be included in the set of image frame previews, wherein the first quantity is selected to be larger than the second quantity. 3. The computer-implemented method of claim 1, wherein selecting the set of image frame previews for the at least some media content items in the ranked set utilizes at least one of an object detection process, an object recognition process, a facial detection process, a facial recognition process, an action detection process, or an action recognition process. 4. The computer-implemented method of claim 1, further comprising:
detecting, during media navigation, a command to access a particular media content item represented by a particular image frame preview out of the one or more image frame previews, the particular media content item being included in the ranked set of media content items;
determining, based on one or more social signals, a particular playback time associated with the particular image frame preview and with the particular media content item; and
enabling the sequential video presentation to continue at the particular playback time. 5. The computer-implemented method of claim 1, further comprising:
detecting, during media navigation, an interaction with respect to a particular image frame preview out of the one or more image frame previews, the interaction being detected for at least a specified threshold time duration; and
providing one or more additional image frame previews associated with the particular image frame preview. 6. The computer-implemented method of claim 1, wherein the information associated with the one or more media content items in the set of media content items includes at least one of a verified entity status signal, a social affinity signal with respect to a viewing user, a social engagement signal, or a time signal. 7. The computer-implemented method of claim 1, further comprising:
providing, prior to the sequential video presentation, access to each media content item in the ranked set of media content items via a respective interface portion within a grid interface, wherein the ranked set of media content items is provided for the sequential video presentation when a command with respect to at least one media content item in the ranked set is detected. 8. The computer-implemented method of claim 7, wherein the media navigation associated with the sequential video presentation enables browsing through the at least some media content items in the ranked set in a sequential order. 9. The computer-implemented method of claim 7, wherein each media content item in the ranked set includes a respective real-time video, wherein at least some media content items in the ranked set are provided from different perspectives, and wherein the media navigation associated with the sequential video presentation enables browsing through at least some of the different perspectives. 10. The computer-implemented method of claim 1, wherein the at least one search query includes at least one of a hashtag, an event, a location, a topic, or an entity. 11. A system comprising:
at least one processor; and
a memory storing instructions that, when executed by the at least one processor, cause the system to perform:
identifying a set of media content items based on at least one search query;
ranking the set of media content items based on information associated with one or more media content items in the set of media content items to produce a ranked set of media content items, wherein the ranked set of media content items includes a plurality of media content items;
providing the ranked set of media content items for sequential video presentation;
selecting a set of image frame previews for at least some media content items in the ranked set, each image frame preview in the set of image frame previews being selected based on a respective confidence score associated with viewer interest of a particular viewing user; and
providing one or more image frame previews, out of the set of image frame previews, during media navigation associated with the sequential video presentation. 11. A system comprising:
at least one processor; and
a memory storing instructions that, when executed by the at least one processor, cause the system to perform:
identifying a set of media content items based on at least one search query;
ranking the set of media content items based on information associated with one or more media content items in the set of media content items to produce a ranked set of media content items, wherein the ranked set of media content items includes a plurality of media content items;
providing the ranked set of media content items for sequential video presentation;
selecting a set of image frame previews for at least some media content items in the ranked set, each image frame preview in the set of image frame previews being selected based on a respective confidence score associated with viewer interest of a particular viewing user; and
providing one or more image frame previews, out of the set of image frame previews, during media navigation associated with the sequential video presentation. 12. The system of claim 11, wherein selecting the set of image frame previews for the at least some media content items in the ranked set further comprises:
determining that a first media content item in the set of media content items is ranked higher than at least a second media content item in the set of media content items;
selecting a first quantity of image frame previews associated with the first media content item to be included in the set of image frame previews; and
selecting a second quantity of image frame previews associated with the second media content item to be included in the set of image frame previews, wherein the first quantity is selected to be larger than the second quantity. 13. The system of claim 11, wherein selecting the set of image frame previews for the at least some media content items in the ranked set utilizes at least one of an object detection process, an object recognition process, a facial detection process, a facial recognition process, an action detection process, or an action recognition process. 14. The system of claim 11, wherein the instructions cause the system to further perform:
detecting, during media navigation, a command to access a particular media content item represented by a particular image frame preview out of the one or more image frame previews, the particular media content item being included in the ranked set of media content items;
determining, based on one or more social signals, a particular playback time associated with the particular image frame preview and with the particular media content item; and
enabling the sequential video presentation to continue at the particular playback time. 15. The system of claim 11, wherein the instructions cause the system to further perform:
detecting, during media navigation, an interaction with respect to a particular image frame preview out of the one or more image frame previews, the interaction being detected for at least a specified threshold time duration; and
providing one or more additional image frame previews associated with the particular image frame preview. 16. A non-transitory computer-readable storage medium including instructions that, when executed by at least one processor of a computing system, cause the computing system to perform a method comprising:
identifying a set of media content items based on at least one search query;
ranking the set of media content items based on information associated with one or more media content items in the set of media content items to produce a ranked set of media content items, wherein the ranked set of media content items includes a plurality of media content items;
providing the ranked set of media content items for sequential video presentation;
selecting a set of image frame previews for at least some media content items in the ranked set, each image frame preview in the set of image frame previews being selected based on a respective confidence score associated with viewer interest of a particular viewing user; and
providing one or more image frame previews, out of the set of image frame previews, during media navigation associated with the sequential video presentation. 16. A non-transitory computer-readable storage medium including instructions that, when executed by at least one processor of a computing system, cause the computing system to perform a method comprising:
identifying a set of media content items based on at least one search query;
ranking the set of media content items based on information associated with one or more media content items in the set of media content items to produce a ranked set of media content items, wherein the ranked set of media content items includes a plurality of media content items;
providing the ranked set of media content items for sequential video presentation;
selecting a set of image frame previews for at least some media content items in the ranked set, each image frame preview in the set of image frame previews being selected based on a respective confidence score associated with viewer interest of a particular viewing user; and
providing one or more image frame previews, out of the set of image frame previews, during media navigation associated with the sequential video presentation. 17. The non-transitory computer-readable storage medium of claim 16, wherein selecting the set of image frame previews for the at least some media content items in the ranked set further comprises:
determining that a first media content item in the set of media content items is ranked higher than at least a second media content item in the set of media content items;
selecting a first quantity of image frame previews associated with the first media content item to be included in the set of image frame previews; and
selecting a second quantity of image frame previews associated with the second media content item to be included in the set of image frame previews, wherein the first quantity is selected to be larger than the second quantity. 18. The non-transitory computer-readable storage medium of claim 16, wherein selecting the set of image frame previews for the at least some media content items in the ranked set utilizes at least one of an object detection process, an object recognition process, a facial detection process, a facial recognition process, an action detection process, or an action recognition process. 19. The non-transitory computer-readable storage medium of claim 16, wherein the instructions cause the system to further perform:
detecting, during media navigation, a command to access a particular media content item represented by a particular image frame preview out of the one or more image frame previews, the particular media content item being included in the ranked set of media content items;
determining, based on one or more social signals, a particular playback time associated with the particular image frame preview and with the particular media content item; and
enabling the sequential video presentation to continue at the particular playback time. 20. The non-transitory computer-readable storage medium of claim 16, wherein the instructions cause the system to further perform:
detecting, during media navigation, an interaction with respect to a particular image frame preview out of the one or more image frame previews, the interaction being detected for at least a specified threshold time duration; and
providing one or more additional image frame previews associated with the particular image frame preview.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318817B2,US10318817B2,Method and apparatus for surveillance,2014-07-08,"method, persons, including, within, distance, included, data, providing, represent, identify, object, steps, scene, head, surveillance, obtaining, image, lateral, predetermined, apparatus, there, that, processing, objects, information, depth, discount, comprising, potential","There is included a method of surveillance comprising the steps of obtaining image data of a scene, the image data including depth data providing depth information, processing the depth data to identify potential head object(s), that may represent head(s) of persons in the scene, and processing the image data to discount objects within a predetermined lateral distance of potential head objects.","1. A method of surveillance comprising the steps of:
obtaining image data of a scene, the image data including depth data providing depth information, the depth data being obtained by a depth sensor,
processing the depth data to identify potential head object(s), that may represent head(s) of persons in the scene, the depth data being processed to determine regions of local minima in the scene, to extract contours of the detected regions of local minima and to determine large enough contours as potential head objects, and
processing the image data to discount potential head objects within a predetermined lateral distance of potential head objects. 1. A method of surveillance comprising the steps of:
obtaining image data of a scene, the image data including depth data providing depth information, the depth data being obtained by a depth sensor,
processing the depth data to identify potential head object(s), that may represent head(s) of persons in the scene, the depth data being processed to determine regions of local minima in the scene, to extract contours of the detected regions of local minima and to determine large enough contours as potential head objects, and
processing the image data to discount potential head objects within a predetermined lateral distance of potential head objects. 2. The method in accordance with claim 1, further comprising the step of identifying the remaining potential head object(s) as head objects representing the heads of persons in the scene. 3. The method in accordance with or claim 2, wherein the image data comprises a video feed, and the method comprises the further step of processing the image data to track motion of one or more identified head objects. 4. The method in accordance with claim 3, further comprising the step of analysing the motion of a head object and determining the behaviour of a person represented by the head object. 5. An apparatus for surveillance, comprising:
an image sensor arranged to obtain image data of a scene, the image sensor comprising a depth sensor arranged to obtain depth data providing depth information, and
a processor arranged to process the depth data to identify potential head objects that may represent heads of persons in the scene, the processor being arranged to determine regions of local minima in the scene, to extract contours of the detected regions of local minima and to determine large enough contours as potential head objects, the processor being further arranged to process the image data to discount potential head objects within a predetermined distance of potential head objects. 5. An apparatus for surveillance, comprising:
an image sensor arranged to obtain image data of a scene, the image sensor comprising a depth sensor arranged to obtain depth data providing depth information, and
a processor arranged to process the depth data to identify potential head objects that may represent heads of persons in the scene, the processor being arranged to determine regions of local minima in the scene, to extract contours of the detected regions of local minima and to determine large enough contours as potential head objects, the processor being further arranged to process the image data to discount potential head objects within a predetermined distance of potential head objects. 6. The apparatus in accordance with claim 5, wherein the image sensor is one of a 3-dimensional camera and a combination of a 2D camera and depth sensor. 7. The apparatus in accordance with claim 6, wherein the processor is arranged to provide a count of the head objects, to provide a count of the persons in the scene. 8. The apparatus in accordance with claim 7, wherein the image sensor is a video camera and the image data comprises a video feed, the processor being arranged to track motion of head objects in order to track motion of persons within the scene. 9. The apparatus in accordance with claim 8, wherein the processor is arranged to determine behaviour of the persons being tracked, on the basis of their motion. 10. A non-transitory computer-readable medium having computer executable instructions for performing a method comprising:
obtaining image data of a scene, the image data including depth data providing depth information, the depth data being obtained by a depth sensor,
processing the depth data to identify potential head object(s), that may represent head(s) of persons in the scene, the processing of the depth data comprising determining regions of local minima in the scene, extracting contours of the detected regions of the local minima and determining large enough contours as potential head objects, and
processing the image data to discount potential head objects within a predetermined lateral distance of potential head objects. 10. A non-transitory computer-readable medium having computer executable instructions for performing a method comprising:
obtaining image data of a scene, the image data including depth data providing depth information, the depth data being obtained by a depth sensor,
processing the depth data to identify potential head object(s), that may represent head(s) of persons in the scene, the processing of the depth data comprising determining regions of local minima in the scene, extracting contours of the detected regions of the local minima and determining large enough contours as potential head objects, and
processing the image data to discount potential head objects within a predetermined lateral distance of potential head objects. 11. The method in accordance with claim 4, wherein the step of analysing the motion comprises analysing a change in the depth data relating to the head object. 12. The method in accordance with claim 1, wherein the image data is obtained by one of a 3D camera and a combination of a 2D camera and a depth sensor. 13. The apparatus in accordance with claim 9, wherein the processor is arranged to track a change in depth data of head objects and determine behaviour based on the change in depth data. 14. The method in accordance with claim 4, wherein the step of processing the depth data comprises processing one or both of the image data and depth data to track rate of change of motion of a head object, and analysing the rate of change of motion to determine the behaviour of a person represented by the head object. 15. The apparatus in accordance with claim 8, wherein the processor is arranged to process one or both of the image data and depth data to track rate of change of motion of a head object, and to analyse the rate of change of motion to determine the behaviour of a person represented by the head object. 16. The method in accordance with claim 4, wherein the step of analysing the motion of a head object to determine behaviour of a person is arranged to track flow direction of head objects in the scene. 17. The apparatus in accordance with claim 8, wherein the processor is arranged to track flow direction of persons in the scene.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318816B2,US10318816B2,"Method, system and apparatus for segmenting an image set to generate a plurality of event clusters",2015-06-26,"method, boundaries, pair, images, determined, intervals, based, corresponding, metadata, calendar, adjustment, sequence, interval, value, each, plurality, with, pairs, accessed, image, predetermined, using, apparatus, clusters, includes, between, more, event, neighboring, ordered, information, segmentation, segmenting, having, generate, least, associated, system, generating, cluster, from, adjacent, segmented, time","Generating a plurality of event clusters by segmenting an image set. Each event cluster includes at least one image from the image set. The image set, having time metadata corresponding to each image in the set, is accessed. A time interval between each pair of neighboring images in the image set is determined. The neighboring images in each of the pairs are adjacent in a time ordered sequence. The time interval for each of the pairs is determined from time metadata associated with each image in the pair. An adjustment value is determined for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information. The images in the image set are segmented to generate the plurality of event clusters. Segmentation boundaries for each event cluster are determined using at least the determined adjustment value.","1. A computer implemented method of segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said method comprising:
accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
determining, using the one or more computer processors, an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 1. A computer implemented method of segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said method comprising:
accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
determining, using the one or more computer processors, an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 2. The method according to claim 1, wherein segmentation boundaries for each event cluster are determined using the determined time interval reduced by the adjustment value. 3. The method according to claim 1, wherein segmentation boundaries for each event cluster are determined using the determined time interval increased by the adjustment value. 4. The method according to claim 1, wherein the predetermined date and time of day period is fixed. 5. The method according to claim 2, wherein the predetermined date and time of day period is fixed, and wherein the determined time interval is further reduced when an overlap period falls on a weekend. 6. The method according to claim 2, wherein the predetermined date and time of day period is fixed, and wherein the determined time interval is further reduced when an overlap period falls on one or more of: a weekend; holiday season; multi-day festivals. 7. The method according to claim 2, wherein the predetermined date and time of day period is fixed, and wherein the determined time interval is further reduced when an overlap period corresponds with one or more of a period around the wedding anniversary of the photographer; a period where the photographer is expected to take a vacation. 8. The method according to claim 1, wherein the image set is ordered based on the time metadata representing image capture. 9. The method according to claim 1, wherein the time interval is an initial time interval. 10. The method according to claim 1, further comprising calculating an adjusted time interval from the adjustment time value of a determined time interval; and
assigning an image to an event cluster based on a comparison of the adjusted time interval with an event time interval associated with the event cluster. 11. An apparatus for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said apparatus comprising:
one or more computer processors; and
one or more computer readable memories for storing instructions which, when executed, cause the one or more computer processors to perform operations of:
accessing the image set having time metadata corresponding to each image in the set;
determining a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
determining an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
segmenting the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 11. An apparatus for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said apparatus comprising:
one or more computer processors; and
one or more computer readable memories for storing instructions which, when executed, cause the one or more computer processors to perform operations of:
accessing the image set having time metadata corresponding to each image in the set;
determining a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
determining an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
segmenting the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 12. A system for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said system comprising:
a memory for storing data and a computer program;
one or more computer processors coupled to the memory for executing the computer program, said computer program comprising instructions for:
accessing, using the one or more computer processors, the image set having time metadata corresponding to each image in the set;
determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
determining, using the one or more computer processors, an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 12. A system for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said system comprising:
a memory for storing data and a computer program;
one or more computer processors coupled to the memory for executing the computer program, said computer program comprising instructions for:
accessing, using the one or more computer processors, the image set having time metadata corresponding to each image in the set;
determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
determining, using the one or more computer processors, an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 13. A non-transitory computer readable medium having a program stored thereon for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said program comprising:
computer readable code for accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
computer readable code for determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
computer readable code for determining, using the one or more computer processors, an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
computer readable code for segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 13. A non-transitory computer readable medium having a program stored thereon for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said program comprising:
computer readable code for accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
computer readable code for determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
computer readable code for determining, using the one or more computer processors, an adjustment value for one or more of the determined time intervals based on at least one of predetermined time of day information and calendar information, wherein the adjustment value is determined based on at least one of the group consisting of:
overlap of the time interval and one or more predetermined date and time of day periods; and
non-overlap of the time interval and one or more predetermined date and time of day periods; and
computer readable code for segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the determined adjustment value. 14. A computer implemented method of segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said method comprising:
accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
adjusting, using the one or more computer processors, each of the determined time intervals at least in case where the time interval overlaps one or more predetermined date and time of day periods:
segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the adjusted time intervals. 14. A computer implemented method of segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said method comprising:
accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
adjusting, using the one or more computer processors, each of the determined time intervals at least in case where the time interval overlaps one or more predetermined date and time of day periods:
segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the adjusted time intervals. 15. The method according to claim 14, wherein one of the predetermined date and time of day periods is set as a time period within which the photographer was sleeping, and in the adjusting, each of the determined time intervals which overlaps the period is reduced. 16. An apparatus for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said apparatus comprising:
one or more computer processors; and
one or more computer readable memories for storing instructions which, when executed, cause the one or more computer processors to perform operations of:
accessing the image set having time metadata corresponding to each image in the set;
determining a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
adjusting each of the determined time intervals at least in case where the time interval overlaps one or more predetermined date and time of day periods:
segmenting the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the adjusted time intervals. 16. An apparatus for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said apparatus comprising:
one or more computer processors; and
one or more computer readable memories for storing instructions which, when executed, cause the one or more computer processors to perform operations of:
accessing the image set having time metadata corresponding to each image in the set;
determining a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
adjusting each of the determined time intervals at least in case where the time interval overlaps one or more predetermined date and time of day periods:
segmenting the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the adjusted time intervals. 17. A non-transitory computer readable medium having a program stored thereon for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said program comprising:
computer readable code for accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
computer readable code for determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
computer readable code for adjusting, using the one or more computer processors, each of the determined time intervals at least in case where the time interval overlaps one or more predetermined date and time of day periods:
computer readable code for segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the adjusted time intervals. 17. A non-transitory computer readable medium having a program stored thereon for segmenting an image set to generate a plurality of event clusters, each event cluster including at least one image from the image set, said program comprising:
computer readable code for accessing, using one or more computer processors, the image set having time metadata corresponding to each image in the set;
computer readable code for determining, using the one or more computer processors, a time interval between each pair of neighbouring images in the image set, the neighbouring images in each of said pairs being adjacent in a time ordered sequence, said time interval for each of said pairs being determined from time metadata associated with each image in the pair;
computer readable code for adjusting, using the one or more computer processors, each of the determined time intervals at least in case where the time interval overlaps one or more predetermined date and time of day periods:
computer readable code for segmenting, using the one or more computer processors, the images in the image set to generate the plurality of event clusters, wherein segmentation boundaries for each event cluster are determined using at least the adjusted time intervals.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318809B2,US10318809B2,Unmanned aircraft structure evaluation system and method,2014-01-10,"method, laterally, camera, sides, vertical, multiple, structure, orientation, device, configured, display, receive, upon, dependent, aircraft, including, around, offset, onto, readable, input, computerized, path, direct, medium, evaluation, computer, image, lateral, obtain, analysis, outline, processors, characteristics, that, unit, being, mounted, executing, more, cause, information, transitory, identification, having, software, height, flight, store, generate, unmanned, relative, system, from, vertically, comprising","A computerized system, comprising: a computer system having an input unit, a display unit, one or more processors and one or more non-transitory computer readable medium, the one or more processors executing image display and analysis software to cause the one or more processors to: receive an identification of a structure from the input device, the structure having multiple sides, an outline, and a height; obtain characteristics of a camera mounted onto an unmanned aircraft; generate unmanned aircraft information including: flight path information configured to direct the unmanned aircraft to fly a flight path around the structure that is laterally and vertically offset from the structure, the lateral and vertical offset being dependent upon the height of the structure, an orientation of the camera relative to the unmanned aircraft, and the characteristics of the camera; and, store the unmanned aircraft information on the one or more non-transitory computer readable medium.","1. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors, and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
receive a first geographic location of a structure of interest, the first geographic location having one or more coordinates of the structure of interest;
display, on the display unit, one or more images depicting an aerial view of the structure of interest;
receive, via the input unit, an alteration of the one or more coordinates of the structure of interest indicating a second geographic location of the structure of interest, the second geographic location being different from the first geographic location, and being a validated location of the structure of interest;
subsequent to receiving the alteration of the one or more coordinates of the structure of interest, generate unmanned aircraft information based on the one or more coordinates of the second geographic location, the unmanned aircraft information including flight path information configured to direct an unmanned aircraft to fly an autonomous flight path above the structure of interest, and camera control information to direct a camera of the unmanned aircraft to capture images of the structure of interest at predetermined target capture points on the structure of interest while the unmanned aircraft is flying the flight path;
receive the images of the structure of interest captured by the camera while the unmanned aircraft is flying the autonomous flight path from the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the images. 1. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors, and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
receive a first geographic location of a structure of interest, the first geographic location having one or more coordinates of the structure of interest;
display, on the display unit, one or more images depicting an aerial view of the structure of interest;
receive, via the input unit, an alteration of the one or more coordinates of the structure of interest indicating a second geographic location of the structure of interest, the second geographic location being different from the first geographic location, and being a validated location of the structure of interest;
subsequent to receiving the alteration of the one or more coordinates of the structure of interest, generate unmanned aircraft information based on the one or more coordinates of the second geographic location, the unmanned aircraft information including flight path information configured to direct an unmanned aircraft to fly an autonomous flight path above the structure of interest, and camera control information to direct a camera of the unmanned aircraft to capture images of the structure of interest at predetermined target capture points on the structure of interest while the unmanned aircraft is flying the flight path;
receive the images of the structure of interest captured by the camera while the unmanned aircraft is flying the autonomous flight path from the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the images. 2. The computerized system of claim 1, wherein the flight path information includes a plurality of flight capture points adjacent to the structure of interest such that the unmanned aircraft captures images of the structure of interest at the plurality of flight capture points. 3. The computerized system of claim 2, wherein first information associated with a first one of the plurality of flight capture points directs the unmanned aircraft to capture a first image of the structure of interest from a first angle relative to the structure of interest and second information associated with a second one of the plurality of flight capture points directs the unmanned aircraft to capture a second image of the structure of interest from a second angle relative to the structure of interest. 4. The computerized system of claim 1, wherein the unmanned aircraft is a multi-rotor aircraft. 5. The computerized system of claim 1, wherein displaying, on the display unit, one or more images depicting an aerial view of the structure of interest includes causing the one or more processors to display a drag and drop element on the aerial view of the structure of interest, and wherein receiving, via the input unit, the alteration of the one or more coordinates of the structure of interest includes a user moving the drag and drop element on the display unit from the first geographic location to the second geographic location. 6. The computerized system of claim 1, wherein receiving, via the input unit, the alteration of the one or more coordinates of the structure of interest indicating the second geographic location of the structure of interest includes the alteration being manual manipulation of the one or more coordinates. 7. The computerized system of claim 1, wherein the computer system has a Geographic Positioning System (GPS), and wherein the first geographic location of the structure of interest is determined by the GPS. 8. The computerized system of claim 1, wherein the first geographic location of the structure of interest is determined by an address of the structure of interest. 9. The computerized system of claim 1, wherein the first geographic location is determined by scrolling on a map displayed on the display unit. 10. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
display on the display unit a graphical representation of a structure of interest to be evaluated, the graphical representation comprising one or more images describing an aerial view of the structure of interest;
display on the display unit a first flight path on the graphical representation of the structure of interest to be evaluated, in which an unmanned aircraft is to navigate about the structure of interest, the first flight path having a first group of flight capture points adjacent to a first side of the structure of interest, and a second group of flight capture points adjacent to a second side of the structure of interest;
alter the first flight path to generate a second flight path;
subsequent to generating the second flight path, generate unmanned aircraft information including flight path information, camera control information, and gimbal control information, the flight path information configured to direct the unmanned aircraft having a computer controlled gimbal mount connected to a camera to fly the second flight path above the structure of interest, the camera control information configured to control the camera to capture overlapping aerial images while the unmanned aircraft is flying the second flight path, and the gimbal control information configured to control the computer controlled gimbal mount to aim the camera at the structure of interest, the second flight path having first instructions to navigate the unmanned aircraft at a first altitude above the structure of interest during a first portion of the flight path and second instructions to navigate the unmanned aircraft at a second altitude above the structure of interest during a second portion of the flight path, wherein the flight path information includes a plurality of flight capture points adjacent to the structure of interest such that the unmanned aircraft captures an image of the structure of interest at the plurality of flight capture points, and wherein first information associated with a first one of the plurality of flight capture points directs the unmanned aircraft to capture a first image of the structure from a first angle relative to the structure and second information associated with a second one of the plurality of flight capture points directs the unmanned aircraft to capture a second image of the structure of interest from a second angle relative to the structure of interest;
receive the overlapping aerial images captured while the unmanned aircraft was flying the second flight path from the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the overlapping aerial images. 10. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
display on the display unit a graphical representation of a structure of interest to be evaluated, the graphical representation comprising one or more images describing an aerial view of the structure of interest;
display on the display unit a first flight path on the graphical representation of the structure of interest to be evaluated, in which an unmanned aircraft is to navigate about the structure of interest, the first flight path having a first group of flight capture points adjacent to a first side of the structure of interest, and a second group of flight capture points adjacent to a second side of the structure of interest;
alter the first flight path to generate a second flight path;
subsequent to generating the second flight path, generate unmanned aircraft information including flight path information, camera control information, and gimbal control information, the flight path information configured to direct the unmanned aircraft having a computer controlled gimbal mount connected to a camera to fly the second flight path above the structure of interest, the camera control information configured to control the camera to capture overlapping aerial images while the unmanned aircraft is flying the second flight path, and the gimbal control information configured to control the computer controlled gimbal mount to aim the camera at the structure of interest, the second flight path having first instructions to navigate the unmanned aircraft at a first altitude above the structure of interest during a first portion of the flight path and second instructions to navigate the unmanned aircraft at a second altitude above the structure of interest during a second portion of the flight path, wherein the flight path information includes a plurality of flight capture points adjacent to the structure of interest such that the unmanned aircraft captures an image of the structure of interest at the plurality of flight capture points, and wherein first information associated with a first one of the plurality of flight capture points directs the unmanned aircraft to capture a first image of the structure from a first angle relative to the structure and second information associated with a second one of the plurality of flight capture points directs the unmanned aircraft to capture a second image of the structure of interest from a second angle relative to the structure of interest;
receive the overlapping aerial images captured while the unmanned aircraft was flying the second flight path from the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the overlapping aerial images. 11. The computerized system of claim 10, wherein the unmanned aircraft is a multi-rotor aircraft. 12. The computerized system of claim 10, wherein the second flight path includes a plurality of spatially disposed flight capture points, a spacing of the flight capture points maintaining at least a minimum overlap between adjacent aerial images to ensure full coverage of the structure of interest in the aerial images captured by the camera. 13. The computerized system of claim 10, wherein the structure of interest has an outline, and wherein the second flight path has flight capture points outside the outline of the structure of interest. 14. The computerized system of claim 13, wherein the second flight path extends entirely beyond the outline of the structure of interest. 15. The computerized system of claim 10, wherein the first flight path is determined automatically by analyzing and using geo-referenced images. 16. The computerized system of claim 10, wherein the structure of interest is a roof of a building. 17. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
display on the display unit a first graphical representation of a structure of interest to be evaluated, an object and a first flight path above or around the structure that would go through the object, the first graphical representation comprising one or more images describing an aerial view of the structure of interest and the object, the structure of interest having an outline and a height;
determine a location of the object from the first graphical representation;
generate unmanned aircraft information using the outline of the structure of interest, and the location of the object, the unmanned aircraft information including flight path information configured to direct an unmanned aircraft through autonomous flight to fly a second flight path above the structure of interest and to avoid the object, and to capture overlapping aerial images of the structure of interest from a camera on the unmanned aircraft while the unmanned aircraft is flying the second flight path, wherein the flight path information includes a plurality of capture points adjacent to the structure such that the unmanned aircraft captures images of the overlapping aerial images of the structure of interest at the plurality of capture points, and wherein first information associated with a first one of the plurality of capture points directs the unmanned aircraft to capture a first image of the structure of interest from a first angle relative to the structure of interest and second information associated with a second one of the plurality of capture points directs the unmanned aircraft to capture a second image of the structure of interest from a second angle relative to the structure of interest;
receive the overlapping aerial images of the structure of interest from the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the overlapping aerial images of the structure captured by the camera of the unmanned aircraft. 17. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
display on the display unit a first graphical representation of a structure of interest to be evaluated, an object and a first flight path above or around the structure that would go through the object, the first graphical representation comprising one or more images describing an aerial view of the structure of interest and the object, the structure of interest having an outline and a height;
determine a location of the object from the first graphical representation;
generate unmanned aircraft information using the outline of the structure of interest, and the location of the object, the unmanned aircraft information including flight path information configured to direct an unmanned aircraft through autonomous flight to fly a second flight path above the structure of interest and to avoid the object, and to capture overlapping aerial images of the structure of interest from a camera on the unmanned aircraft while the unmanned aircraft is flying the second flight path, wherein the flight path information includes a plurality of capture points adjacent to the structure such that the unmanned aircraft captures images of the overlapping aerial images of the structure of interest at the plurality of capture points, and wherein first information associated with a first one of the plurality of capture points directs the unmanned aircraft to capture a first image of the structure of interest from a first angle relative to the structure of interest and second information associated with a second one of the plurality of capture points directs the unmanned aircraft to capture a second image of the structure of interest from a second angle relative to the structure of interest;
receive the overlapping aerial images of the structure of interest from the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the overlapping aerial images of the structure captured by the camera of the unmanned aircraft. 18. The computerized system of claim 17, wherein the unmanned aircraft is a multi-rotor aircraft. 19. The computerized system of claim 17, wherein the structure of interest is a roof of a building. 20. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
display on the display unit a first graphical representation of a structure of interest to be evaluated, the first graphical representation comprising one or more images describing an aerial view of the structure of interest, an object and a first flight path above or around the structure that would go through the object, the structure of interest having an outline and a height;
determine a location of the object from the first graphical representation;
generate unmanned aircraft information using the outline of the structure of interest and the location of the object, the unmanned aircraft information including flight path information configured to direct an unmanned aircraft through autonomous flight to fly a second flight path above the structure of interest and to avoid the object, and to capture overlapping aerial images of the structure of interest from a camera on the unmanned aircraft while the unmanned aircraft is flying the second flight path, the flight path information including instructions to direct a yaw of the unmanned aircraft to aim the camera at target capture points on the structure of interest, wherein the flight path information includes a plurality of capture points adjacent to the structure of interest such that the unmanned aircraft captures an image of the structure at the plurality of capture points, and wherein first information associated with a first one of the plurality of flight capture points directs the unmanned aircraft to capture a first image of the structure of interest from a first angle relative to the structure of interest and second information associated with a second one of the plurality of capture points directs the unmanned aircraft to capture a second image of the structure of interest from a second angle relative to the structure of interest;
receive the overlapping aerial images from the unmanned aircraft, the overlapping aerial images captured during the autonomous flight of the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the overlapping aerial images. 20. A computerized system, comprising:
a computer system having an input unit, a display unit, one or more processors and one or more non-transitory computer readable medium, the one or more processors executing software to cause the one or more processors to:
display on the display unit a first graphical representation of a structure of interest to be evaluated, the first graphical representation comprising one or more images describing an aerial view of the structure of interest, an object and a first flight path above or around the structure that would go through the object, the structure of interest having an outline and a height;
determine a location of the object from the first graphical representation;
generate unmanned aircraft information using the outline of the structure of interest and the location of the object, the unmanned aircraft information including flight path information configured to direct an unmanned aircraft through autonomous flight to fly a second flight path above the structure of interest and to avoid the object, and to capture overlapping aerial images of the structure of interest from a camera on the unmanned aircraft while the unmanned aircraft is flying the second flight path, the flight path information including instructions to direct a yaw of the unmanned aircraft to aim the camera at target capture points on the structure of interest, wherein the flight path information includes a plurality of capture points adjacent to the structure of interest such that the unmanned aircraft captures an image of the structure at the plurality of capture points, and wherein first information associated with a first one of the plurality of flight capture points directs the unmanned aircraft to capture a first image of the structure of interest from a first angle relative to the structure of interest and second information associated with a second one of the plurality of capture points directs the unmanned aircraft to capture a second image of the structure of interest from a second angle relative to the structure of interest;
receive the overlapping aerial images from the unmanned aircraft, the overlapping aerial images captured during the autonomous flight of the unmanned aircraft; and
generate a structure report for the structure of interest based at least in part on the overlapping aerial images. 21. The computerized system of claim 20, wherein the unmanned aircraft is a multi-rotor aircraft. 22. The computerized system of claim 20, wherein the flight path information includes a plurality of spatially disposed flight capture points, a spacing of the flight capture points maintaining at least a minimum overlap between adjacent aerial images to ensure full coverage of the structure of interest in the aerial images captured by the camera. 23. The computerized system of claim 20, wherein the flight path information directs the unmanned aircraft to fly outside of the outline of the structure of interest. 24. The computerized system of claim 20, wherein the flight path information directs the unmanned aircraft to fly entirely beyond the outline of the structure of interest.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318819B2,US10318819B2,Camera surveillance planning and tracking system,2016-01-05,"method, technologies, camera, calculating, product, identifying, coverage, receiving, input, based, program, placement, stereo, specification, herein, computer, surveillance, displaying, operates, using, planning, infrared, embodiment, embodiments, disclosed, least, system, pair, example, tracking","Disclosed herein are system, method, and computer program product embodiments for camera surveillance planning and tracking using, for example, infrared technologies. An embodiment operates by receiving a camera specification input and placement input, calculating a surveillance coverage based on the camera specification and the placement input, displaying the surveillance coverage on a map, and identifying at least one stereo pair.","1. A method, comprising:
receiving a camera specification and placement input;
calculating a surveillance coverage of a camera according to the camera specification and the placement input, wherein the surveillance coverage comprises a geographic area;
displaying the surveillance coverage of the camera on a graphical user interface map, wherein the graphical user interface map displays the geographic area of the surveillance coverage;
identifying a valid video stereo pair according to the surveillance coverage;
identifying, from images received from the valid video stereo pair, a first foreground object having a first trajectory;
identifying, from the images, a second foreground object having a second trajectory;
comparing the second trajectory to the first trajectory to determine a difference between the second trajectory and the first trajectory;
determining that the difference between the second trajectory and the first trajectory exceeds a first threshold and that the second foreground object is a new object that differs from the first foreground object;
in response to the determining, applying size and trajectory filtering to the second foreground image to determine that a size of the second foreground object and the second trajectory exceed a second threshold to selectively designate the second foreground object as an object of interest; and
in response to the applying, updating the graphical user interface map to display the second trajectory on the geographic area of the surveillance coverage. 1. A method, comprising:
receiving a camera specification and placement input;
calculating a surveillance coverage of a camera according to the camera specification and the placement input, wherein the surveillance coverage comprises a geographic area;
displaying the surveillance coverage of the camera on a graphical user interface map, wherein the graphical user interface map displays the geographic area of the surveillance coverage;
identifying a valid video stereo pair according to the surveillance coverage;
identifying, from images received from the valid video stereo pair, a first foreground object having a first trajectory;
identifying, from the images, a second foreground object having a second trajectory;
comparing the second trajectory to the first trajectory to determine a difference between the second trajectory and the first trajectory;
determining that the difference between the second trajectory and the first trajectory exceeds a first threshold and that the second foreground object is a new object that differs from the first foreground object;
in response to the determining, applying size and trajectory filtering to the second foreground image to determine that a size of the second foreground object and the second trajectory exceed a second threshold to selectively designate the second foreground object as an object of interest; and
in response to the applying, updating the graphical user interface map to display the second trajectory on the geographic area of the surveillance coverage. 2. The method of claim 1, further comprising:
calibrating the camera, wherein calibrating the camera comprises generating a plurality of camera parameters. 3. The method of claim 2, wherein the camera specification comprises the plurality of camera parameters. 4. The method of claim 1, further comprising:
receiving a simulation object input, wherein the simulation object input comprises a point in the geographic area. 5. The method of claim 4, the identifying comprising:
highlighting an overlap of the simulation object input and the surveillance coverage on the graphical user interface map. 6. The method of claim 1, the identifying comprising:
eliminating a pair of cameras from a plurality of cameras based upon the surveillance coverage. 7. The method of claim 1, further comprising:
receiving a plurality of simulation object inputs, wherein the simulation object inputs comprise points on the graphical user interface map. 8. A system, comprising:
a memory; and
at least one processor coupled to the memory and configured to:
receive an infrared camera specification and placement input;
calculate a surveillance coverage of an infrared camera according to the infrared camera specification and the placement input, wherein the surveillance coverage comprises a geographic area;
display the surveillance coverage of the infrared camera on a graphical user interface map, wherein the graphical user interface map displays the geographic area of the surveillance coverage;
identify a valid video stereo pair according to the surveillance coverage;
identify, from images received from the valid video stereo pair, a first foreground object having a first trajectory;
identify, from the images, a second foreground object having a second trajectory;
compare the second trajectory to the first trajectory to determine a difference between the second trajectory and the first trajectory;
determine that the difference between the second trajectory and the first trajectory exceeds a first threshold and that the second foreground object is a new object that differs from the first foreground object;
in response to the determining, apply size and trajectory filtering to the second foreground image to determine that a size of the second foreground object and the second trajectory exceed a second threshold to selectively designate the second foreground object as an object of interest; and
in response to the applying, update the graphical user interface map to display the second trajectory on the geographic area of the surveillance coverage. 8. A system, comprising:
a memory; and
at least one processor coupled to the memory and configured to:
receive an infrared camera specification and placement input;
calculate a surveillance coverage of an infrared camera according to the infrared camera specification and the placement input, wherein the surveillance coverage comprises a geographic area;
display the surveillance coverage of the infrared camera on a graphical user interface map, wherein the graphical user interface map displays the geographic area of the surveillance coverage;
identify a valid video stereo pair according to the surveillance coverage;
identify, from images received from the valid video stereo pair, a first foreground object having a first trajectory;
identify, from the images, a second foreground object having a second trajectory;
compare the second trajectory to the first trajectory to determine a difference between the second trajectory and the first trajectory;
determine that the difference between the second trajectory and the first trajectory exceeds a first threshold and that the second foreground object is a new object that differs from the first foreground object;
in response to the determining, apply size and trajectory filtering to the second foreground image to determine that a size of the second foreground object and the second trajectory exceed a second threshold to selectively designate the second foreground object as an object of interest; and
in response to the applying, update the graphical user interface map to display the second trajectory on the geographic area of the surveillance coverage. 9. The system of claim 8, the at least one processor further configured to:
calibrate the infrared camera, wherein calibrating the camera comprises generating a plurality of camera parameters. 10. The system of claim 9, wherein the infrared camera specification comprises the plurality of camera parameters. 11. The system of claim 8, the at least one processor further configured to:
receive a simulation object input, wherein the simulation object input comprises a point in the geographic area. 12. The system of claim 11, wherein to identify, the at least one processor is further configured to:
highlight an overlap of the simulation object input and the surveillance coverage on the graphical user interface map. 13. The system of claim 8, wherein to identify, the at least one processor is further configured to:
eliminate a pair of cameras from a plurality of cameras based upon the surveillance coverage. 14. A tangible, non-transitory computer-readable device having instructions stored thereon that, when executed by at least one computing device, cause the at least one computing device to perform operations comprising:
receiving a camera specification and placement input;
calculating a surveillance coverage of a camera according to the camera specification and the placement input, wherein the surveillance coverage comprises a geographic area;
displaying the surveillance coverage of the camera on a graphical user interface map, wherein the graphical user interface map displays the geographic area of the surveillance coverage;
identifying a valid video stereo pair according to the surveillance coverage;
identifying, from images received from the valid video stereo pair, a first foreground object having a first trajectory;
identifying, from the images, a second foreground object having a second trajectory;
comparing the second trajectory to the first trajectory to determine a difference between the second trajectory and the first trajectory;
determining that the difference between the second trajectory and the first trajectory exceeds a first threshold and that the second foreground object is a new object that differs from the first foreground object;
in response to the determining, applying size and trajectory filtering to the second foreground image to determine that a size of the second foreground object and the second trajectory exceed a second threshold to selectively designate the second foreground object as an object of interest; and
in response to the applying, updating the graphical user interface map to display the second trajectory on the geographic area of the surveillance coverage. 14. A tangible, non-transitory computer-readable device having instructions stored thereon that, when executed by at least one computing device, cause the at least one computing device to perform operations comprising:
receiving a camera specification and placement input;
calculating a surveillance coverage of a camera according to the camera specification and the placement input, wherein the surveillance coverage comprises a geographic area;
displaying the surveillance coverage of the camera on a graphical user interface map, wherein the graphical user interface map displays the geographic area of the surveillance coverage;
identifying a valid video stereo pair according to the surveillance coverage;
identifying, from images received from the valid video stereo pair, a first foreground object having a first trajectory;
identifying, from the images, a second foreground object having a second trajectory;
comparing the second trajectory to the first trajectory to determine a difference between the second trajectory and the first trajectory;
determining that the difference between the second trajectory and the first trajectory exceeds a first threshold and that the second foreground object is a new object that differs from the first foreground object;
in response to the determining, applying size and trajectory filtering to the second foreground image to determine that a size of the second foreground object and the second trajectory exceed a second threshold to selectively designate the second foreground object as an object of interest; and
in response to the applying, updating the graphical user interface map to display the second trajectory on the geographic area of the surveillance coverage. 15. The non-transitory computer-readable device of claim 14, the operations further comprising:
calibrating the camera, wherein calibrating the camera comprises generating a plurality of camera parameters. 16. The non-transitory computer-readable device of claim 15, wherein the camera specification comprises the plurality of camera parameters. 17. The non-transitory computer-readable device of claim 14, the operations further comprising:
receiving a simulation object input, wherein the simulation object input comprises a point in the geographic area. 18. The non-transitory computer-readable device of claim 17, the identifying comprising:
highlighting an overlap of the simulation object input and the surveillance coverage on the graphical user interface map. 19. The non-transitory computer-readable device of claim 14, the identifying comprising:
eliminating a pair of cameras from a plurality of cameras based upon the surveillance coverage. 20. The non-transitory computer-readable device of claim 14, the operations further comprising:
receiving a plurality of simulation object inputs, wherein the simulation object inputs comprise points on the graphical user interface map. 21. The method of claim 1, further comprising:
determining a first 2D trajectory of the first foreground object based on images captured from a first camera of the valid video stereo pair;
determining a second 2D trajectory of the first foreground object based on images captured from a second camera of the valid video stereo pair; and
generating a 3D triangulated trajectory for the first foreground object based on the first 2D trajectory and the second 2D trajectory. 22. The system of claim 8, wherein the at least one processor is further configured to:
determine a first 2D trajectory of the first foreground object based on images captured from a first camera of the valid video stereo pair;
determine a second 2D trajectory of the first foreground object based on images captured from a second camera of the valid video stereo pair; and
generate a 3D triangulated trajectory for the first foreground object based on the first 2D trajectory and the second 2D trajectory. 23. The non-transitory computer-readable device of claim 14, wherein the operations further comprise:
determining a first 2D trajectory of an object based on images captured from a first camera of the valid video stereo pair;
determining a second 2D trajectory of the object based on images captured from a second camera of the valid video stereo pair; and
generating a 3D triangulated trajectory for the object based on the first 2D trajectory and the second 2D trajectory. 24. The method of claim 1, wherein updating the graphical user interface map further comprises:
displaying an altitude measurement with the second trajectory.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318820B2,US10318820B2,Image processing to identify selected individuals in a field of view,2017-03-31,"method, relationship, positioning, identifying, capturing, device, implemented, instructions, detecting, captured, imaging, reference, live, based, while, order, data, wherein, related, identify, view, scene, score, value, friend, computer, ranking, where, field, each, image, individuals, using, includes, between, ability, user, select, processing, selected, record, also, overlap, movement, enable, determining, having, provide, further, recording, social, motion, sets, tracking","A computer-implemented method for detecting and ranking individuals includes capturing live image data of a scene using an imaging device and identifying overlap between reference image data of the selected individuals and the live image data, wherein the overlap includes image data of the selected individuals in the scene. The method also includes capturing sets of image data of the selected individuals while the imaging device is in motion and recording the movement to provide a tracking record having instructions for positioning the imaging device where the imaging device captured the sets of image data of the selected individuals in the scene. The method further includes determining a score for each of the selected individuals in the scene based on a value related to an ability to view each friend and/or a social relationship value in order to enable a user to select who to aim the imaging device at.","1. A computer-implemented method for detecting selected individuals, the method comprising:
accessing, using a processor of an imaging device, reference image data of the selected individuals;
capturing, using the imaging device, live image data of a scene;
identifying, using the processor of the imaging device, overlap between the reference image data of the selected individuals and the live image data of the scene, wherein the overlap comprises image data of the selected individuals in the scene;
based at least in part on the live image data of the selected individuals in the scene, capturing, using the imaging device, sets of image data of the selected individuals in the scene, wherein capturing the sets of image data is performed while the imaging device is in motion;
capturing, using a movement sensor of the imaging device, movement data of the imaging device, wherein capturing the movement data is performed while the imaging device is capturing the sets of image data of the selected individuals in the scene;
based at least in part on the movement data and the sets of image data of the selected individuals in the scene, generating, using the processor of the imaging device, a tracking record comprising instructions for positioning the imaging device where the imaging device captured the sets of image data of the selected individuals in the scene;
providing an indication on a display of the imaging device that each of the selected individuals has been detected in accordance with a ranking of the selected individuals, the ranking being based on an ability to view each friend and/or a value related to a social relationship; and
providing movement instructions based on the tracking record to a user to move the imaging device in order to focus on one or more of the selected individuals in response to a selection by the user. 1. A computer-implemented method for detecting selected individuals, the method comprising:
accessing, using a processor of an imaging device, reference image data of the selected individuals;
capturing, using the imaging device, live image data of a scene;
identifying, using the processor of the imaging device, overlap between the reference image data of the selected individuals and the live image data of the scene, wherein the overlap comprises image data of the selected individuals in the scene;
based at least in part on the live image data of the selected individuals in the scene, capturing, using the imaging device, sets of image data of the selected individuals in the scene, wherein capturing the sets of image data is performed while the imaging device is in motion;
capturing, using a movement sensor of the imaging device, movement data of the imaging device, wherein capturing the movement data is performed while the imaging device is capturing the sets of image data of the selected individuals in the scene;
based at least in part on the movement data and the sets of image data of the selected individuals in the scene, generating, using the processor of the imaging device, a tracking record comprising instructions for positioning the imaging device where the imaging device captured the sets of image data of the selected individuals in the scene;
providing an indication on a display of the imaging device that each of the selected individuals has been detected in accordance with a ranking of the selected individuals, the ranking being based on an ability to view each friend and/or a value related to a social relationship; and
providing movement instructions based on the tracking record to a user to move the imaging device in order to focus on one or more of the selected individuals in response to a selection by the user. 2. The method as claimed in claim 1, further comprising determining a score for each of the selected individuals in the scene based on a value related to an ability to view each friend and/or a value related to a social relationship to provide a plurality of scores, wherein the ranking is based on a ranking of the plurality of scores. 3. The method as claimed in claim 2, wherein the score comprises a value for technical ease of viewing a detected friend based on the series of photographs, a distance from the user to the detected friend based on the series of photographs, and/or a strength of social friendship for the detected friend based on input from social media. 4. The method as claimed in claim 3, further comprising querying one or more social media web-sites to obtain social friendship information for each friend in a plurality of friends. 5. The method as claimed in claim 1, further comprising querying one or more social media web-sites to obtain the reference photograph for one or more friends in a plurality of friends. 6. The method as claimed in claim 1, further comprising downloading a file having the reference photograph for one or more friends in a plurality of friends, the downloading being performed by the user. 7. The method as claimed in claim 1, wherein taking a series of photographs comprises automatically zooming a zoom-lens of the smart imaging device in order to provide increased image resolution capable of identifying facial features. 8. The method as claimed in claim 6, further comprising automatically zooming the zoom-lens in response to the user moving the smart imaging device in accordance with the movement instructions in order to provide sufficient resolution for viewing the selected detected friend. 9. The method as claimed in claim 1, further comprising:
generating a zone map comprising a zone for each of the multiple detected friends, each zone comprising a color indication corresponding to the ranking of scores for the multiple detected friends; and
displaying the zone map on the display of the smart imaging device. 10. The method as claimed in claim 1, wherein at least a portion of the method is capable of being deployed in a cloud computing environment.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318824B2,US10318824B2,Algorithm to extend detecting range for AVM stop line detection,2014-07-23,"method, camera, images, detecting, onto, receives, portion, providing, extended, view, extend, range, extract, upper, provides, image, such, bird, vehicle, front, includes, that, line, detection, roadway, stop, detected, system, remapped, controller, from, programmed, algorithm",A system and method on a vehicle for providing extended detection range of a stop line on a roadway. The system and method includes a front view camera on the vehicle that provides images of the roadway in front of the vehicle and a controller that receives images from the front view camera. The controller is programmed to extract an upper portion of a front view image that is remapped onto an extended bird's eye view image such that the detection range of the stop line is detected.,"1. A system on a vehicle that provides extended detection range of a stop line on a roadway, said system comprising:
a front view camera on the vehicle that provides a front view image of the roadway in front of the vehicle; and
a controller that is programmed to:
receive the front view image from the front view camera;
map a region of the front view image into an original bird's eye view;
detect lane markers in the original bird's eye view;
calculate a pixel intensity reference value from the detected lane markers;
extract an upper region of the front view image;
extract white line pixels from the upper region using the pixel intensity reference value;
remap the white line pixels onto an extended bird's eye view according to a camera calibration look up table;
determine a position of a stop line candidate using the white line pixels via a binarization method; and
track the position of the stop line candidate in the extended bird's eye view in subsequent extracted images. 1. A system on a vehicle that provides extended detection range of a stop line on a roadway, said system comprising:
a front view camera on the vehicle that provides a front view image of the roadway in front of the vehicle; and
a controller that is programmed to:
receive the front view image from the front view camera;
map a region of the front view image into an original bird's eye view;
detect lane markers in the original bird's eye view;
calculate a pixel intensity reference value from the detected lane markers;
extract an upper region of the front view image;
extract white line pixels from the upper region using the pixel intensity reference value;
remap the white line pixels onto an extended bird's eye view according to a camera calibration look up table;
determine a position of a stop line candidate using the white line pixels via a binarization method; and
track the position of the stop line candidate in the extended bird's eye view in subsequent extracted images. 2. The system according to claim 1 further comprising using a predetermined safe value as the reference value if the land markers are not detected in the images from the front view camera. 3. The system according to claim 1 wherein a position of the stop line is predicted using the stop line candidate position that is tracked in the following extracted images. 4. The system according to claim 1 wherein the controller is further programmed to detect an extracted peak from the extracted upper portion of the front view image that corresponds to the stop line. 5. The system according to claim 1 wherein the controller is further programmed to detect an extracted peak from the extracted upper portion of the images using a moving average filter. 6. A system on a vehicle that provides extended detection range of a stop line on a roadway, said system comprising:
a front view camera on the vehicle, said front view camera being capable of providing a front view image of the roadway in front of the vehicle; and
a controller that is programmed to:
receive the front view image from the front view camera;
map a region of the front view image into an original bird's eye view;
detect lane markers in the original bird's eye view;
calculate a pixel intensity reference value from the detected lane markers;
extract an upper region of the front view image;
extract white line pixels from the upper region using the pixel intensity reference value;
remap the white line pixels onto an extended bird's eye view according to a camera calibration look up table;
determine a position of a stop line candidate using the white line pixels via a binarization method; and
track the position of the stop line candidate in the extended bird's eye view in subsequent extracted images. 6. A system on a vehicle that provides extended detection range of a stop line on a roadway, said system comprising:
a front view camera on the vehicle, said front view camera being capable of providing a front view image of the roadway in front of the vehicle; and
a controller that is programmed to:
receive the front view image from the front view camera;
map a region of the front view image into an original bird's eye view;
detect lane markers in the original bird's eye view;
calculate a pixel intensity reference value from the detected lane markers;
extract an upper region of the front view image;
extract white line pixels from the upper region using the pixel intensity reference value;
remap the white line pixels onto an extended bird's eye view according to a camera calibration look up table;
determine a position of a stop line candidate using the white line pixels via a binarization method; and
track the position of the stop line candidate in the extended bird's eye view in subsequent extracted images. 7. The system according to claim 6 wherein a predetermined safe value is used as the reference value if lane markers are not detected in the images from the front view camera. 8. The system according to claim 6 wherein a position of the stop line is predicted using the stop line candidate position that is tracked in the following extracted images. 9. The system according to claim 6 wherein the controller is further programmed to detect an extracted peak from the extracted upper portion of the images using a moving average filter. 10. A method for providing extended detection range on a vehicle for a stop line on a roadway, said method comprising:
providing a front view camera on the vehicle that provides a front view image of the roadway in front of the vehicle; and
using a controller that is programmed to:
receive the front view image from the front view camera;
map a region of the front view image into an original bird's eye view;
detect lane markers in the original bird's eye view;
calculate a pixel intensity reference value from the detected lane markers;
extract an upper region of the front view image;
extract white line pixels from the upper region using the pixel intensity reference value;
remap the white line pixels onto an extended bird's eye view according to a camera calibration look up table;
determine a position of a stop line candidate using the white line pixels via a binarization method; and
track the position of the stop line candidate in the extended bird's eye view in subsequent extracted images. 10. A method for providing extended detection range on a vehicle for a stop line on a roadway, said method comprising:
providing a front view camera on the vehicle that provides a front view image of the roadway in front of the vehicle; and
using a controller that is programmed to:
receive the front view image from the front view camera;
map a region of the front view image into an original bird's eye view;
detect lane markers in the original bird's eye view;
calculate a pixel intensity reference value from the detected lane markers;
extract an upper region of the front view image;
extract white line pixels from the upper region using the pixel intensity reference value;
remap the white line pixels onto an extended bird's eye view according to a camera calibration look up table;
determine a position of a stop line candidate using the white line pixels via a binarization method; and
track the position of the stop line candidate in the extended bird's eye view in subsequent extracted images. 11. The method according to claim 10 further comprising using a predetermined safe value is used as the reference value if the lane markers are not detected in the images from the front view camera. 12. The method according to claim 10 wherein the controller is further programmed to detect an extracted peak from the extracted upper portion of the front view image that corresponds to the stop line. 13. The method according to claim 10 wherein the controller is further programmed to detect an extracted peak from the extracted upper portion of the images using a moving average filter.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318818B2,US10318818B2,Method and apparatus for detecting suspicious activity using video analysis,2004-06-21,"method, obtains, fraudulent, parameter, video, detecting, expected, environment, scanned, data, concerning, compares, sale, identify, applied, sweethearting, retail, activity, with, point, obtaining, such, analyzing, obtain, area, using, apparatus, analysis, transactions, item, suspicious, that, terminal, transaction, automatically, occurs, automated, indicate, detection, count, detects, least, detected, associated, outcome, indicated, system, scan, algorithm, items","A system detects a transaction outcome by obtaining video data associated with a transaction area and analyzing the video data to obtain at least one video transaction parameter concerning transactions associated with the transaction area. The transaction area can be a video count of items indicated in the video data as detected by an automated item detection algorithm applied to the video data. The system obtains at least one expected transaction parameter concerning an expected transaction that occurs in the transaction area, such as a scan count of items scanned at a point of sale terminal. The system automatically compares the video transaction parameter(s) to the expected transaction parameter(s) to identify a transaction outcome that may indicate fraudulent activity such as sweethearting in a retail environment.","1. A method comprising:
via computer processor hardware, performing operations of:
receiving visual information generated by a camera that monitors a transaction area, the visual information capturing images of items passing through the transaction area;
producing video count information comprising a video count number of items indicating a number of items captured in the visual information, by performing a video analysis procedure which counts the items captured in the captured images, as identified from a region of interest of the transaction area, and wherein the video analysis procedure further includes analyzing movement of the items passing through the transaction area to detect scan avoidance of the items with a scanner disposed in the transaction area;
receiving transaction data from the scanner located within a transaction terminal, the transaction data tracking items passing through the transaction area;
producing scan count information comprising a scan count number of items indicating a number of items captured in the transaction data, the scan count information based on an item read region within the transaction area from which the transaction data is gathered via scan detection performed by the scanner, and wherein the number of items captured from the transaction data only includes items which have been identified from the scan detection;

and
performing a comparison between the visual information and the transaction data to identify a discrepancy between the visual information and the transaction data, the comparison comprising comparing the video count information to the scan count information. 1. A method comprising:
via computer processor hardware, performing operations of:
receiving visual information generated by a camera that monitors a transaction area, the visual information capturing images of items passing through the transaction area;
producing video count information comprising a video count number of items indicating a number of items captured in the visual information, by performing a video analysis procedure which counts the items captured in the captured images, as identified from a region of interest of the transaction area, and wherein the video analysis procedure further includes analyzing movement of the items passing through the transaction area to detect scan avoidance of the items with a scanner disposed in the transaction area;
receiving transaction data from the scanner located within a transaction terminal, the transaction data tracking items passing through the transaction area;
producing scan count information comprising a scan count number of items indicating a number of items captured in the transaction data, the scan count information based on an item read region within the transaction area from which the transaction data is gathered via scan detection performed by the scanner, and wherein the number of items captured from the transaction data only includes items which have been identified from the scan detection;

and
performing a comparison between the visual information and the transaction data to identify a discrepancy between the visual information and the transaction data, the comparison comprising comparing the video count information to the scan count information. 2. The method as in claim 1, wherein comparing the video count information to the scan count information further comprises:
detecting that the video count number of items is different than the scan count number of items. 3. The method as in claim 2 further comprising:
in response to detecting that the video count number of items is different than the scan count number of items, generating an alert notifying personnel of corresponding suspicious activity associated with scanning of the items in the transaction area. 4. The method as in claim 1 further comprising:
producing discrepancy information associated with the identified discrepancy, the discrepancy information indicating a difference between an actual count of items as detected using the transaction data and an expected count of items as detected using the visual information. 5. The method as in claim 1 further comprising:
producing discrepancy information associated with the discrepancy, the discrepancy information indicating a difference between an actual count of items as derived from the transaction data and an expected count of items as derived from the visual information. 6. The method as in claim 5 further comprising:
producing a suspicion level based on the discrepancy information; and
providing notification of the suspicion level to a human operator that reviews the transaction data with respect to the visual information. 7. The method as in claim 1, wherein performing the comparison further comprises:
maintaining a reference image capturing the transaction area, the reference image representing background objects in the transaction area; and
comparing the images in the visual information to the reference image to detect the items passing through the transaction area. 8. The method as in claim 7, wherein comparing the images in the visual information to the reference image to detect the items passing through the transaction area further comprises:
identifying the items based on a difference between the images in the real-time video data and the reference image. 9. The method as in claim 1 further comprising:
forwarding the visual information to a human reviewer at a remote location with respect to the transaction area. 10. The method as in claim 1 further comprising:
performing the comparison between the visual information and the transaction data in substantially real-time with respect generation of the visual information and the transaction data. 11. The method as in claim 1 further comprising:
delaying the comparison between the visual information and the transaction data until after completion of passing the items through the transaction area. 12. The method of claim 1, further comprising:
identifying presence of a given item as passing through the transaction area; and
in response to detecting absence of a record in the transaction data corresponding to the given item, producing a notification to security personnel. 13. The method of claim 1, wherein the transaction area is part of a retail system in which a customer presents at least a portion of the items to a cashier for purchase. 14. The method of claim 1, wherein the transaction area is part of a retail system in which a customer presents the items for passing through the transaction area. 15. The method of claim 1 wherein the video count includes items in the scan count and items which eluded being scanned while passing through the transaction area, and wherein the video count is greater than the scan count.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318822B2,US10318822B2,Object tracking,2017-04-06,"module, transformation, receiving, including, imaging, based, systems, data, object, methods, using, includes, dimensions, provided, correlating, determines, location, tracks, height, ground, space, system, dimensional, three, tracking",Methods and systems are provided for tracking an object. The system includes a data receiving module receiving two dimensional imaging data including an object and height map data correlating ground height and location. A two dimensions to three dimensions transformation module determines a location of the object in three dimensional space based on the two dimensional imaging data and the height map data. A tracking module tracks the object using the location of the object.,"1. An object tracking system for a vehicle having one or more vehicle control components, comprising:
a vehicle control module in communication with the one or more control components, the vehicle control module including:
a non-transitory computer readable medium comprising:
a data storage module configured to, by a processor, store in a data storage device height map data correlating ground height and location and generated from lidar data;
a data receiving module configured to, by a processor, receive two dimensional imaging data generated by a camera and including at least one object;
a two dimensions to three dimensions transformation module configured to, by a processor, determine a location of the camera in the height map data based on a real world location of the camera, project the at least one object of the two dimensional imaging data into the height map data based on the location and ray tracing to determine a distance of the object from the camera, and determine a location of the at least one object in a three dimensional space based on the distance;
a tracking module configured to, by a processor, track at least one object using the location of the at least one object in the three dimensional space and generate tracking data based thereon; and
the vehicle control module being configured to actuate at least one of the one or more control components based, in part, on the tracking data. 1. An object tracking system for a vehicle having one or more vehicle control components, comprising:
a vehicle control module in communication with the one or more control components, the vehicle control module including:
a non-transitory computer readable medium comprising:
a data storage module configured to, by a processor, store in a data storage device height map data correlating ground height and location and generated from lidar data;
a data receiving module configured to, by a processor, receive two dimensional imaging data generated by a camera and including at least one object;
a two dimensions to three dimensions transformation module configured to, by a processor, determine a location of the camera in the height map data based on a real world location of the camera, project the at least one object of the two dimensional imaging data into the height map data based on the location and ray tracing to determine a distance of the object from the camera, and determine a location of the at least one object in a three dimensional space based on the distance;
a tracking module configured to, by a processor, track at least one object using the location of the at least one object in the three dimensional space and generate tracking data based thereon; and
the vehicle control module being configured to actuate at least one of the one or more control components based, in part, on the tracking data. 2. The object tracking system of claim 1, wherein the two dimensions to three dimensions transformation module is further configured to project the at least one object of the two dimensional imaging data into the height map data to determine a ground intersection of the at least one object and, based on the ground intersection and the correlation of ground height and location in the height map data to determine the distance of the object from the camera. 3. The object tracking system of claim 1, wherein the data storage module is configured to store calibration data including camera pose data, and wherein the two dimensions to three dimensions transformation module is configured to project the at least one object of the two dimensional imaging data into the height map data from the camera pose. 4. The object tracking system of claim 1, comprising an object identification module configured to demarcate the at least one object in the two dimensional imaging data to obtain two dimensional object data, and wherein the two dimensions to three dimensions transformation module is configured to determine the location of the at least one object in the three dimensional space based on the two dimensional object data. 5. The object tracking system of claim 4, wherein the object identification module is configured to determine at least one bounding box as the two dimensional object data. 6. The object tracking system of claim 4, wherein the two dimensions to three dimensions transformation module is configured to transform a bottom of the at least one bounding box to a ground intersection in the height map data in determining the location of the at least one object in three dimensional space. 7. The object tracking system of claim 1, comprising a visual classification module configured to run a neural network to classify the at least one object and to determine dimensions of the at least one object based on the classification, wherein the tracking module is configured to track at least one object using the location of the at least one object and the dimensions of the at least one object. 8. The object tracking system of claim 7, comprising an object identification module configured to demarcate the at least one object to obtain at least one bounding box, wherein the visual classification module is configured to perform bounding box regression on the at least one bounding box using the neural network to obtain at least one regressed bounding box, and wherein the two dimensions to three dimensions transformation module is configured to determine the location of the at least one object in three dimensional space based on the two dimensional imaging data, the height map data and the at least one regressed bounding box. 9. The object tracking system of claim 1, wherein the data receiving module is configured to receive three dimensional imaging data including at least one other object, wherein the tracking module is configured to track the at least one object based on the two dimensional imaging data and the at least one other object based on the three dimensional imaging data. 10. An autonomous vehicle, comprising:
a sensor system comprising at least one camera that generates two dimensional imaging data including at least one object;
a data storage device configured to store height map data correlating ground height and location;
a processor configured to:
determine a location of the at least one camera in the height map data based on a real world location of the autonomous vehicle and a pose of the at least one camera,
project the at least one object of the two dimensional imaging data into the height map data based on the location and ray tracing to determine a distance of the object from the camera,
determine a location of the at least one object in a three dimensional space based on the distance;
track at least one object using the location of the at least one object and to responsively output tracking data; and
an autonomous vehicle control module configured to control one or more vehicle components based, in part, on the tracking data. 10. An autonomous vehicle, comprising:
a sensor system comprising at least one camera that generates two dimensional imaging data including at least one object;
a data storage device configured to store height map data correlating ground height and location;
a processor configured to:
determine a location of the at least one camera in the height map data based on a real world location of the autonomous vehicle and a pose of the at least one camera,
project the at least one object of the two dimensional imaging data into the height map data based on the location and ray tracing to determine a distance of the object from the camera,
determine a location of the at least one object in a three dimensional space based on the distance;
track at least one object using the location of the at least one object and to responsively output tracking data; and
an autonomous vehicle control module configured to control one or more vehicle components based, in part, on the tracking data. 11. The autonomous vehicle of claim 10, wherein the processor is configured to transform a height of the at least one object in the two dimensional imaging data to a ground intersection in the height map data and determine the distance based on the ground intersection. 12. The autonomous vehicle of claim 10, wherein the processor is configured to demarcate the at least one object to obtain at least one bounding box and to perform bounding box regression on the at least one bounding box using a neural network to obtain at least one regressed bounding box, and wherein processor is configured to determine the location of the at least one object in the three dimensional space based on the at least one regressed bounding box. 13. The autonomous vehicle of claim 12, wherein the processor is configured to transform a bottom of the at least one bounding box to a ground intersection in the height map data in determining the location of the at least one object in three dimensional space. 14. A method of tracking at least one object, comprising:
storing, via a processor, height map data correlating ground height and real world location;
receiving, via a processor, two dimensional imaging data including at least one object;
determining, via a processor, a location of the camera in the height map data based on a real world location of the camera;
projecting, via a processor, the at least one object of the two dimensional imaging data into the height map data based on the location and ray tracing to determine a distance of the object from the camera;
determining, via a processor, a location of the at least one object in a three dimensional space based on the distance;
tracking, via a processor, at least one object using the location of the at least one object for use in autonomous vehicle control; and
exercising vehicle control based, in part, on the tracking. 14. A method of tracking at least one object, comprising:
storing, via a processor, height map data correlating ground height and real world location;
receiving, via a processor, two dimensional imaging data including at least one object;
determining, via a processor, a location of the camera in the height map data based on a real world location of the camera;
projecting, via a processor, the at least one object of the two dimensional imaging data into the height map data based on the location and ray tracing to determine a distance of the object from the camera;
determining, via a processor, a location of the at least one object in a three dimensional space based on the distance;
tracking, via a processor, at least one object using the location of the at least one object for use in autonomous vehicle control; and
exercising vehicle control based, in part, on the tracking. 15. The method of claim 14, comprising receiving, via a processor, calibration data including camera pose data spatially relating the autonomous vehicle and the at least one camera, determining, via a processor, a camera pose in the height map data based on the location of the vehicle and the camera pose data and ray tracing the at least one object of the two dimensional imaging data into the height map data from the camera pose in the height map data in determining a location of the at least one object in three dimensional space. 16. The method of claim 14, comprising determining, via a processor, a ground intersection in the height map data based on the at least one object in the two dimensional imaging data in determining location of the at least one object in three dimensional space.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318823B2,US10318823B2,Forward-facing multi-imaging system for navigating a vehicle,2013-10-14,"autonomous, forward, device, configured, images, receive, driver, navigational, imaging, features, interfaces, cameras, implementation, analyze, based, systems, data, acquired, stereo, devices, response, navigating, navigation, assist, image, vehicle, area, methods, multi, analysis, include, processing, facing, provided, also, more, capture, cause, provide, least, further, system, monocular, acquire","Systems and methods use cameras to provide autonomous navigation features. In one implementation, a driver-assist system is provided for a vehicle. The system may include one or more image capture devices configured to acquire images of an area forward of the vehicle. The system may also include at least one processing device configured to receive, via one or more data interfaces, the images. The at least one processing device may be further configured to analyze the images acquired by the one or more image capture devices and cause at least one navigational response in the vehicle based on monocular and/or stereo image analysis of the images.","1. A driver-assist system for a vehicle, the system comprising:
a first image capture device configured to acquire a first plurality of images of an area forward of the vehicle, wherein the first image capture device has a first angular field of view;
a second image capture device configured to acquire a second plurality of images of an area forward of the vehicle, wherein the second image capture device has a second angular field of view greater than the first angular field of view, the second angular field of view having at least a portion overlapping with the first angular field of view;
a third image capture device configured to acquire a third plurality of images of an area forward of the vehicle, wherein the third image capture device has a third angular field of view greater than the second angular field of view, the third angular field of view having at least a portion overlapping with the first angular field of view and with the second angular field of view;
a data interface; and
at least one processing device configured to:
receive, via the data interface, the first, second, and third plurality of images;
analyze two or more images of the first, second, and third plurality of images to detect a feature associated with an environment of the vehicle and track a position of the detected feature relative to the vehicle; and
transmit a control signal generated in response to the detected and tracked feature to at least one of a plurality of vehicle control systems to cause the vehicle to take at least one navigational response based on image information derived from any two of the first, second, and third plurality of images. 1. A driver-assist system for a vehicle, the system comprising:
a first image capture device configured to acquire a first plurality of images of an area forward of the vehicle, wherein the first image capture device has a first angular field of view;
a second image capture device configured to acquire a second plurality of images of an area forward of the vehicle, wherein the second image capture device has a second angular field of view greater than the first angular field of view, the second angular field of view having at least a portion overlapping with the first angular field of view;
a third image capture device configured to acquire a third plurality of images of an area forward of the vehicle, wherein the third image capture device has a third angular field of view greater than the second angular field of view, the third angular field of view having at least a portion overlapping with the first angular field of view and with the second angular field of view;
a data interface; and
at least one processing device configured to:
receive, via the data interface, the first, second, and third plurality of images;
analyze two or more images of the first, second, and third plurality of images to detect a feature associated with an environment of the vehicle and track a position of the detected feature relative to the vehicle; and
transmit a control signal generated in response to the detected and tracked feature to at least one of a plurality of vehicle control systems to cause the vehicle to take at least one navigational response based on image information derived from any two of the first, second, and third plurality of images. 2. The system of claim 1, wherein the first angular field of view is selected from within a range of 20 degrees to 40 degrees. 3. The system of claim 1, wherein the second angular field of view is selected from within a range of 40 degrees to 60 degrees. 4. The system of claim 1, wherein the third angular field of view is selected from within a range of 100 degrees to 190 degrees. 5. The system of claim 1, wherein the at least one processing device is further configured to:
cause the vehicle to take the at least one navigational response based at least partially on map data and a determined position of the vehicle. 6. The system of claim 1, wherein the at least one processing device is further configured to:
cause the vehicle to take the at least one navigational response based on a relative velocity between the vehicle and an object detected within any of the first, second, and third plurality of images, wherein the at least one processing device is configured to determine the relative velocity based on any of the first, second, and third plurality of images. 7. The system of claim 1, wherein the at least one processing device is further configured to:
cause the vehicle to take the at least one navigational response based on a relative acceleration between the vehicle and an object detected within any of the first, second, and third plurality of images, wherein the at least one processing device is configured to determine the relative acceleration based on any of the first, second, and third plurality of images. 8. The system of claim 1, wherein the derived image information identifies one or more lane marks. 9. The system of claim 1, wherein analyzing two or more images of the first, second, and third plurality of images to detect a feature associated with the environment of the vehicle includes:
calculating a disparity of pixels between images obtained by at least two different ones from the first image capture device, the second image capture device, and the third image capture device. 10. The system of claim 9, wherein analyzing two or more images of the first, second, and third plurality of images to detect a feature associated with the environment of the vehicle includes:
generating a three-dimensional reconstruction of the environment of the vehicle based on the calculated disparity. 11. The system of claim 10, wherein analyzing two or more images of the first, second, and third plurality of images to detect a feature associated with the environment of the vehicle includes:
combining the three-dimensional reconstruction of the environment with three-dimensional map data or three-dimensional information calculated based on images from one of the first, second, or third plurality of images. 12. The system of claim 9, wherein analyzing two or more images of the first, second, and third plurality of images to detect a feature associated with the environment of the vehicle includes:
calculating a camera displacement relating to at least one of the first image capture device, the second image capture device, and the third image capture device; and
calculating the disparity of pixels between successive images. 13. The system of claim 1, wherein the at least one processing device is further configured to select the two or more images of the first, second, and third plurality of images for analysis based on one or more objects detected in the first, second, and third plurality of images. 14. The system of claim 1, wherein the at least one processing device is further configured to select the two or more images of the first, second, and third plurality of images for analysis based on at least one of the following: image quality, image resolution, effective field of view reflected in the images, the number of captured frames, a percentage of frames in which an object of interest appears, or a proportion of the object that appears in the frames. 15. A vehicle, comprising:
a body;
a first image capture device configured to acquire a first plurality of images of an area forward of the body of the vehicle, wherein the first image capture device has a first angular field of view;
a second image capture device configured to acquire a second plurality of images of an area forward of the body of the vehicle, wherein the second image capture device has a second angular field of view greater than the first angular field of view, the second angular field of view having at least a portion overlapping with the first angular field of view;
a third image capture device configured to acquire a third plurality of images of an area forward of the body of the vehicle, wherein the third image capture device has a third angular field of view greater than the second angular field of view, the third angular field of view having at least a portion overlapping with the first angular field of view and with the second angular field of view;
a plurality of vehicle control systems;
a data interface; and
at least one processing device configured to:
receive, via the data interface, the first, second, and third plurality of images;
analyze two or more images of the first, second, and third plurality of images to detect a feature associated with an environment of the vehicle and track a position of the detected feature relative to the vehicle; and
transmit a control signal generated in response to the detected and tracked feature to at least one of the plurality of vehicle control systems to cause the vehicle to take at least one navigational response based on image information derived from any two of the first, second, and third plurality of images. 15. A vehicle, comprising:
a body;
a first image capture device configured to acquire a first plurality of images of an area forward of the body of the vehicle, wherein the first image capture device has a first angular field of view;
a second image capture device configured to acquire a second plurality of images of an area forward of the body of the vehicle, wherein the second image capture device has a second angular field of view greater than the first angular field of view, the second angular field of view having at least a portion overlapping with the first angular field of view;
a third image capture device configured to acquire a third plurality of images of an area forward of the body of the vehicle, wherein the third image capture device has a third angular field of view greater than the second angular field of view, the third angular field of view having at least a portion overlapping with the first angular field of view and with the second angular field of view;
a plurality of vehicle control systems;
a data interface; and
at least one processing device configured to:
receive, via the data interface, the first, second, and third plurality of images;
analyze two or more images of the first, second, and third plurality of images to detect a feature associated with an environment of the vehicle and track a position of the detected feature relative to the vehicle; and
transmit a control signal generated in response to the detected and tracked feature to at least one of the plurality of vehicle control systems to cause the vehicle to take at least one navigational response based on image information derived from any two of the first, second, and third plurality of images. 16. The vehicle of claim 15, wherein the at least one processing device is further configured to:
cause the vehicle to take the at least one navigational response based at least partially on map data and a determined position of the vehicle. 17. The vehicle of claim 15, wherein the at least one processing device is further configured to:
cause the vehicle to take the at least one navigational response based on a relative velocity between the vehicle and an object detected within any of the first, second, and third plurality of images, wherein the at least one processing device is configured to determine the relative velocity based on any of the first, second, and third plurality of images. 18. The vehicle of claim 15, wherein the at least one processing device is further configured to:
cause the vehicle to take the at least one navigational response based on a relative acceleration between the vehicle and an object detected within any of the first, second, and third plurality of images, wherein the at least one processing device is configured to determine the relative acceleration based on any of the first, second, and third plurality of images. 19. The vehicle of claim 15, wherein the derived image information identifies one or more lane marks.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318825B2,US10318825B2,Vehicle-mounted image recognition apparatus and method of manufacturing the same,2014-12-17,"method, direction, closer, optical, half, higher, this, without, recognition, lower, imaging, than, away, through, achieved, position, adjusting, lens, recognizing, lanes, with, high, when, image, same, different, exhibiting, vehicle, expensive, using, apparatus, axis, selecting, between, circumferential, middle, plane, resolution, sensor, mounted, center, both, manufacturing, radial, least, forming, make, improves, projected, focus, system, from, accuracy, located, traffic","In a vehicle-mounted image recognition apparatus, a resolution of an image projected on an imaging plane of an image sensor is different at a position away from a center between a circumferential direction and a radial direction. To make a circumferential resolution higher than a radial resolution, at least a lower half of the imaging plane is located closer to a circumferential focus than a middle of a radial focus and the circumferential focus at a position off the optical axis center of the image projected on the image sensor through the image-forming optical system. This is achieved by adjusting the position of the imaging plane when manufacturing or by selecting a lens with high circumferential resolution. This apparatus improves recognition accuracy in recognizing traffic lanes without using an expensive lens exhibiting high resolution both in the circumferential direction and in the radial direction.","1. A vehicle-mounted image recognition apparatus comprising:
a fixed-focus image-forming optical system having an optical axis, the image-forming optical system forming an image of a front scene at a rear position on the optical axis;
an image sensor disposed at the rear position on the optical axis of the image-forming optical system, the optical axis extending through an imaging plane of the image sensor;
a sensor retainer holding the image sensor;
an image-forming optical system retainer holding the image-forming optical system; and
an integrated circuit obtaining data of the image captured by the image sensor and performing an image recognition process; wherein
at least a portion of a gap between the sensor retainer and the image-forming optical system retainer is filled with an adhesive;
an edge portion of the image-forming optical system retainer is not in contact with the sensor retainer;
at least a lower half of the imaging plane is located closer to a circumferential focus than a middle between a radial focus and the circumferential focus, at a position distant from an optical axis center of the image by about 70% of an image height of the image;
a distance between the circumferential focus and the imaging plane is smaller than a distance between the circumferential focus and the radial focus;
the integrated circuit recognizing a line indicating a traffic lane on a road surface in performing the image recognition process;
the radial focus is defined as a point at which a maximum radial resolving power of light converging through the image-forming optical system is obtained when a projection surface moves along the optical axis;
the circumferential focus is defined as a point at which a maximum circumferential resolving power of the converging light is obtained when the projection surface moves along the optical axis;
the lower half is defined as a portion of the imaging plane on which a lower half of the scene in a vertical direction is projected;
the image height is defined as half a diagonal length of the imaging plane; and
the optical axis center is defined as an intersection of the optical axis and the imaging plane. 1. A vehicle-mounted image recognition apparatus comprising:
a fixed-focus image-forming optical system having an optical axis, the image-forming optical system forming an image of a front scene at a rear position on the optical axis;
an image sensor disposed at the rear position on the optical axis of the image-forming optical system, the optical axis extending through an imaging plane of the image sensor;
a sensor retainer holding the image sensor;
an image-forming optical system retainer holding the image-forming optical system; and
an integrated circuit obtaining data of the image captured by the image sensor and performing an image recognition process; wherein
at least a portion of a gap between the sensor retainer and the image-forming optical system retainer is filled with an adhesive;
an edge portion of the image-forming optical system retainer is not in contact with the sensor retainer;
at least a lower half of the imaging plane is located closer to a circumferential focus than a middle between a radial focus and the circumferential focus, at a position distant from an optical axis center of the image by about 70% of an image height of the image;
a distance between the circumferential focus and the imaging plane is smaller than a distance between the circumferential focus and the radial focus;
the integrated circuit recognizing a line indicating a traffic lane on a road surface in performing the image recognition process;
the radial focus is defined as a point at which a maximum radial resolving power of light converging through the image-forming optical system is obtained when a projection surface moves along the optical axis;
the circumferential focus is defined as a point at which a maximum circumferential resolving power of the converging light is obtained when the projection surface moves along the optical axis;
the lower half is defined as a portion of the imaging plane on which a lower half of the scene in a vertical direction is projected;
the image height is defined as half a diagonal length of the imaging plane; and
the optical axis center is defined as an intersection of the optical axis and the imaging plane. 2. The vehicle-mounted image recognition apparatus of claim 1, wherein an image captured by the image sensor has generally a rectangular shape; and
a length from a center of the image to an either one of the horizontal edge of the image is smaller than about 70% of an image height of the image. 3. The vehicle-mounted image recognition apparatus of claim 1, wherein the distance between the circumferential focus and the imaging plane is less than half a distance between the imaging plane and the radial focus at the position distant from the optical axis center by about 70% of the image height. 4. The vehicle-mounted image recognition apparatus of claim 2, wherein the distance between the circumferential focus and the imaging plane is less than half a distance between the imaging plane and the radial focus at the position distant from the optical axis center by about 70% of the image height. 5. The vehicle-mounted image recognition apparatus of claim 1, wherein
at the position distant from the optical axis center on the imaging plane of the image sensor by about 70% of the image height, the circumferential resolving power of the image projected on the imaging plane through the image-forming optical system is higher than the radial resolving power;
the circumferential resolving power is defined as a modulation transfer function value with respect to an image of a plurality of black straight lines arranged in a circumferential direction at an interval of about 1/(9d) lp/mm and extending in a radial direction on the imaging plane, d (mm) being a pixel spacing of the image sensor. 6. The vehicle-mounted image recognition apparatus of claim 2, wherein
at the position distant from the optical axis center on the imaging plane of the image sensor by about 70% of the image height, the circumferential resolving power of the image projected on the imaging plane through the image-forming optical system is higher than the radial resolving power;
the circumferential resolving power is defined as a modulation transfer function value with respect to an image of a plurality of black straight lines arranged in a circumferential direction at an interval of about 1/(9d) lp/mm and extending in a radial direction on the imaging plane, d (mm) being a pixel spacing of the image sensor. 7. The vehicle-mounted image recognition apparatus of claim 3, wherein
at the position distant from the optical axis center on the imaging plane of the image sensor by about 70% of the image height, the circumferential resolving power of the image projected on the imaging plane through the image-forming optical system is higher than the radial resolving power;
the circumferential resolving power is defined as a modulation transfer function value with respect to an image of a plurality of black straight lines arranged in a circumferential direction at an interval of about 1/(9d) lp/mm and extending in a radial direction on the imaging plane, d (mm) being a pixel spacing of the image sensor. 8. The vehicle-mounted image recognition apparatus of claim 4, wherein
at the position distant from the optical axis center on the imaging plane of the image sensor by about 70% of the image height, the circumferential resolving power of the image projected on the imaging plane through the image-forming optical system is higher than the radial resolving power;
the circumferential resolving power is defined as a modulation transfer function value with respect to an image of a plurality of black straight lines arranged in a circumferential direction at an interval of about 1/(9d) lp/mm and extending in a radial direction on the imaging plane, d (mm) being a pixel spacing of the image sensor. 9. The vehicle-mounted image recognition apparatus of claim 5, wherein
the image sensor includes a color filter array on a surface of the imaging plane; and
the circumferential resolving power has a value greater than or equal to about 50%. 10. The vehicle-mounted image recognition apparatus of claim 6, wherein
the image sensor includes a color filter array on a surface of the imaging plane; and
the circumferential resolving power has a value greater than or equal to about 50%. 11. The vehicle-mounted image recognition apparatus of claim 7, wherein
the image sensor includes a color filter array on a surface of the imaging plane; and
the circumferential resolving power has a value greater than or equal to about 50%. 12. The vehicle-mounted image recognition apparatus of claim 8, wherein
the image sensor includes a color filter array on a surface of the imaging plane; and
the circumferential resolving power has a value greater than or equal to about 50%. 13. The vehicle-mounted image recognition apparatus of claim 9, wherein the image-forming optical system has an F value less than or equal to 2. 14. The vehicle-mounted image recognition apparatus of claim 10, wherein the image-forming optical system has an F value less than or equal to 2. 15. The vehicle-mounted image recognition apparatus of claim 11, wherein the image-forming optical system has an F value less than or equal to 2. 16. The vehicle-mounted image recognition apparatus of claim 12, wherein the image-forming optical system has an F value less than or equal to 2. 17. A vehicle-mounted image recognition apparatus comprising:
a fixed-focus image-forming optical system having an optical axis, the image-forming optical system forming an image of a front scene at a rear position on the optical axis;
an image sensor disposed at the rear position on the optical axis of the image-forming optical system, the optical axis extending through an imaging plane of the image sensor;
a sensor retainer holding the image sensor;
an image-forming optical system retainer holding the image-forming optical system; and
an integrated circuit obtaining data of the image captured by the image sensor and performing an image recognition process; wherein
at least a portion of a gap between the sensor retainer and the image-forming optical system retainer is filled with an adhesive;
an edge portion of the image-forming optical system retainer is not in contact with the sensor retainer;
at least in the lower half of the imaging plane located at a position of 70% of the image height from the optical axis center, a circumferential resolving power of an image projected on the image sensor by the imaging optical system is higher than a radial resolving power;
the integrated circuit recognizing a line indicating a traffic lane on a road surface in performing the image recognition process;
the radial focus is defined as a point at which a maximum radial resolving power of light converging through the image-forming optical system is obtained when a projection surface moves along the optical axis;
the circumferential focus is defined as a point at which a maximum circumferential resolving power of the converging light is obtained when the projection surface moves along the optical axis;
the lower half is defined as a portion of the imaging plane on which a lower half of the scene in a vertical direction is projected;
the image height is defined as half a diagonal length of the imaging plane; and
the optical axis center is defined as an intersection of the optical axis and the imaging plane. 17. A vehicle-mounted image recognition apparatus comprising:
a fixed-focus image-forming optical system having an optical axis, the image-forming optical system forming an image of a front scene at a rear position on the optical axis;
an image sensor disposed at the rear position on the optical axis of the image-forming optical system, the optical axis extending through an imaging plane of the image sensor;
a sensor retainer holding the image sensor;
an image-forming optical system retainer holding the image-forming optical system; and
an integrated circuit obtaining data of the image captured by the image sensor and performing an image recognition process; wherein
at least a portion of a gap between the sensor retainer and the image-forming optical system retainer is filled with an adhesive;
an edge portion of the image-forming optical system retainer is not in contact with the sensor retainer;
at least in the lower half of the imaging plane located at a position of 70% of the image height from the optical axis center, a circumferential resolving power of an image projected on the image sensor by the imaging optical system is higher than a radial resolving power;
the integrated circuit recognizing a line indicating a traffic lane on a road surface in performing the image recognition process;
the radial focus is defined as a point at which a maximum radial resolving power of light converging through the image-forming optical system is obtained when a projection surface moves along the optical axis;
the circumferential focus is defined as a point at which a maximum circumferential resolving power of the converging light is obtained when the projection surface moves along the optical axis;
the lower half is defined as a portion of the imaging plane on which a lower half of the scene in a vertical direction is projected;
the image height is defined as half a diagonal length of the imaging plane; and
the optical axis center is defined as an intersection of the optical axis and the imaging plane. 18. The vehicle-mounted image recognition apparatus of claim 17, wherein
the image sensor includes a color filter array on a surface of the imaging plane; and
a circumferential resolving power has a value greater than or equal to about 50%;
the circumferential resolving power is defined as a modulation transfer function value with respect to an image of a plurality of black straight lines arranged in a circumferential direction at an interval of about 1/(9d) lp/mm and extending in a radial direction on the imaging plane, d (mm) being a pixel spacing of the image sensor. 19. The vehicle-mounted image recognition apparatus of claim 17, wherein an image captured by the image sensor has generally a rectangular shape; and
a length from a center of the image to an either one of the horizontal edge of the image is smaller than about 70% of an image height of the image. 20. The vehicle-mounted image recognition apparatus of claim 18, wherein the imaging plane of the image sensor has generally a rectangular shape; and
a length from a center of the imaging plane to an either one of the horizontal edge of the imaging plane is smaller than about 70% of an image height of the image. 21. The vehicle-mounted image recognition apparatus of claim 17, wherein the image-forming optical system has an F value less than or equal to 2. 22. The vehicle-mounted image recognition apparatus of claim 18, wherein the image-forming optical system has an F value less than or equal to 2. 23. The vehicle-mounted image recognition apparatus of claim 19, wherein the image-forming optical system has an F value less than or equal to 2. 24. The vehicle-mounted image recognition apparatus of claim 20, wherein the image-forming optical system has an F value less than or equal to 2.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318826B2,US10318826B2,Rear obstacle detection and distance estimation,2016-10-07,"method, parameters, camera, disclosure, second, structure, identifying, estimation, captured, features, model, within, distance, planar, based, corresponding, systems, wherein, view, obstacle, estimating, image, vehicle, frame, methods, using, includes, relates, facing, rear, detection, comprise, objects, determining, from, adjacent, motion, first, frames","The disclosure relates to systems and methods for estimating or determining the motion of a vehicle and/or the distance to objects within view of a rear camera. A method for rear obstacle detection using structure from motion includes identifying image features in a first frame corresponding to features in a second frame, wherein the first frame and the second frame comprise adjacent image frames captured by a rear-facing camera of a vehicle. The method includes determining parameters for a non-planar motion model based on the image features. The method includes determining camera motion based on the parameters for the non-planar motion model.","1. A method for rear obstacle detection using structure from motion, the method comprising:
identifying image features in a first frame corresponding to features in a second frame, wherein the first frame and the second frame comprise adjacent image frames captured by a rear-facing camera of a vehicle;
determining parameters for a non-planar motion model based on the image features;
generating a three-dimensional reconstruction of feature points of the image features based on the non-planar motion model;
determining a height of one or more objects based on corresponding feature points of the feature points;
determining a distance to the one or more objects having a height above a threshold;
determining camera motion based on the parameters for the non-planar motion model; and
notifying a driver or automated driving system of a presence of the one or more objects having the height above the threshold. 1. A method for rear obstacle detection using structure from motion, the method comprising:
identifying image features in a first frame corresponding to features in a second frame, wherein the first frame and the second frame comprise adjacent image frames captured by a rear-facing camera of a vehicle;
determining parameters for a non-planar motion model based on the image features;
generating a three-dimensional reconstruction of feature points of the image features based on the non-planar motion model;
determining a height of one or more objects based on corresponding feature points of the feature points;
determining a distance to the one or more objects having a height above a threshold;
determining camera motion based on the parameters for the non-planar motion model; and
notifying a driver or automated driving system of a presence of the one or more objects having the height above the threshold. 2. The method of claim 1, further comprising identifying one or more feature points as part of a same object based on one or more of a pixel intensity or two-dimensional location within the first frame or second frame. 3. The method of claim 1, further comprising determining a scale for location of the feature points in three-dimensional space based on feature points within a predefined ground area in the first frame and the second frame. 4. The method of claim 1, further comprising classifying features as inliers and outliers, wherein determining the parameters comprises determining based on the inliers. 5. The method of claim 1, further comprising performing local bundle adjustment on image features for improved accuracy. 6. A system for rear obstacle detection using structure from motion, the system comprising:
a monocular camera of a vehicle; and
a vehicle controller in communication with the rear-facing camera, wherein the vehicle controller is configured to:
obtain a series of image frames captured by the monocular camera during movement of the vehicle;
identify image features in a first frame corresponding to features in a second frame, wherein the first frame and the second frame comprise adjacent image frames in the series of image frames;
determine parameters for a non-planar motion model based on the image features;
generate a three-dimensional reconstruction of feature points of the image features based on the non-planar motion model;
determine a height of one or more objects based on corresponding feature points of the feature points;
determine a distance to the one or more objects having a height above a threshold;
determine camera motion based on the parameters for the non-planar motion model; and
notify a driver or automated driving system of a presence of the one or more objects having the height above the threshold. 6. A system for rear obstacle detection using structure from motion, the system comprising:
a monocular camera of a vehicle; and
a vehicle controller in communication with the rear-facing camera, wherein the vehicle controller is configured to:
obtain a series of image frames captured by the monocular camera during movement of the vehicle;
identify image features in a first frame corresponding to features in a second frame, wherein the first frame and the second frame comprise adjacent image frames in the series of image frames;
determine parameters for a non-planar motion model based on the image features;
generate a three-dimensional reconstruction of feature points of the image features based on the non-planar motion model;
determine a height of one or more objects based on corresponding feature points of the feature points;
determine a distance to the one or more objects having a height above a threshold;
determine camera motion based on the parameters for the non-planar motion model; and
notify a driver or automated driving system of a presence of the one or more objects having the height above the threshold. 7. The system of claim 6, wherein the vehicle controller is further configured to identify one or more feature points as part of a same object based on one or more of a pixel intensity or two-dimensional location within the first frame or second frame. 8. The system of claim 6, wherein the vehicle controller is further configured to determine a scale for location of the feature points in three-dimensional space based on feature points within a predefined ground area in the first frame and the second frame. 9. The system of claim 6, wherein the vehicle controller is further configured to classify features as inliers or outliers, wherein determining the parameters comprises determining based on the inliers. 10. The method of claim 6, wherein the vehicle controller is further configured to perform local bundle adjustment on image features for improved accuracy. 11. Non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
identify image features in a first frame corresponding to features in a second frame, wherein the first frame and the second frame comprise adjacent image frames captured by a rear-facing camera of a vehicle;
determine parameters for a non-planar motion model based on the image features;
generate a three-dimensional reconstruction of feature points of the image features based on the non-planar motion model;
determine a height of one or more objects based on corresponding feature points of the feature points;
determine a distance to the one or more objects having a height above a threshold;
determine camera motion based on the parameters for the non-planar motion model; and
notify a driver or automated driving system of a presence of the one or more objects having the height above the threshold. 11. Non-transitory computer readable storage media storing instructions that, when executed by one or more processors, cause the one or more processors to:
identify image features in a first frame corresponding to features in a second frame, wherein the first frame and the second frame comprise adjacent image frames captured by a rear-facing camera of a vehicle;
determine parameters for a non-planar motion model based on the image features;
generate a three-dimensional reconstruction of feature points of the image features based on the non-planar motion model;
determine a height of one or more objects based on corresponding feature points of the feature points;
determine a distance to the one or more objects having a height above a threshold;
determine camera motion based on the parameters for the non-planar motion model; and
notify a driver or automated driving system of a presence of the one or more objects having the height above the threshold. 12. The non-transitory computer readable storage media of claim 11, wherein the instructions further cause the one or more processors to identify one or more feature points as part of a same object based on one or more of a pixel intensity or two-dimensional location within the first frame or second frame. 13. The non-transitory computer readable storage media of claim 11, wherein the instructions further cause the one or more processors to determine a scale for location of the feature points in three-dimensional space based on feature points within a predefined ground area in the first frame and the second frame. 14. The non-transitory computer readable storage media of claim 11, wherein the instructions further cause the one or more processors to classify features as inliers and outliers, wherein determining the parameters comprises determining based on the inliers.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318829B2,US10318829B2,"Passenger counting device, system, method and program, and vehicle movement amount calculation device, method and program",2016-03-17,"method, direction, amount, calculating, determination, passenger, device, detecting, acquiring, face, distance, based, passengers, program, means, faces, plurality, photographing, counting, image, vehicle, number, distances, includes, movement, depth, determining, detected, calculation, system, from","The passenger counting system includes a photographing means  20  for photographing a vehicle and acquiring an image, and a passenger counting device  200 . A passenger counting device  200  includes: a movement amount calculation means  21  for calculating a movement amount of a vehicle based on an image of the vehicle; a depth distance calculation means  22  for calculating a distance in a depth direction of a face of a passenger of the vehicle based on the movement amount of the vehicle; and a passenger number determination means  23  for detecting the face of the passenger of the vehicle from the image and determining the number of passengers of the vehicle based on distances in the depth direction of a plurality of detected faces of passengers.","1. A passenger counting device comprising:
a movement amount calculation unit, implemented by a processor, for calculating a movement amount of a vehicle based on an image of the vehicle;
a depth distance calculation unit, implemented by the processor, for calculating a distance in a depth direction of a face of a passenger of the vehicle based on the movement amount of the vehicle; and
a passenger number determination unit, implemented by the processor, for detecting the face of the passenger of the vehicle from the image and determining a number of passengers of the vehicle based on distances in the depth direction of a plurality of detected faces of passengers. 1. A passenger counting device comprising:
a movement amount calculation unit, implemented by a processor, for calculating a movement amount of a vehicle based on an image of the vehicle;
a depth distance calculation unit, implemented by the processor, for calculating a distance in a depth direction of a face of a passenger of the vehicle based on the movement amount of the vehicle; and
a passenger number determination unit, implemented by the processor, for detecting the face of the passenger of the vehicle from the image and determining a number of passengers of the vehicle based on distances in the depth direction of a plurality of detected faces of passengers. 2. The passenger counting device according to claim 1, wherein
the passenger number determination unit determines presence or absence of erroneous detection based on a distance in the depth direction between a plurality of detected faces of passengers. 3. The passenger counting device according to claim 1, wherein
the movement amount calculation unit estimates an error of the movement amount of the vehicle using a steepest descent method. 4. The passenger counting device according to claim 3, wherein
the movement amount calculation unit measures a distance from a first position to a second position of a face of a specific person for each timing based on images of the vehicle at a plurality of timings and estimates the error of the movement amount of the vehicle when an objective function becomes an extreme value using the steepest descent method by setting a function including a difference between the distance at a first timing and the distance at a second timing as the objective function. 5. The passenger counting device according to claim 1, wherein
the passenger number determination unit determines that a plurality of faces of passengers are faces of different persons when a distance in the depth direction between the plurality of detected faces of passengers is equal to or longer than a first threshold value. 6. The passenger counting device according to claim 1, wherein
the passenger number determination unit determines that a plurality of faces of passengers are faces of different persons when a distance in the depth direction between the plurality of detected faces of passengers is equal to or longer than a first threshold value or when a distance in a traveling direction between the plurality of faces of passengers is equal to or longer than a second threshold value. 7. The passenger counting device according to claim 1, wherein
the depth distance calculation unit calculates a distance in the depth direction from a photographing unit to the face of the passenger based on the calculated movement amount of the vehicle and a direction toward the face of the passenger of the vehicle from a position of the photographing unit that has photographed the vehicle. 8. A passenger counting method comprising:
calculating a movement amount of a vehicle based on an image of the vehicle;
calculating a distance in a depth direction of a face of a passenger of the vehicle based on the movement amount of the vehicle; and
detecting the face of the passenger of the vehicle from the image and determining a number of passengers of the vehicle based on distances in the depth direction of a plurality of detected faces of passengers. 8. A passenger counting method comprising:
calculating a movement amount of a vehicle based on an image of the vehicle;
calculating a distance in a depth direction of a face of a passenger of the vehicle based on the movement amount of the vehicle; and
detecting the face of the passenger of the vehicle from the image and determining a number of passengers of the vehicle based on distances in the depth direction of a plurality of detected faces of passengers. 9. The passenger counting method according to claim 8, wherein
presence or absence of erroneous detection is determined based on a distance in the depth direction between a plurality of detected faces of passengers. 10. The passenger counting method according to claim 8, wherein
an error of the movement amount of the vehicle is estimated using a steepest descent method. 11. The passenger counting method according to claim 10, wherein
a distance from a first position to a second position of a face of a specific person is measured for each timing based on images of the vehicle at a plurality of timings, and the error of the movement amount of the vehicle when an objective function becomes an extreme value is estimated using the steepest descent method by setting a function including a difference between the distance at a first timing and the distance at a second timing as the objective function. 12. The passenger counting method according to claim 8, wherein
when a distance in the depth direction between a plurality of detected faces of passengers is equal to or longer than a first threshold value, it is determined that the plurality of faces of passengers are faces of different persons. 13. The passenger counting method according to claim 8, wherein
when a distance in the depth direction between a plurality of detected faces of passengers is equal to or longer than a first threshold value or when a distance in a traveling direction between the plurality of faces of passengers is equal to or longer than a second threshold value, it is determined that the plurality of faces of passengers are faces of different persons. 14. The passenger counting method according to claim 8, wherein
a distance in the depth direction from a photographing unit to the face of the passenger is calculated based on the calculated movement amount of the vehicle and a direction toward the face of the passenger of the vehicle from a position of the photographing unit that has photographed the vehicle. 15. A non-transitory computer-readable recording medium recording a passenger counting program configured to cause a computer to execute:
a movement amount calculation process of calculating a movement amount of a vehicle based on an image of the vehicle;
a depth distance calculation process of calculating a distance in a depth direction of a face of a passenger of the vehicle based on the movement amount of the vehicle; and
a passenger number determination process of detecting the face of the passenger of the vehicle from the image and determining a number of passengers of the vehicle based on distances in the depth direction of a plurality of detected faces of passengers. 15. A non-transitory computer-readable recording medium recording a passenger counting program configured to cause a computer to execute:
a movement amount calculation process of calculating a movement amount of a vehicle based on an image of the vehicle;
a depth distance calculation process of calculating a distance in a depth direction of a face of a passenger of the vehicle based on the movement amount of the vehicle; and
a passenger number determination process of detecting the face of the passenger of the vehicle from the image and determining a number of passengers of the vehicle based on distances in the depth direction of a plurality of detected faces of passengers. 16. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to determine presence or absence of erroneous detection based on a distance in the depth direction between a plurality of detected faces of passengers in the passenger number determination process. 17. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to estimate an error of the movement amount of the vehicle using a steepest descent method in the movement amount calculation process. 18. The non-transitory computer-readable recording medium according to claim 17, wherein
the passenger counting program causes the computer to measure a distance from a first position to a second position of a face of a specific person for each timing based on images of the vehicle at a plurality of timings, and to estimate the error of the movement amount of the vehicle when an objective function becomes an extreme value using the steepest descent method by setting a function including a difference between the distance at a first timing and the distance at a second timing as the objective function in the movement amount calculation process. 19. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to determine that a plurality of faces of passengers are faces of different persons when a distance in the depth direction between the plurality of detected faces of passengers is equal to or longer than a first threshold value in the passenger number determination process. 20. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to determine that a plurality of faces of passengers are faces of different persons when a distance in the depth direction between the plurality of detected faces of passengers is equal to or longer than a first threshold value or when a distance in a traveling direction between the plurality of faces of passengers is equal to or longer than a second threshold value in the passenger number determination process.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318828B2,US10318828B2,Vehicle behavior analysis,2013-12-19,"monitor, near, camera, identifying, device, instructions, computing, including, within, interface, identify, view, transmit, field, communicatively, travelling, potentially, vehicle, external, storing, analysis, includes, include, vehicles, potential, some, sensor, also, mounted, processor, storage, neighboring, information, embodiments, server, erratic, disclosed, subsystem, extra, coupleable, system, executable, behavior","Embodiments are disclosed for identifying potentially erratic vehicles travelling near a vehicle including an in-vehicle computing system. In some embodiments, an in-vehicle computing system for a vehicle includes a processor, a sensor subsystem communicatively coupleable to a camera mounted on the vehicle, and an external device interface communicatively coupleable to an extra-vehicle server. The in-vehicle computing system may also include a storage device storing instructions executable by the processor to monitor neighboring vehicles within a field of view of the camera, identify a potential erratic vehicle, and transmit vehicle information to the extra-vehicle server.","1. An in-vehicle computing system for a vehicle, comprising:
a processor;
a sensor subsystem communicatively coupleable to a camera mounted on the vehicle;
an external device interface communicatively coupleable to an extra-vehicle device located externally from each of the vehicle, the sensor subsystem, and the camera; and
a storage device storing instructions executable by the processor to:
monitor, with the sensor subsystem, neighboring vehicles within a field of view of the camera;
image potential erratic behavior of an imaged vehicle via the camera;
identify, with the processor, the imaged vehicle as a potential erratic vehicle based on the imaged potential erratic behavior, wherein identifying the potential erratic vehicle includes identifying, with the processor using information from the sensor subsystem, a behavior of each of the neighboring vehicles and comparing, with the processor, the behavior of each of the neighboring vehicles to the potential erratic behavior of the potential erratic vehicle; and
automatically transmit, via the external device interface, vehicle information from the in-vehicle computing system for the vehicle to the extra-vehicle device located externally from the vehicle, the vehicle information indicating the imaged potential erratic behavior of the potential erratic vehicle. 1. An in-vehicle computing system for a vehicle, comprising:
a processor;
a sensor subsystem communicatively coupleable to a camera mounted on the vehicle;
an external device interface communicatively coupleable to an extra-vehicle device located externally from each of the vehicle, the sensor subsystem, and the camera; and
a storage device storing instructions executable by the processor to:
monitor, with the sensor subsystem, neighboring vehicles within a field of view of the camera;
image potential erratic behavior of an imaged vehicle via the camera;
identify, with the processor, the imaged vehicle as a potential erratic vehicle based on the imaged potential erratic behavior, wherein identifying the potential erratic vehicle includes identifying, with the processor using information from the sensor subsystem, a behavior of each of the neighboring vehicles and comparing, with the processor, the behavior of each of the neighboring vehicles to the potential erratic behavior of the potential erratic vehicle; and
automatically transmit, via the external device interface, vehicle information from the in-vehicle computing system for the vehicle to the extra-vehicle device located externally from the vehicle, the vehicle information indicating the imaged potential erratic behavior of the potential erratic vehicle. 2. The in-vehicle computing system of claim 1, wherein identifying the potential erratic vehicle includes determining one or more driving characteristics of each neighboring vehicle including speed, erratic acceleration, frequent braking, and driving patterns of each neighboring vehicle, and then automatically transmitting, without distracting a driver of the vehicle, the one or more driving characteristics to the extra-vehicle device in coordination with an identifier of the potential erratic vehicle. 3. The in-vehicle computing system of claim 1, wherein identifying the potential erratic vehicle further includes sending image data from the camera to an external device located remotely from the in-vehicle computing system, the external device running an application configured to analyze the image data and identify the potential erratic vehicle based on the image data. 4. The in-vehicle computing system of claim 1, wherein the extra-vehicle device is an extra-vehicle server, and wherein the instructions are further executable by the processor to identify a type of roadway on which the potential erratic vehicle is traversing and a type of indicator of the potential erratic vehicle, and transmit the identified type of roadway and the type of indicator to the extra-vehicle server in association with a vehicle identifier for the potential erratic vehicle, then receive an alert confirming that the potential erratic vehicle is verified as an erratic vehicle responsive to the extra-vehicle server comparing the vehicle identifier with vehicle identifiers received from one or more other observing vehicles and determining that a threshold number of observing vehicles identified the potential erratic vehicle, the threshold number of observing vehicles being based at least on the type of roadway on which the observing vehicles are traversing when observing the potential erratic vehicle. 5. The in-vehicle computing system of claim 1, wherein the extra-vehicle device is a mobile device, and wherein the instructions are further executable by the processor to receive an alert identifying an erratic vehicle identified by another neighboring vehicle. 6. The in-vehicle computing system of claim 1, wherein the camera includes a camera system comprising a rear-facing camera and at least one additional camera, and wherein the instructions are executable by the processor to identify the imaged vehicle as the potential erratic vehicle based on observing a threshold number of indicators of the imaged potential erratic behavior, wherein each of the indicators of the imaged potential erratic behavior is weighted relative to other indicators. 7. The in-vehicle computing system of claim 1, further comprising a Controller Area Network (CAN) stack communicatively connected to an electronic control unit for the camera via a CAN bus of the vehicle, the CAN stack configured to provide instructions to control operation of the camera, wherein at least the processor and the external device interface form an integrated head unit installed in an instrument panel of a cabin of the vehicle. 8. The in-vehicle computing system of claim 1, wherein the vehicle information includes, in addition to the vehicle information indicating the imaged potential erratic behavior, a vehicle identifier for the potential erratic vehicle and/or a vehicle status or condition of the potential erratic vehicle. 9. The in-vehicle computing system of claim 8, wherein the vehicle identifier includes one or more of a plate number, a make of the potential erratic vehicle, a model of the potential erratic vehicle, and a color of the potential erratic vehicle, and wherein the vehicle condition of the potential erratic vehicle includes one or more of a tire puncture, an incompletely closed door, a shifting vehicle load, and a non-functioning light. 10. The in-vehicle computing system of claim 8, wherein the vehicle status includes one or more of a location and speed of the potential erratic vehicle, wherein identifying the potential erratic vehicle further comprises identifying the potential erratic vehicle as being potentially erratic based at least on the vehicle status, and wherein transmitting the vehicle information to the extra-vehicle device comprises transmitting the vehicle status to the extra-vehicle device. 11. An in-vehicle computing system for a vehicle, comprising:
a processor;
a sensor subsystem communicatively coupleable to a camera mounted on the vehicle;
an external device interface communicatively coupleable to an extra-vehicle server that is separate from the vehicle and located externally from each of the vehicle, the sensor subsystem, and the camera; and
a storage device storing instructions executable by the processor to:
for each neighboring vehicle within a field of view of the camera, control the camera, via the sensor subsystem, to capture at least one frame of image data including a license plate of the neighboring vehicle;
control the camera, via the sensor subsystem, to capture a stream of image data indicating one or more driving behaviors exhibited by each neighboring vehicle within the field of view of the camera;
compare, with the processor, the one or more driving behaviors to driving behaviors exhibited by the vehicle and/or one or more other neighboring vehicles;
identify, with the processor, a potential erratic vehicle of the neighboring vehicles based on the stream of image data and the one or more driving behaviors;
automatically transmit, via the external device interface, vehicle information corresponding to the potential erratic vehicle to the extra-vehicle server, the vehicle information including an identifier presented on the license plate of the potential erratic vehicle and the one or more driving behaviors exhibited by the potential erratic vehicle;
receive, from the extra-vehicle server via the external device interface, an alert identifying the potential erratic vehicle as a confirmed erratic vehicle responsive to the extra-vehicle server determining that a threshold number of observing vehicles identified the potential erratic vehicle based on the one or more driving behaviors of the potential erratic vehicle; and
present, via an output device, the alert identifying the potential erratic vehicle as the confirmed erratic vehicle to a driver. 11. An in-vehicle computing system for a vehicle, comprising:
a processor;
a sensor subsystem communicatively coupleable to a camera mounted on the vehicle;
an external device interface communicatively coupleable to an extra-vehicle server that is separate from the vehicle and located externally from each of the vehicle, the sensor subsystem, and the camera; and
a storage device storing instructions executable by the processor to:
for each neighboring vehicle within a field of view of the camera, control the camera, via the sensor subsystem, to capture at least one frame of image data including a license plate of the neighboring vehicle;
control the camera, via the sensor subsystem, to capture a stream of image data indicating one or more driving behaviors exhibited by each neighboring vehicle within the field of view of the camera;
compare, with the processor, the one or more driving behaviors to driving behaviors exhibited by the vehicle and/or one or more other neighboring vehicles;
identify, with the processor, a potential erratic vehicle of the neighboring vehicles based on the stream of image data and the one or more driving behaviors;
automatically transmit, via the external device interface, vehicle information corresponding to the potential erratic vehicle to the extra-vehicle server, the vehicle information including an identifier presented on the license plate of the potential erratic vehicle and the one or more driving behaviors exhibited by the potential erratic vehicle;
receive, from the extra-vehicle server via the external device interface, an alert identifying the potential erratic vehicle as a confirmed erratic vehicle responsive to the extra-vehicle server determining that a threshold number of observing vehicles identified the potential erratic vehicle based on the one or more driving behaviors of the potential erratic vehicle; and
present, via an output device, the alert identifying the potential erratic vehicle as the confirmed erratic vehicle to a driver. 12. The in-vehicle computing system of claim 11, wherein the output device comprises a display of the vehicle and/or one or more speakers of the vehicle, the in-vehicle computing system further comprising a frame grabber communicatively connected to the camera and configured to receive the stream of image data from the camera and separate the stream of image data into individual frames. 13. The in-vehicle computing system of claim 12, further comprising a video analytics module configured to analyze the individual frames from the frame grabber to determine the one or more driving behaviors, wherein the one or more driving behaviors includes frequent lane changes, and identifying the potential erratic vehicle further comprises determining that a number of times the potential erratic vehicle changed lanes exceeded a threshold above an average number of times each of the neighboring vehicles changed lanes. 14. A method of identifying a potential erratic vehicle within a field of view of a camera of a vehicle, the method comprising:
receiving, at an extra-vehicle server external to the vehicle, vehicle information for a potential erratic vehicle, the vehicle information including an indication of an erratic behavior of the potential erratic vehicle as identified by at least the vehicle and a type of a roadway on which the potential erratic vehicle is traversing;
determining, with the extra-vehicle server, a number of observing vehicles from which vehicle information for the potential erratic vehicle is received; and
responsive to determining that the number of observing vehicles is above a threshold number of observing vehicles, generating and transmitting an alert to one or more vehicles in a geographical vicinity of the potential erratic vehicle confirming that the potential erratic vehicle is verified as an erratic vehicle. 14. A method of identifying a potential erratic vehicle within a field of view of a camera of a vehicle, the method comprising:
receiving, at an extra-vehicle server external to the vehicle, vehicle information for a potential erratic vehicle, the vehicle information including an indication of an erratic behavior of the potential erratic vehicle as identified by at least the vehicle and a type of a roadway on which the potential erratic vehicle is traversing;
determining, with the extra-vehicle server, a number of observing vehicles from which vehicle information for the potential erratic vehicle is received; and
responsive to determining that the number of observing vehicles is above a threshold number of observing vehicles, generating and transmitting an alert to one or more vehicles in a geographical vicinity of the potential erratic vehicle confirming that the potential erratic vehicle is verified as an erratic vehicle. 15. The method of claim 14, wherein the alert from the extra-vehicle server includes a vehicle identifier and a vehicle status of the erratic vehicle, and wherein the one or more vehicles in the geographical vicinity of the potential erratic vehicle are determined to be within a threshold distance of a last known location of the verified erratic vehicle, the threshold distance being based on a determined speed of the verified erratic vehicle. 16. A method of identifying a potential erratic vehicle within a field of view of a camera of a vehicle, the method comprising:
receiving, at an extra-vehicle server external to the vehicle, vehicle information for a potential erratic vehicle, the vehicle information including an indication of an erratic behavior exhibited by the potential erratic vehicle as identified by at least the vehicle;
determining, with the extra-vehicle server, a number of observing vehicles from which vehicle information for the potential erratic vehicle is received; and
responsive to determining that the number of observing vehicles is above a threshold number of observing vehicles, generating and transmitting an alert to one or more vehicles in a geographical vicinity of the potential erratic vehicle confirming that the potential erratic vehicle is verified as an erratic vehicle;
the method further comprising determining, with the extra-vehicle server, a number of erratic behaviors identified for the potential erratic vehicle, and wherein the alert is transmitted to the one or more vehicles responsive to determining that a threshold number of erratic behaviors is identified by the vehicle. 16. A method of identifying a potential erratic vehicle within a field of view of a camera of a vehicle, the method comprising:
receiving, at an extra-vehicle server external to the vehicle, vehicle information for a potential erratic vehicle, the vehicle information including an indication of an erratic behavior exhibited by the potential erratic vehicle as identified by at least the vehicle;
determining, with the extra-vehicle server, a number of observing vehicles from which vehicle information for the potential erratic vehicle is received; and
responsive to determining that the number of observing vehicles is above a threshold number of observing vehicles, generating and transmitting an alert to one or more vehicles in a geographical vicinity of the potential erratic vehicle confirming that the potential erratic vehicle is verified as an erratic vehicle;
the method further comprising determining, with the extra-vehicle server, a number of erratic behaviors identified for the potential erratic vehicle, and wherein the alert is transmitted to the one or more vehicles responsive to determining that a threshold number of erratic behaviors is identified by the vehicle. 17. The method of claim 16, wherein the threshold number of erratic behaviors is based on one or more of a vehicle operating condition and an environmental condition. 18. The method of claim 14, wherein the threshold number of observing vehicles is based on one or more of the type of the roadway that the observing vehicles are traversing and a traffic condition on the roadway. 19. The in-vehicle computing system of claim 1, wherein monitoring neighboring vehicles within the field of view of the camera includes capturing image data including a number plate of each neighboring vehicle within the field of view of the camera to identify each neighboring vehicle and, for each identified neighboring vehicle, capturing a stream of image data imaging the identified neighboring vehicle over time. 20. The in-vehicle computing system of claim 1, wherein identifying the potential erratic vehicle includes determining that the image data provides evidence that the potential erratic vehicle is erratically overtaking one or more other vehicles. 21. The in-vehicle computing system of claim 13, further comprising an event generator configured to generate a signal indicating a presence of the potential erratic vehicle and/or associated details based on data received at the event generator from the video analytics module.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318827B2,US10318827B2,Object detection neural networks,2016-12-19,"precision, region, reading, camera, networks, generated, likelihood, environment, including, features, feature, input, belonging, systems, characterizing, implementations, object, laser, score, patch, computer, each, neural, high, image, respective, methods, apparatus, using, network, includes, that, obtained, some, sensor, storage, detection, more, processed, encoded, media, generate, category, represents, projected, vector, generating, from, dimensional, predictions, three, categories, first, located, programs","Methods, systems, and apparatus, including computer programs encoded on computer storage media, for generating object detection predictions from a neural network. In some implementations, an input characterizing a first region of an environment is obtained. The input includes a projected laser image generated from a three-dimensional laser sensor reading of the first region, a camera image patch generated from a camera image of the first region, and a feature vector of features characterizing the first region. The input is processed using a high precision object detection neural network to generate a respective object score for each object category in a first set of one or more object categories. Each object score represents a respective likelihood that an object belonging to the object category is located in the first region of the environment.","1. A method comprising:
processing, by one or more computers, a camera image collected by one or more cameras of a vehicle using a first high-recall object detection neural network, wherein the camera image depicts an environment in a vicinity of the vehicle, and wherein the first high-recall object detection neural network is configured to:
receive the camera image; and
process the camera image to generate:
(i) data defining a plurality of bounding boxes in the camera image, each bounding box showing a different region of the environment; and
(ii) for each of the plurality of bounding boxes, a respective first confidence score that represents a probability that an object belonging to an object category from a second set of one or more object categories is present in the region of the environment that shows the bounding box;


determining, based at least on the respective first confidence scores, that additional processing is required to determine a higher-precision confidence score for a first region of the environment shown by a first bounding box of the plurality of bounding boxes;
in response to determining that additional processing is required:
obtaining, by the one or more computers, an input characterizing the first region of an environment, the input comprising:
(i) a projected laser image collected by one or more laser sensors of the vehicle and generated from a three-dimensional laser sensor reading of the first region of the environment;
(ii) a camera image patch generated from the camera image of the environment and corresponding to the first bounding box for the first region; and
(iii) a feature vector of features characterizing the first region; and

processing, by the one or more computers, the input using a high precision object detection neural network to generate a respective object score for each object category in a first set of one or more object categories, wherein each object score represents a respective probability that an object belonging to the object category is located in the first region of the environment, and wherein processing the input comprises:
processing the projected laser image through a laser sub-neural network to generate an alternative representation of the projected laser image;
processing the camera image patch through a camera sub-neural network to generate an alternative representation of the camera image patch;
processing the feature vector through a feature sub-neural network to generate an alternative representation of the feature vector; and

processing, by the one or more computers, the alternative representation of the projected laser image, the alternative representation of the camera image patch, and the alternative representation of the feature vector through a combining sub-neural network to generate the respective object score for each of the one or more object categories. 1. A method comprising:
processing, by one or more computers, a camera image collected by one or more cameras of a vehicle using a first high-recall object detection neural network, wherein the camera image depicts an environment in a vicinity of the vehicle, and wherein the first high-recall object detection neural network is configured to:
receive the camera image; and
process the camera image to generate:
(i) data defining a plurality of bounding boxes in the camera image, each bounding box showing a different region of the environment; and
(ii) for each of the plurality of bounding boxes, a respective first confidence score that represents a probability that an object belonging to an object category from a second set of one or more object categories is present in the region of the environment that shows the bounding box;


determining, based at least on the respective first confidence scores, that additional processing is required to determine a higher-precision confidence score for a first region of the environment shown by a first bounding box of the plurality of bounding boxes;
in response to determining that additional processing is required:
obtaining, by the one or more computers, an input characterizing the first region of an environment, the input comprising:
(i) a projected laser image collected by one or more laser sensors of the vehicle and generated from a three-dimensional laser sensor reading of the first region of the environment;
(ii) a camera image patch generated from the camera image of the environment and corresponding to the first bounding box for the first region; and
(iii) a feature vector of features characterizing the first region; and

processing, by the one or more computers, the input using a high precision object detection neural network to generate a respective object score for each object category in a first set of one or more object categories, wherein each object score represents a respective probability that an object belonging to the object category is located in the first region of the environment, and wherein processing the input comprises:
processing the projected laser image through a laser sub-neural network to generate an alternative representation of the projected laser image;
processing the camera image patch through a camera sub-neural network to generate an alternative representation of the camera image patch;
processing the feature vector through a feature sub-neural network to generate an alternative representation of the feature vector; and

processing, by the one or more computers, the alternative representation of the projected laser image, the alternative representation of the camera image patch, and the alternative representation of the feature vector through a combining sub-neural network to generate the respective object score for each of the one or more object categories. 2. The method of claim 1, wherein the environment is an environment in proximity of an autonomous vehicle, and wherein the three-dimensional laser sensor reading and the camera image are captured by sensors of the autonomous vehicle. 3. The method of claim 1, wherein the combining sub-neural network is configured to:
process the alternative representation of the laser projected image and the alternative representation of the camera image patch through one or more visual combining neural network layers to generate a visual combined representation;
process the visual combined representation and the alternative representation of the feature vector through one or more final combining layers to generate a final combined representation; and
process the final combined representation through an output neural network layer to generate the object scores. 4. The method of claim 1, further comprising:
generating a plurality of camera image patches from the obtained camera image of the environment;
determining a respective size value for each of the plurality of camera image patches;
adjusting the respective size values for each of the plurality of camera image patches to generate a plurality of range-normalized camera image patches, wherein the values of the respective sizes for each of the plurality of range-normalized camera image patches are substantially similar; and
processing the plurality of range-normalized camera image patches using the first high-recall object detection neural network. 5. The method of claim 1, further comprising:
obtaining a projected laser image of the environment;
processing the projected laser image using a second high-recall object detection neural network, wherein the second high-recall object detection neural network is configured to:
receive the projected laser image; and
process the projected laser image to generate:
(i) data defining a plurality of bounding boxes in the projected laser image; and
(ii) for each of the plurality of bounding boxes, a respective second confidence score that represents a probability that an object belonging to an object category from the second set of one or more object categories is present in the region of the environment shown in the bounding box. 6. The method of claim 5, further comprising:
obtaining respective top-down projections of a plurality of channels of the input characterizing the first region of the environment, each top-down projection representing a projection of an area surrounding the autonomous vehicle; and
combining the respective top-down projections to generate the projected laser image. 7. The method of claim 1, further comprising:
determining, from the first confidence scores and the second confidence scores, that additional processing is required to determine a higher-precision confidence score for the first region, and in response, obtaining the first input characterizing the first region and processing the first input using the high precision object detection neural network. 8. The method of claim 1, wherein the projected laser image is generated from a three-dimensional laser sensor reading collected by a long-range laser sensor. 9. The method of claim 1, wherein the projected laser image is generated from a three-dimensional laser sensor reading collected by a short-range laser sensor. 10. The method of claim 1, wherein the one or more object categories comprise at least a pedestrian category and a cyclist category. 11. The method of claim 1, wherein:
the obtained input characterizing the first region of the environment comprises a multiplexed signal, and
processing the input using a high precision object detection neural network comprises processing each subcomponent of the obtained input to generate a respective object score for a particular object category, wherein each subcomponent of the obtained input corresponds to a different object category. 12. A system comprising:
one or more computers; and
a non-transitory computer-readable medium coupled to the one or more computers having instructions stored thereon, which, when executed by the one or more computers, cause the one or more computers to perform operations comprising:
processing, by one or more computers, a camera image collected by one or more cameras of a vehicle using a first high-recall object detection neural network, wherein the camera image depicts an environment in a vicinity of the vehicle, and wherein the first high-recall object detection neural network is configured to:
receive the camera image; and
process the camera image to generate:
(i) data defining a plurality of bounding boxes in the camera image, each bounding box showing a different region of the environment; and
(ii) for each of the plurality of bounding boxes, a respective first confidence score that represents a probability that an object belonging to an object category from a second set of one or more object categories is present in the region of the environment that shows the bounding box;


determining, based at least on the respective first confidence scores, that additional processing is required to determine a higher-precision confidence score for a first region of the environment shown by a first bounding box of the plurality of bounding boxes;
in response to determining that additional processing is required:
obtaining, by the one or more computers, an input characterizing the first region of an environment, the input comprising:
(i) a projected laser image collected by one or more laser sensors of the vehicle and generated from a three-dimensional laser sensor reading of the first region of the environment;
(ii) a camera image patch generated from the camera image of the environment and corresponding to the first bounding box for the first region; and
(iii) a feature vector of features characterizing the first region; and

processing, by the one or more computers, the input using a high precision object detection neural network to generate a respective object score for each object category in a first set of one or more object categories, wherein each object score represents a respective probability that an object belonging to the object category is located in the first region of the environment, and wherein processing the input comprises:
processing the projected laser image through a laser sub-neural network to generate an alternative representation of the projected laser image;
processing the camera image patch through a camera sub-neural network to generate an alternative representation of the camera image patch;
processing the feature vector through a feature sub-neural network to generate an alternative representation of the feature vector; and

processing, by the one or more computers, the alternative representation of the projected laser image, the alternative representation of the camera image patch, and the alternative representation of the feature vector through a combining sub-neural network to generate the respective object score for each of the one or more object categories. 12. A system comprising:
one or more computers; and
a non-transitory computer-readable medium coupled to the one or more computers having instructions stored thereon, which, when executed by the one or more computers, cause the one or more computers to perform operations comprising:
processing, by one or more computers, a camera image collected by one or more cameras of a vehicle using a first high-recall object detection neural network, wherein the camera image depicts an environment in a vicinity of the vehicle, and wherein the first high-recall object detection neural network is configured to:
receive the camera image; and
process the camera image to generate:
(i) data defining a plurality of bounding boxes in the camera image, each bounding box showing a different region of the environment; and
(ii) for each of the plurality of bounding boxes, a respective first confidence score that represents a probability that an object belonging to an object category from a second set of one or more object categories is present in the region of the environment that shows the bounding box;


determining, based at least on the respective first confidence scores, that additional processing is required to determine a higher-precision confidence score for a first region of the environment shown by a first bounding box of the plurality of bounding boxes;
in response to determining that additional processing is required:
obtaining, by the one or more computers, an input characterizing the first region of an environment, the input comprising:
(i) a projected laser image collected by one or more laser sensors of the vehicle and generated from a three-dimensional laser sensor reading of the first region of the environment;
(ii) a camera image patch generated from the camera image of the environment and corresponding to the first bounding box for the first region; and
(iii) a feature vector of features characterizing the first region; and

processing, by the one or more computers, the input using a high precision object detection neural network to generate a respective object score for each object category in a first set of one or more object categories, wherein each object score represents a respective probability that an object belonging to the object category is located in the first region of the environment, and wherein processing the input comprises:
processing the projected laser image through a laser sub-neural network to generate an alternative representation of the projected laser image;
processing the camera image patch through a camera sub-neural network to generate an alternative representation of the camera image patch;
processing the feature vector through a feature sub-neural network to generate an alternative representation of the feature vector; and

processing, by the one or more computers, the alternative representation of the projected laser image, the alternative representation of the camera image patch, and the alternative representation of the feature vector through a combining sub-neural network to generate the respective object score for each of the one or more object categories. 13. The system of claim 12, wherein the environment is an environment in proximity of an autonomous vehicle, and wherein the three-dimensional laser sensor reading and the camera image are captured by sensors of the autonomous vehicle. 14. The system of claim 12, wherein the combining sub-neural network is configured to:
process the alternative representation of the laser projected image and the alternative representation of the camera image patch through one or more visual combining neural network layers to generate a visual combined representation;
process the visual combined representation and the alternative representation of the feature vector through one or more final combining layers to generate a final combined representation; and
process the final combined representation through an output neural network layer to generate the object scores. 15. A non-transitory computer storage device encoded with a computer program, the program comprising instructions that when executed by one or more computers cause the one or more computers to perform operations comprising:
processing, by one or more computers, a camera image collected by one or more cameras of a vehicle using a first high-recall object detection neural network, wherein the camera image depicts an environment in a vicinity of the vehicle, and wherein the first high-recall object detection neural network is configured to:
receive the camera image; and
process the camera image to generate:
(i) data defining a plurality of bounding boxes in the camera image, each bounding box showing a different region of the environment; and
(ii) for each of the plurality of bounding boxes, a respective first confidence score that represents a probability that an object belonging to an object category from a second set of one or more object categories is present in the region of the environment that shows the bounding box;


determining, based at least on the respective first confidence scores, that additional processing is required to determine a higher-precision confidence score for a first region of the environment shown by a first bounding box of the plurality of bounding boxes;
in response to determining that additional processing is required:
obtaining, by the one or more computers, an input characterizing the first region of an environment, the input comprising:
(i) a projected laser image collected by one or more laser sensors of the vehicle and generated from a three-dimensional laser sensor reading of the first region of the environment;
(ii) a camera image patch generated from the camera image of the environment and corresponding to the first bounding box for the first region; and
(iii) a feature vector of features characterizing the first region; and

processing, by the one or more computers, the input using a high precision object detection neural network to generate a respective object score for each object category in a first set of one or more object categories, wherein each object score represents a respective probability that an object belonging to the object category is located in the first region of the environment, and wherein processing the input comprises:
processing the projected laser image through a laser sub-neural network to generate an alternative representation of the projected laser image;
processing the camera image patch through a camera sub-neural network to generate an alternative representation of the camera image patch;
processing the feature vector through a feature sub-neural network to generate an alternative representation of the feature vector; and

processing, by the one or more computers, the alternative representation of the projected laser image, the alternative representation of the camera image patch, and the alternative representation of the feature vector through a combining sub-neural network to generate the respective object score for each of the one or more object categories. 15. A non-transitory computer storage device encoded with a computer program, the program comprising instructions that when executed by one or more computers cause the one or more computers to perform operations comprising:
processing, by one or more computers, a camera image collected by one or more cameras of a vehicle using a first high-recall object detection neural network, wherein the camera image depicts an environment in a vicinity of the vehicle, and wherein the first high-recall object detection neural network is configured to:
receive the camera image; and
process the camera image to generate:
(i) data defining a plurality of bounding boxes in the camera image, each bounding box showing a different region of the environment; and
(ii) for each of the plurality of bounding boxes, a respective first confidence score that represents a probability that an object belonging to an object category from a second set of one or more object categories is present in the region of the environment that shows the bounding box;


determining, based at least on the respective first confidence scores, that additional processing is required to determine a higher-precision confidence score for a first region of the environment shown by a first bounding box of the plurality of bounding boxes;
in response to determining that additional processing is required:
obtaining, by the one or more computers, an input characterizing the first region of an environment, the input comprising:
(i) a projected laser image collected by one or more laser sensors of the vehicle and generated from a three-dimensional laser sensor reading of the first region of the environment;
(ii) a camera image patch generated from the camera image of the environment and corresponding to the first bounding box for the first region; and
(iii) a feature vector of features characterizing the first region; and

processing, by the one or more computers, the input using a high precision object detection neural network to generate a respective object score for each object category in a first set of one or more object categories, wherein each object score represents a respective probability that an object belonging to the object category is located in the first region of the environment, and wherein processing the input comprises:
processing the projected laser image through a laser sub-neural network to generate an alternative representation of the projected laser image;
processing the camera image patch through a camera sub-neural network to generate an alternative representation of the camera image patch;
processing the feature vector through a feature sub-neural network to generate an alternative representation of the feature vector; and

processing, by the one or more computers, the alternative representation of the projected laser image, the alternative representation of the camera image patch, and the alternative representation of the feature vector through a combining sub-neural network to generate the respective object score for each of the one or more object categories. 16. The non-transitory computer storage device of claim 15, wherein the environment is an environment in proximity of an autonomous vehicle, and wherein the three-dimensional laser sensor reading and the camera image are captured by sensors of the autonomous vehicle. 17. The non-transitory computer storage device of claim 15, wherein the combining sub-neural network is configured to:
process the alternative representation of the laser projected image and the alternative representation of the camera image patch through one or more visual combining neural network layers to generate a visual combined representation;
process the visual combined representation and the alternative representation of the feature vector through one or more final combining layers to generate a final combined representation; and
process the final combined representation through an output neural network layer to generate the object scores.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318830B2,US10318830B2,"Passenger counting device, system, method and program",2016-03-17,"method, second, determination, passenger, separation, device, receiving, light, images, acquiring, only, based, wavelength, program, passengers, means, plurality, photographing, counting, image, state, vehicle, specific, number, includes, band, determining, system, first, separate",A passenger counting system includes: a first photographing means  30  for photographing a vehicle and acquiring a first image; a second photographing means  31  for photographing the vehicle in a state of receiving only light of a first specific wavelength band and acquiring a second image; and a passenger counting device  200 . The passenger counting device  200  includes: an image separation means  42  for acquiring a plurality of separate images based on the first image and the second image; and a passenger number determination means  43  for determining the number of passengers of the vehicle based on the plurality of separate images.,"1. A passenger counting device comprising:
an image separation unit, implemented by a processor, for acquiring a plurality of separate images based on a first image acquired by photographing a vehicle and a second image acquired by photographing the vehicle in a state of receiving light of a first specific wavelength band;
a specifying unit, implemented by a processor, for specifying a separate image in which reflection on the vehicle is reduced as compared to the first image from among the plurality of separate images; and
a passenger number determination unit, implemented by a processor, for determining a number of passengers of the vehicle based on the specified separate image in which the reflection is reduced. 1. A passenger counting device comprising:
an image separation unit, implemented by a processor, for acquiring a plurality of separate images based on a first image acquired by photographing a vehicle and a second image acquired by photographing the vehicle in a state of receiving light of a first specific wavelength band;
a specifying unit, implemented by a processor, for specifying a separate image in which reflection on the vehicle is reduced as compared to the first image from among the plurality of separate images; and
a passenger number determination unit, implemented by a processor, for determining a number of passengers of the vehicle based on the specified separate image in which the reflection is reduced. 2. The passenger counting device according to claim 1, wherein
the passenger number determination unit performs face detection on the separate image in which the reflection is reduced and determines a number of detected faces as the number of passengers of the vehicle. 3. The passenger counting device according to claim 1, wherein
the specifying unit specifies the separate image in which the reflection is reduced according to a number of body parts in each of the plurality of separate images. 4. The passenger counting device according to claim 1, wherein
the passenger number determination unit acquires a number of passengers for each of the plurality of separate images and determines a maximum value of the acquired number of passengers as the number of passengers of the vehicle. 5. The passenger counting device according to claim 1, wherein
the image separation unit performs image separation using independent component analysis. 6. The passenger counting device according to claim 1, wherein
the image separation unit separates image data into two images having different distances from a photographing unit based on the first image and the second image. 7. The passenger counting device according to claim 1, wherein
the image separation unit acquires the plurality of separate images based on the first image, the second image, and a third image acquired by photographing the vehicle in a state of receiving light of a second specific wavelength band different from the first specific wavelength band,
the specifying unit specifies the separate image in which reflection on the vehicle is reduced as compared to the first image from among the plurality of separate images, and
the passenger number determination unit determines the number of passengers of the vehicle based on the specified separate image in which the reflection is reduced. 8. A passenger counting method comprising:
acquiring a plurality of separate images based on a first image acquired by photographing a vehicle and a second image acquired by photographing the vehicle in a state of receiving light of a first specific wavelength band;
specifying a separate image in which reflection on the vehicle is reduced as compared to the first image from among the plurality of separate images; and
determining a number of passengers of the vehicle based on the specified separate image in which the reflection is reduced. 8. A passenger counting method comprising:
acquiring a plurality of separate images based on a first image acquired by photographing a vehicle and a second image acquired by photographing the vehicle in a state of receiving light of a first specific wavelength band;
specifying a separate image in which reflection on the vehicle is reduced as compared to the first image from among the plurality of separate images; and
determining a number of passengers of the vehicle based on the specified separate image in which the reflection is reduced. 9. The passenger counting method according to claim 8, wherein
face detection is performed on the separate image in which the reflection is reduced and a number of detected faces is determined as the number of passengers of the vehicle. 10. The passenger counting method according to claim 8, wherein
the separate image in which the reflection is reduced is specified according to a number of body parts in each of the plurality of separate images. 11. The passenger counting method according to claim 8, wherein
a number of passengers is acquired for each of the plurality of separate images, and a maximum value of the acquired number of passengers is determined as the number of passengers of the vehicle. 12. The passenger counting method according to claim 8, wherein
image separation is performed using independent component analysis. 13. The passenger counting method according to claim 8, wherein
image data is separated into two images having different distances from the photographing unit based on the first image and the second image. 14. The passenger counting method according to claim 8, wherein
the plurality of separate images are acquired based on the first image, the second image, and a third image acquired by photographing the vehicle in a state of receiving light of a second specific wavelength band different from the first specific wavelength band,
a separate image in which reflection on the vehicle is reduced as compared to the first image is specified from among the plurality of separate images, and
the number of passengers of the vehicle is determined based on the specified separate image in which the reflection is reduced. 15. A non-transitory computer-readable recording medium recording a passenger counting program configured to cause a computer to execute:
an image separation process of acquiring a plurality of separate images based on a first image acquired by photographing a vehicle and a second image acquired by photographing the vehicle in a state of receiving light of a first specific wavelength band;
a specifying process of specifying a separate image in which reflection on the vehicle is reduced as compared to the first image from among the plurality of separate images; and
a passenger number determination process of determining a number of passengers of the vehicle based on the specified separate image in which the reflection is reduced. 15. A non-transitory computer-readable recording medium recording a passenger counting program configured to cause a computer to execute:
an image separation process of acquiring a plurality of separate images based on a first image acquired by photographing a vehicle and a second image acquired by photographing the vehicle in a state of receiving light of a first specific wavelength band;
a specifying process of specifying a separate image in which reflection on the vehicle is reduced as compared to the first image from among the plurality of separate images; and
a passenger number determination process of determining a number of passengers of the vehicle based on the specified separate image in which the reflection is reduced. 16. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to perform face detection on the separate image in which the reflection is reduced and to determine a number of detected faces as the number of passengers of the vehicle in the passenger number determination process. 17. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to specify the separate image in which the reflection is reduced according to a number of body parts in each of the plurality of separate images in the specifying process. 18. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to acquire a number of passengers for each of the plurality of separate images and to determine a maximum value of the acquired number of passengers as the number of passengers of the vehicle in the passenger number determination process. 19. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to perform image separation using independent component analysis in the image separation process. 20. The non-transitory computer-readable recording medium according to claim 15, wherein
the passenger counting program causes the computer to separate image data into two images having different distances from the photographing unit based on the first image and the second image in the image separation process.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318833B2,US10318833B2,System and method for person identification and personality assessment based on EEG signal,,"method, invention, properties, this, present, novel, based, assessment, brain, means, form, signature, used, unique, highly, electroencephalography, specific, person, relates, processing, more, inherent, identification, signal, system, recording, personality, particularly","The present invention relates a novel system and method for person identification and personality assessment based on electroencephalography (EEG) signal. More particularly, this invention relates to a novel method of EEG recording and processing to map the inherent and unique properties of brain in the form of highly specific brain signature to be used as means for person identification and personality assessment.","1. An EEG based personality assessment system comprising:
a means for sensing brainwaves of a user;
a means for recording a pattern of the brainwaves of the user;
a means for processing the recorded pattern, the means capable of performing an analysis of the brainwave pattern by giving values for three parameters each for left and right side of the brain, namely, ρ, θ and h, and plotting them with time resulting into a four dimensional visualization;
a means for representing four dimensional visualization of the entire EEG activity of the user's brain obtained by the means for processing, as brain signature; and a means for securely storing the brain signature, wherein, multiple output data resulting from the pattern recorded are taken together and the structure of their correlation and higher-order statistics matrices are analyzed through its eignevalues, eigenvector, eigendirection and eigenspaces and other signal processing techniques like fast Fourier transform compression sensing, wavelet transform to obtain said four dimensional visualization. 1. An EEG based personality assessment system comprising:
a means for sensing brainwaves of a user;
a means for recording a pattern of the brainwaves of the user;
a means for processing the recorded pattern, the means capable of performing an analysis of the brainwave pattern by giving values for three parameters each for left and right side of the brain, namely, ρ, θ and h, and plotting them with time resulting into a four dimensional visualization;
a means for representing four dimensional visualization of the entire EEG activity of the user's brain obtained by the means for processing, as brain signature; and a means for securely storing the brain signature, wherein, multiple output data resulting from the pattern recorded are taken together and the structure of their correlation and higher-order statistics matrices are analyzed through its eignevalues, eigenvector, eigendirection and eigenspaces and other signal processing techniques like fast Fourier transform compression sensing, wavelet transform to obtain said four dimensional visualization. 2. The EEG based system as claimed in claim 1, wherein said means for sensing and recording brainwave pattern comprises an Electroencephalograph (EEG). 3. The EEG based system as claimed in claim 1, wherein the means for sensing and recording brainwave pattern includes spinal EEG and different scalp EEG electrodes. 4. The EEG based system as claimed in claim 1, wherein said means for processing pattern comprises a programmed processing unit. 5. The EEG based system as claimed in claim 1, wherein said brainwaves comprise EEG signals. 6. The EEG based system as claimed in claim 1, wherein said analysis further includes statistical, temporal and spectral property assessment of the EEG signals through four dimensional visualization of eigenvectors, eigenvalues, and properties of inner produce spaces plotted with time. 7. The EEG based system as claimed in claim 1, wherein the analysis further includes the step of creating correlation matrix from EEG signals obtained in a fixed duration of time from the various locations of the scalp. 8. The EEG based system as claimed in claim 1, wherein the means for representing four dimensional visualization of the entire EEG activity of the user's brain is an output device selected from the group comprising a printer, a visual output screen, an audio-visual output device or a combination thereof. 9. The EEG based system as claimed in claim 1, wherein the brain signature is unique to a user and is capable for being used as biometric authentication as well as a personality assessment tool. 10. A method of EEG based authentication and assessment of a user comprising the steps of:
recording EEG signal data to obtain a brainwave pattern;
optionally grouping the signals obtained in the recording step, into at least two groups;
breaking the signal data into frames;
creating covariance matrix of each frame obtain in the breaking step;
computing eignenvectors and eigenvalues;
computing response vector covariance matrix from its eigenvectors and eignevalues;
reducing the covariance matrix to there dimensional spherical coordinate parameters;
plotting the three dimensional parameters as a function of time to obtain four dimensional visualization as unique brain signature of the EEG signals of the recording step;
storing the brain signature for subsequent references; and
authenticating the user on the basis of stored brain signatures. 10. A method of EEG based authentication and assessment of a user comprising the steps of:
recording EEG signal data to obtain a brainwave pattern;
optionally grouping the signals obtained in the recording step, into at least two groups;
breaking the signal data into frames;
creating covariance matrix of each frame obtain in the breaking step;
computing eignenvectors and eigenvalues;
computing response vector covariance matrix from its eigenvectors and eignevalues;
reducing the covariance matrix to there dimensional spherical coordinate parameters;
plotting the three dimensional parameters as a function of time to obtain four dimensional visualization as unique brain signature of the EEG signals of the recording step;
storing the brain signature for subsequent references; and
authenticating the user on the basis of stored brain signatures. 11. The method of EEG based authentication claimed in claim 10, wherein the signal data comprises at least 100 samples of each signal per second. 12. The method of EEG based authentication as claimed in claim 10, wherein each group comprises plurality of signals. 13. The method of EEG based authentication as claimed in claim 11, wherein a frame comprises data consisting of at least 100 samples, of all the signals in each group taken separately. 14. The method of EEC based authentication as claimed in claim 10, wherein covariance matrix is a symmetric matrix constructed with the signals and at least 100 samples for each signal. 15. The method of EEG based authentication as claimed in claim 10, wherein an eigenvector of a matrix is a vector which when multiplied by that matrix results in a scaled version of the original vector itself and the eigenvectors and eigenvalues are computed by equation Axi=AiXi′ where Xi is on of the eignevectors of A and Ai is its corresponding eigenvalue and wherein a n×n symmetric matrix give n eignevectors with corresponding n eigenvalues, n being any arbitrary integer. 16. The method of EEG based authentication is claimed in claim 10, wherein the three spherical coordinates comprises an angle a vector makes with the z-axis represented by ρ, an angle a vector makes with the x-axis represented by θ, and the length of the vector represented by h. 17. The method of EEG based diagnosis as claimed in claim 10, wherein the brain signature is unique to a user and is capable of being used as biometric authentication as well as personality assessment tool.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318831B2,US10318831B2,Method and system for monitoring the status of the driver of a vehicle,2016-07-21,"method, containing, following, principal, facial, deviations, variability, spatial, driver, belong, including, estimation, creation, face, model, thereof, events, identified, patterns, match, temporal, intensity, estimate, classification, identify, status, steps, computer, with, optional, head, image, combination, vehicle, estimates, acquisition, drivers, that, pose, respect, sensor, pixels, comprises, grayscale, depth, identification, information, current, relating, disclosed, relative, neutral, expression, distraction, system, from, monitoring, color, clustering","A system and method are disclosed relating to a computer system that estimates the status of the driver of a vehicle. The system comprises the following steps: Acquisition of an image from a depth sensor containing depth and optional an IR intensity image and an RGB color image; identification of pixels that belong to the drivers head; creation of a 3D model of the head including an intensity model and a variability estimate for depth, grayscale and color information; estimation of the principal head pose and the neutral facial expression; estimation of the current relative head pose with respect to the principal head pose; identification of pixels that do not match the neutral face model with respect to depth, grayscale or color information or any combination thereof; clustering of the pixels with identified deviations; classification of spatial and temporal patterns to identify driver status and distraction events.","1. A method for real-time driver state monitoring comprising:
operating an image processing system to receive a sequence of depth images from a depth sensor, each depth image comprising image data representing an image of a scene, wherein said image data comprises a depth value indicating distance from the depth sensor;
the image processing system identifying a human head in the scene using the sequence of images;
the image processing system using the depth images to track position and orientation of the human head with respect to a principal position and orientation where the human head is modeled as a 3D surface to define a head model and continuously adapted over time to obtain an average neutral face model where the neutral face is stored in a database including history of changes;
the image processing system identifying depth deviations from the neutral face; and
the image processing system classifying such depth deviations as different driver states, where depth variations are compared to the history of changes that has been stored, for a specific driver, in the database and where the comparison is used to extract features for driver-state detection. 1. A method for real-time driver state monitoring comprising:
operating an image processing system to receive a sequence of depth images from a depth sensor, each depth image comprising image data representing an image of a scene, wherein said image data comprises a depth value indicating distance from the depth sensor;
the image processing system identifying a human head in the scene using the sequence of images;
the image processing system using the depth images to track position and orientation of the human head with respect to a principal position and orientation where the human head is modeled as a 3D surface to define a head model and continuously adapted over time to obtain an average neutral face model where the neutral face is stored in a database including history of changes;
the image processing system identifying depth deviations from the neutral face; and
the image processing system classifying such depth deviations as different driver states, where depth variations are compared to the history of changes that has been stored, for a specific driver, in the database and where the comparison is used to extract features for driver-state detection. 2. The method as claimed in claim 1 where supplementary images comprising gray-scale or color images are obtained from the depth sensor or an additional sensor that has been pixel-wise registered to the depth image. 3. The method as claimed in claim 1 where all but head pixels are excluded according to a CAD model of an interior of a car. 4. The method as claimed in claim 1 where the human head is identified by applying a random decision forest. 5. The method as claimed in claim 1 where the human head is identified by skin color. 6. The method as claimed in claim 1 where the human head is identified by a face detection method. 7. The method as claimed in claim 1 where the head model is obtained by an iterative closest point match. 8. A system for real-time driver state monitoring, comprising:
a depth sensor to capture depth images of a scene and generate image data representing the image of the scene, wherein said image data comprises a depth value indicating distance from the depth sensor; and
an image processing system operatively connected to said sensor to receive a sequence of depth images from the sensor, the image processing system being programmed to identify a human head in the scene using the sequence of images, using the depth images to track position and orientation of the human head with respect to a principal position and orientation where the human head is modeled as a 3D surface to define a head model and continuously adapted over time to obtain an average neutral face model, identifying depth deviations from the neutral face, and classifying such depth deviations as different driver states, and wherein the image processing system is programmed whereby the neutral face is stored in a database including history of changes and whereby depth variations are compared to the history of changes that has been stored, for a specific driver, in the database and where the comparison is used to extract features for driver-state detection. 8. A system for real-time driver state monitoring, comprising:
a depth sensor to capture depth images of a scene and generate image data representing the image of the scene, wherein said image data comprises a depth value indicating distance from the depth sensor; and
an image processing system operatively connected to said sensor to receive a sequence of depth images from the sensor, the image processing system being programmed to identify a human head in the scene using the sequence of images, using the depth images to track position and orientation of the human head with respect to a principal position and orientation where the human head is modeled as a 3D surface to define a head model and continuously adapted over time to obtain an average neutral face model, identifying depth deviations from the neutral face, and classifying such depth deviations as different driver states, and wherein the image processing system is programmed whereby the neutral face is stored in a database including history of changes and whereby depth variations are compared to the history of changes that has been stored, for a specific driver, in the database and where the comparison is used to extract features for driver-state detection. 9. The system as claimed in claim 8 where the image processing system receives supplementary images comprising gray-scale or color images from the depth sensor or an additional sensor that has been pixel-wise registered to the depth image. 10. The system as claimed in claim 8 where the image processing system is programmed whereby all but head pixels are excluded according to a CAD model of an interior of a car. 11. The system as claimed in claim 8 where the image processing system is programmed whereby the human head is identified by applying a random decision forest. 12. The system as claimed in claim 8 where the image processing system is programmed whereby the human head is identified by skin color. 13. The system as claimed in claim 8 where the image processing system is programmed whereby the human head is identified by a face detection method. 14. The system as claimed in claim 8 where the image processing system is programmed whereby the head model is obtained by an iterative closest point match.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318832B2,US10318832B2,Method and apparatus for authenticating user using vein pattern,2014-10-23,"method, near, generated, light, including, reflected, based, vein, receives, skin, stored, where, authenticating, when, generates, image, using, apparatus, user, received, that, terminal, provided, sensor, infrared, diode, registered, pattern, authenticates, project, toward, matches, emitting, adjacent","A method and apparatus for authenticating a user using a vein pattern are provided that project a near infrared (NIR) ray toward a skin of the user using a light emitting diode (LED), where the user is adjacent to a terminal including the LED. An image sensor receives a light reflected by the skin, generates a vein pattern of the skin based on an image generated using the received light, and authenticates the user as a registered user of a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern.","1. A method of authenticating a user using a vein pattern, the method comprising:
verifying whether a skin of the user is positioned less than a preset distance from a terminal;
switching an operating mode of the terminal to an authentication mode, dependent on a result of the verifying;
increasing outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
projecting a near infrared (NIR) ray toward the skin of the user, using the LED, wherein the user is adjacent to a terminal comprising the LED;
receiving a light reflected by the skin using the image sensor;
verifying whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generating a vein pattern of the skin based on the image generated using the received light; and
authenticating the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 1. A method of authenticating a user using a vein pattern, the method comprising:
verifying whether a skin of the user is positioned less than a preset distance from a terminal;
switching an operating mode of the terminal to an authentication mode, dependent on a result of the verifying;
increasing outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
projecting a near infrared (NIR) ray toward the skin of the user, using the LED, wherein the user is adjacent to a terminal comprising the LED;
receiving a light reflected by the skin using the image sensor;
verifying whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generating a vein pattern of the skin based on the image generated using the received light; and
authenticating the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 2. The method of claim 1, further comprising:
verifying whether the generated vein pattern matches the pre-stored vein pattern. 3. The method of claim 1, wherein the terminal is a wearable device. 4. The method of claim 1, further comprising:
controlling the projecting of the NIR ray by increasing an integration time when an output of the LED is determined to be lower than a preset output. 5. The method of claim 1, further comprising:
unlocking the terminal in response to the user being authenticated as the registered user. 6. The method of claim 1, further comprising:
unlocking a device interoperating with the terminal in response to the user being authenticated as the registered user. 7. The method of claim 1, wherein the switching of the operating mode of the terminal to the authentication mode is performed upon the skin being determined to be positioned less than the preset distance from the terminal. 8. The method of claim 1, further comprising:
detecting a pulse wave of a vein of the skin using the received light; and
verifying whether the detected pulse wave matches a pre-stored pulse wave,
wherein the authenticating comprises authenticating the user as the registered user in response to the generated vein pattern matching the pre-stored vein pattern and in response to the detected pulse wave matching the pre-stored pulse wave. 9. The method of claim 8, wherein the detecting comprises detecting the pulse wave using an image amplifier. 10. The method of claim 1, wherein the image sensor comprises one of a complementary metal-oxide semiconductor (CMOS) image sensor (CIS), a charge coupled device (CCD), a laser speckle sensor, and a sensor using indium-gallium-sulfur (InGaS). 11. The method of claim 1, further comprising:
registering the registered user at the terminal. 12. The method of claim 11, wherein the registering comprises:
projecting an NIR ray toward a skin of the registered user adjacent to the terminal, using the LED;
receiving a light reflected by the skin of the registered user using the image sensor;
generating a vein pattern of the skin of the registered user based on an image generated using the received light; and
registering the registered user at the terminal by storing the vein pattern. 13. The method of claim 1, wherein the authenticating comprises correcting the image to verify whether the generated vein pattern matches the pre-stored vein pattern. 14. The method of claim 13, wherein the correcting comprises:
grayscaling the image; and
correcting at least one of a scale, a point of view, and an orientation of the grayscaled image. 15. The method of claim 1, further comprising:
verifying whether the terminal is worn by the user when the terminal is a wearable device,
wherein the projecting is performed in response to the terminal being worn by the user. 16. The method of claim 15, further comprising:
verifying whether the terminal is removed; and
switching an operating mode of the terminal to a locking mode in response to the terminal being removed. 17. The method of claim 1, further comprising:
transmitting information corresponding to the registered user to a device interoperating with the terminal when the user is authenticated as the registered user. 18. A terminal comprising:
one or more processors configured to:
verify whether a skin of the user is positioned less than a preset distance from a terminal;
switch an operating mode of the terminal to an authentication mode, dependent on a result of the verifying;
increase outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
project a near infrared (NIR) ray toward a skin of a user of the terminal using the LED, wherein the user is adjacent to the terminal comprising the LED;
receive a light reflected by the skin using the image sensor;
verify whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generate a vein pattern of the skin based on the image generated using the received light; and
authenticate the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 18. A terminal comprising:
one or more processors configured to:
verify whether a skin of the user is positioned less than a preset distance from a terminal;
switch an operating mode of the terminal to an authentication mode, dependent on a result of the verifying;
increase outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
project a near infrared (NIR) ray toward a skin of a user of the terminal using the LED, wherein the user is adjacent to the terminal comprising the LED;
receive a light reflected by the skin using the image sensor;
verify whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generate a vein pattern of the skin based on the image generated using the received light; and
authenticate the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 19. A method of authenticating a user using a vein pattern performed by a terminal, the method comprising:
verifying whether a skin of the user is positioned less than a preset distance from a terminal;
switching an operating mode of the terminal to an authentication mode, dependent on a result of the verifying;
increasing outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
projecting a near infrared (NIR) ray toward a touched skin of the user using the LED;
receiving a light reflected by the skin using the image sensor;
verifying whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generating a vein pattern of the skin based on the image generated using the received light; and
authenticating the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 19. A method of authenticating a user using a vein pattern performed by a terminal, the method comprising:
verifying whether a skin of the user is positioned less than a preset distance from a terminal;
switching an operating mode of the terminal to an authentication mode, dependent on a result of the verifying;
increasing outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
projecting a near infrared (NIR) ray toward a touched skin of the user using the LED;
receiving a light reflected by the skin using the image sensor;
verifying whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generating a vein pattern of the skin based on the image generated using the received light; and
authenticating the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 20. The method of claim 19, further comprising:
verifying whether the generated vein pattern matches the pre-stored vein pattern. 21. The method of claim 19, wherein the terminal is a wearable device. 22. The method of claim 19, further comprising:
controlling the projecting of the NIR ray by increasing an integration time when an output of the LED is determined to be lower than a preset output. 23. The method of claim 19, further comprising:
unlocking the terminal in response to the user being authenticated as the registered user. 24. The method of claim 19, further comprising:
unlocking a device interoperating with the terminal in response to the user being authenticated as the registered user. 25. The method of claim 19, further comprising:
detecting a pulse wave of a vein of the skin using the received light; and
verifying whether the detected pulse wave matches a pre-stored pulse wave,
wherein the authenticating comprises authenticating the user as the registered user in response to the generated vein pattern matching the pre-stored vein pattern and in response to the detected pulse wave matching the pre-stored pulse wave. 26. The method of claim 25, wherein the detecting comprises detecting the pulse wave using an image amplifier. 27. The method of claim 19, wherein the image sensor comprises one of a complementary metal-oxide semiconductor (CMOS) image sensor (CIS), a charge coupled device (CCD), a laser speckle sensor, and a sensor using indium-gallium-sulfur (InGaS). 28. The method of claim 19, further comprising:
registering the registered user at the terminal. 29. The method of claim 28, wherein the registering comprises:
recognizing a touch of the registered user;
projecting an NIR ray toward a touched skin of the registered user using the LED;
receiving a light reflected by the skin of the registered user using the image sensor;
generating a vein pattern of the skin of the registered user using the received light; and
registering the registered user at the terminal by storing the vein pattern. 30. The method of claim 19, wherein the authenticating comprises correcting the image to verify whether the generated vein pattern matches the pre-stored vein pattern. 31. The method of claim 30, wherein the correcting comprises:
grayscaling the image; and
correcting at least one of a scale, a point of view, and an orientation of the grayscaled image. 32. The method of claim 19, further comprising:
verifying whether the terminal is worn by the user in response to the terminal being a wearable device,
wherein the projecting is performed in response to the terminal being worn by the user. 33. The method of claim 32, further comprising:
verifying whether the terminal is removed; and
switching an operating mode of the terminal to a locking mode in response to the terminal being removed. 34. The method of claim 19, further comprising:
transmitting information corresponding to the registered user to a device interoperating with the terminal when the user is authenticated as the registered user. 35. A terminal comprising:
one or more processors configured to:
verify whether a skin of a user is positioned on the terminal;
switch an operating mode of the terminal to an authentication mode upon the verifying of the skin of the user being positioned on the terminal;
increase outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
project a near infrared (NIR) ray toward a touched skin of the user using the LED;
receive a light reflected by the skin using the image sensor;
verify whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generate a vein pattern of the skin based on the image generated using the received light; and
authenticate the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 35. A terminal comprising:
one or more processors configured to:
verify whether a skin of a user is positioned on the terminal;
switch an operating mode of the terminal to an authentication mode upon the verifying of the skin of the user being positioned on the terminal;
increase outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
project a near infrared (NIR) ray toward a touched skin of the user using the LED;
receive a light reflected by the skin using the image sensor;
verify whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generate a vein pattern of the skin based on the image generated using the received light; and
authenticate the user as a registered user corresponding to a pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 36. A method of authenticating a user using a vein pattern, the method comprising:
automatically, in response to a verification that a skin of the user is positioned less than a preset distance from a terminal, switching an operating mode of the terminal to an authentication mode;
increasing outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
projecting a near infrared (NIR) ray toward the skin of the user using the LED;
receiving a light reflected by the skin using the image sensor;
verifying whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generating a vein pattern of the skin based on the image generated using the received light; and
authenticating the user as a registered user based on the generated vein pattern and a pre-stored vein pattern. 36. A method of authenticating a user using a vein pattern, the method comprising:
automatically, in response to a verification that a skin of the user is positioned less than a preset distance from a terminal, switching an operating mode of the terminal to an authentication mode;
increasing outputs of a light emitting diode (LED) and an image sensor to preset outputs in response to the operating mode being switched to the authentication mode;
projecting a near infrared (NIR) ray toward the skin of the user using the LED;
receiving a light reflected by the skin using the image sensor;
verifying whether an image generated using the received light exhibits a vein pattern;
in response to the image generated using the received light exhibiting a vein pattern, generating a vein pattern of the skin based on the image generated using the received light; and
authenticating the user as a registered user based on the generated vein pattern and a pre-stored vein pattern. 37. The method of claim 36, wherein the authenticating comprises authenticating the user as a registered user corresponding to the pre-stored vein pattern when the generated vein pattern matches the pre-stored vein pattern. 38. The method of claim 36, further comprising:
performing the verification that the skin of the user is positioned less than the preset distance from the terminal.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318835B2,US10318835B2,Extraction of data from a digital image,2013-03-12,"first, digital, capturing, device, back, characters, computing, recognition, extraction, instrument, protrudes, based, extracted, process, indented, data, receives, thereon, application, side, each, plurality, into, image, such, extracts, item, front, that, categorizes, payment, comprises, information, depicted, relating, applies, from, imprinted, character, comprising, categories, sets","Capturing information from an imprinted item comprises a computing device that receives an image of a back side of an imprinted item, the payment instrument comprising information imprinted thereon such that the imprinted information protrudes from a front side of the imprinted item and the imprinted information is indented into the back side of the imprinted item. The computing device extracts sets of characters from the image of the back side of the imprinted item based on the imprinted information indented into the back side of the imprinted item and depicted in the image of the back side of the imprinted item. The computing device applies a first character recognition application to process the sets of characters extracted from the image of the back side of the imprinted item and categorizes each of the sets of characters into one of a plurality of categories relating to the information.","1. A computer-implemented method for capturing information from imprinted items, comprising:
receiving, using one or more computing devices, an image of a back side of an imprinted item, the imprinted item comprising information imprinted thereon such that the imprinted information protrudes from a front side of the imprinted item and the imprinted information is indented into the back side of the imprinted item;
extracting, using the one or more computing devices, sets of characters from the image of the back side of the imprinted item based on the imprinted information indented into the back side of the imprinted item and depicted in the image of the back side of the imprinted item;
applying, using the one or more computing devices, a first character recognition application to process the sets of characters extracted from the image of the back side of the imprinted item; and
categorizing, using the one or more computing devices, each of the sets of characters into one of a plurality of categories relating to information required to use data from the imprinted item. 1. A computer-implemented method for capturing information from imprinted items, comprising:
receiving, using one or more computing devices, an image of a back side of an imprinted item, the imprinted item comprising information imprinted thereon such that the imprinted information protrudes from a front side of the imprinted item and the imprinted information is indented into the back side of the imprinted item;
extracting, using the one or more computing devices, sets of characters from the image of the back side of the imprinted item based on the imprinted information indented into the back side of the imprinted item and depicted in the image of the back side of the imprinted item;
applying, using the one or more computing devices, a first character recognition application to process the sets of characters extracted from the image of the back side of the imprinted item; and
categorizing, using the one or more computing devices, each of the sets of characters into one of a plurality of categories relating to information required to use data from the imprinted item. 2. The computer-implemented method of claim 1, further comprising communicating, using the one or more computing devices, the sets of characters and the categories associated with the sets of characters to a third party application. 3. The computer-implemented method of claim 1, further comprising reversing, using the one or more computing devices, the imprinted information from the back side of the imprinted item in connection with applying the first character recognition application. 4. The computer-implemented method of claim 1, further comprising:
receiving, using the one or more computing devices, an image of a front side of the imprinted item;
extracting, using the one or more computing devices, sets of characters from the image of the front side of the imprinted item based on the imprinted information protruding from the front side of the imprinted item and depicted in the image of the front side of the imprinted item;
applying, using the one or more computing devices, a second character recognition application to process the sets of characters extracted from the image of the front side of the imprinted item;
categorizing, using the one or more computing devices, each of the sets of characters into one of the plurality of categories relating to information required to use data from the imprinted item;
comparing, using the one or more computing devices, a processed set of characters from the front side of the imprinted item associated with a particular one of the categories to a processed set of characters from the back side of the imprinted item associated with the particular one of the categories; and
refining, using the one or more computing devices, the processing of the compared sets of characters from the front side and the back side of the imprinted item based on the comparison of the compared sets of characters from the front side and the back side of the imprinted item. 5. The computer-implemented method of claim 4, further comprising:
displaying, using the one or more computing devices, a request for a validation or correction of at least one of the sets of characters, in response to the comparison determining that the processed set of characters from the front side of the imprinted item associated with the particular one of the categories differ from the processed set of characters from the back side of the imprinted item associated with the particular one of the categories. 6. The computer-implemented method of claim 4, wherein the first and second character recognition applications are a same application. 7. The computer-implemented method of claim 4, wherein the first and second character recognition applications are different. 8. The computer-implemented method of claim 1, further comprising:
receiving, using the one or more computing devices, an image of the front side of the imprinted item;
determining, using the one or more computing devices, a first set of characters from the front of the imprinted item that are in a location on the imprinted item that corresponds to a second set of characters on the back side of the imprinted item; and
combining, using the one or more computing devices, the images of the first set of characters and the second set of characters to produce a combined set of characters,
wherein the first character recognition process is applied to the combined set of characters. 9. The computer-implemented method of claim 1, wherein the image of back side of the imprinted item is captured by a camera operating on the one or more computing devices. 10. The computer-implemented method of claim 1, wherein the categories comprise one or more of a user account number, a user name, an account expiration date, a phone number of an issuer of the account, and a signature of the user. 11. A computer program product, comprising:
a non-transitory computer-readable storage device having computer-executable program instructions embodied thereon that when executed by a computer cause the computer to capture information from imprinted items, the computer-executable program instructions comprising:
computer-executable program instructions to receive an image of a back side of an imprinted item, the imprinted item comprising information imprinted thereon such that the imprinted information protrudes from a front side of the imprinted item and the imprinted information is indented into the back side of the imprinted item;
computer-executable program instructions to extract sets of characters from the image of the back side of the imprinted item based on the imprinted information indented into the back side of the imprinted item and depicted in the image of the back side of the imprinted item;
computer-executable program instructions to apply a first character recognition application to process the sets of characters extracted from the image of the back side of the imprinted item; and
computer-executable program instructions to categorize each of the sets of characters into one of a plurality of categories relating to information required to use data from the imprinted item. 11. A computer program product, comprising:
a non-transitory computer-readable storage device having computer-executable program instructions embodied thereon that when executed by a computer cause the computer to capture information from imprinted items, the computer-executable program instructions comprising:
computer-executable program instructions to receive an image of a back side of an imprinted item, the imprinted item comprising information imprinted thereon such that the imprinted information protrudes from a front side of the imprinted item and the imprinted information is indented into the back side of the imprinted item;
computer-executable program instructions to extract sets of characters from the image of the back side of the imprinted item based on the imprinted information indented into the back side of the imprinted item and depicted in the image of the back side of the imprinted item;
computer-executable program instructions to apply a first character recognition application to process the sets of characters extracted from the image of the back side of the imprinted item; and
computer-executable program instructions to categorize each of the sets of characters into one of a plurality of categories relating to information required to use data from the imprinted item. 12. The computer program product of claim 11, further comprising computer-executable program instructions to communicate the sets of characters and the categories associated with the sets of characters to a third party application. 13. The computer program product of claim 11, further comprising computer-executable program instructions to reverse the imprinted information from the back side of the imprinted item in connection with applying the first character recognition application. 14. The computer program product of claim 11, further comprising:
computer-executable program instructions to receive an image of a front side of the imprinted item;
computer-executable program instructions to extract sets of characters from the image of the front side of the imprinted item based on the imprinted information protruding from the front side of the imprinted item and depicted in the image of the front side of the imprinted item;
computer-executable program instructions to apply a second character recognition application to process the sets of characters extracted from the image of the front side of the imprinted item;
computer-executable program instructions to categorize each of the sets of characters into one of the plurality of categories relating to information required to use data from the imprinted item;
computer-executable program instructions to compare a processed set of characters from the front side of the imprinted item associated with a particular one of the categories to a processed set of characters from the back side of the imprinted item associated with the particular one of the categories; and
computer-executable program instructions to refine the processing of the compared sets of characters from the front side and the back side of the imprinted item based on the comparison of the compared sets of characters from the front side and the back side of the imprinted item. 15. A system for capturing information from imprinted items, the system comprising:
a storage resource; and
a processor communicatively coupled to the storage resource, wherein the processor executes application code instructions that are stored in the storage resource and that cause the system to:
receive an image of a back side of an imprinted item, the imprinted item comprising information imprinted thereon such that the imprinted information protrudes from a front side of the imprinted item and the imprinted information is indented into the back side of the imprinted item;
extract sets of characters from the image of the back side of the imprinted item based on the imprinted information indented into the back side of the imprinted item and depicted in the image of the back side of the imprinted item;
apply a first character recognition application to process the sets of characters extracted from the image of the back side of the imprinted item; and
categorize each of the sets of characters into one of a plurality of categories relating to information required to use data from the imprinted item. 15. A system for capturing information from imprinted items, the system comprising:
a storage resource; and
a processor communicatively coupled to the storage resource, wherein the processor executes application code instructions that are stored in the storage resource and that cause the system to:
receive an image of a back side of an imprinted item, the imprinted item comprising information imprinted thereon such that the imprinted information protrudes from a front side of the imprinted item and the imprinted information is indented into the back side of the imprinted item;
extract sets of characters from the image of the back side of the imprinted item based on the imprinted information indented into the back side of the imprinted item and depicted in the image of the back side of the imprinted item;
apply a first character recognition application to process the sets of characters extracted from the image of the back side of the imprinted item; and
categorize each of the sets of characters into one of a plurality of categories relating to information required to use data from the imprinted item. 16. The system of claim 15, the processor executing further application code instructions that are stored in the storage device and that cause the system to:
receive an image of a front side of the imprinted item;
extract sets of characters from the image of the front side of the imprinted item based on the imprinted information protruding from the front side of the imprinted item and depicted in the image of the front side of the imprinted item;
apply a second character recognition application to process the sets of characters extracted from the image of the front side of the imprinted item;
categorize each of the sets of characters into one of the plurality of categories relating to information required to use data from the imprinted item;
compare a processed set of characters from the front side of the imprinted item associated with a particular one of the categories to a processed set of characters from the back side of the imprinted item associated with the particular one of the categories; and
refine the processing of the compared sets of characters from the front side and the back side of the imprinted item based on the comparison of the compared sets of characters from the front side and the back side of the imprinted item. 17. The system of claim 16, the processor executing further application code instructions that are stored in the storage device and that cause the system to display a request for a validation or correction of at least one of the sets of characters, in response to the comparison determining that the processed set of characters from the front side of the imprinted item associated with the particular one of the categories differ from the processed set of characters from the back side of the imprinted item associated with the particular one of the categories. 18. The system of claim 16, wherein the first and second character recognition applications are a same application. 19. The system of claim 16, the processor executing further application code instructions that are stored in the storage device and that cause the system to:
receive an image of the front side of the imprinted item;
determine a first set of characters from the front of the imprinted item that are in a location on the imprinted item that corresponds to a second set of characters on the back side of the imprinted item; and

combine the first set of characters and the second set of characters to produce an image of a combined set of characters, wherein the first character recognition process is applied to the combined set of characters.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318836B2,US10318836B2,System and method for designating surveillance camera regions of interest,2016-03-22,"method, monitor, integrated, optical, camera, region, useful, designating, multiple, this, device, enables, monitors, defined, positioned, cameras, within, analyzes, based, process, data, operator, regions, devices, response, phone, scene, each, when, surveillance, definition, image, same, analytics, analyzing, across, interest, outline, moves, between, user, simultaneously, setup, scenes, preferably, their, embodiments, disclosed, either, system, from, mobile, tracking, designation, stores","A system and method of designating regions of interest for surveillance cameras is disclosed. The system enables definition of regions of interest simultaneously across multiple cameras. This is useful when the cameras are positioned to monitor the same region of interest within their scenes. Each camera monitors image data for designation of regions of interest, stores the regions of interest, and analyzes the image data from the cameras based on the regions of interest. Preferably, each camera has an integrated analytics system for analyzing the image data based on the regions of interest. A setup process between a user device (e.g. mobile phone) and the cameras enables definition of the regions of interest. In embodiments, the regions of interest are defined in response to the cameras tracking either the user device or an optical device as an operator moves the devices to outline the regions of interest within the scene.","1. A method of designating regions of interest in the fields of view of surveillance cameras and analyzing image data from the surveillance cameras, the method comprising:
designating the regions of interest by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot is moved to thereby define the regions of interest;
capturing image data with the surveillance cameras; and
analyzing the image data for designation of regions of interest. 1. A method of designating regions of interest in the fields of view of surveillance cameras and analyzing image data from the surveillance cameras, the method comprising:
designating the regions of interest by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot is moved to thereby define the regions of interest;
capturing image data with the surveillance cameras; and
analyzing the image data for designation of regions of interest. 2. The method of claim 1, further comprising analyzing the image data from the surveillance cameras based on the regions of interest. 3. The method of claim 2, wherein analyzing the image data from the surveillance cameras based on the regions of interest comprises:
detecting predetermined optical patterns; and
generating metadata for the image data in response to detecting the predetermined optical patterns. 4. The method of claim 2, wherein analyzing the image data from the surveillance cameras based on the regions of interest comprises tracking movement of objects or persons relative to point of sale terminals or product displays or thresholds of doors or areas movement along streets or hallways. 5. The method of claim 1, further comprising analyzing the image data on analytics systems embedded in the surveillance cameras to determine the designated regions of interest. 6. The method of claim 1, further comprising analyzing the image data on external analytics systems to determine the designated regions of interest. 7. The method of claim 1, further comprising analyzing the image data on analytics systems executing on a portable computing device to determine the designated regions of interest. 8. A surveillance camera system, comprising:
one or more surveillance cameras capturing image data; and
an analytics system analyzing the image data for designation of regions of interest by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot is moved to thereby define the regions of interest. 8. A surveillance camera system, comprising:
one or more surveillance cameras capturing image data; and
an analytics system analyzing the image data for designation of regions of interest by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot is moved to thereby define the regions of interest. 9. The system of claim 8, wherein the analytics system analyzes the image data from the surveillance cameras based on the regions of interest. 10. The system of claim 9, wherein analytics system detects predetermined optical patterns; and generating metadata for the image data in response to detecting the predetermined optical patterns. 11. The system of claim 9, wherein analytics system tracks movement of objects or persons relative to regions of interest corresponding to point of sale terminals or product displays or thresholds of doors or area of movement along streets or hallways. 12. The system of claim 8, wherein the analytics system is embedded in the surveillance cameras to determine the designated regions of interest. 13. The system of claim 8, wherein the analytics system is an external analytics system. 14. The system of claim 8, wherein the analytics system executes on a portable computing device to determine the designated regions of interest. 15. A method of designating regions of interest in the fields of view of surveillance cameras, the method comprising:
capturing image data with the surveillance cameras wherein the fields of view of the cameras are overlapping by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot it is moved to thereby define the regions of interest; and
analyzing the image data for designation of the same regions of interest in each of the different fields for different cameras simultaneously. 15. A method of designating regions of interest in the fields of view of surveillance cameras, the method comprising:
capturing image data with the surveillance cameras wherein the fields of view of the cameras are overlapping by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot it is moved to thereby define the regions of interest; and
analyzing the image data for designation of the same regions of interest in each of the different fields for different cameras simultaneously. 16. A surveillance camera system, comprising:
surveillance cameras capturing image data and having overlapping fields of view; and
an analytics system analyzing the image data from the surveillance cameras for designation of regions of interest by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot it is moved to thereby define the regions of interest. 16. A surveillance camera system, comprising:
surveillance cameras capturing image data and having overlapping fields of view; and
an analytics system analyzing the image data from the surveillance cameras for designation of regions of interest by tracking a light spot projected into fields of view of the surveillance cameras by a laser pointer as the light spot it is moved to thereby define the regions of interest.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318837B2,US10318837B2,Distinguishing between stock keeping units using a physical dimension of a region depicted in an image,2014-12-23,"method, region, disclosure, keeping, recognition, including, identifier, based, ratio, physical, identified, receives, portion, pixel, appearance, application, size, units, plurality, each, generates, image, using, item, interest, vary, between, includes, that, unit, dimension, determines, pixels, stock, depicted, identifies, distinguishing, system, stocking, items, shelving, similar","The disclosure includes a system and method for distinguishing between stock keeping units of similar appearance that vary in size. An image recognition application receives an image including a shelving unit stocking a plurality of items, identifies each item in the image, generates a region of interest for each identified item in the image, identifies a physical dimension of a portion of region depicted in the image, determines a dimension of the region of interest for each identified item and the portion of region in pixels, determines a pixel-to-physical dimension ratio using the dimension in pixels of the portion of region and the physical dimension of the portion of region depicted in the image, and determines a stock keeping unit identifier of each identified item in the image based on the pixel-to-physical dimension ratio and the dimension of the region of interest for each identified item.","1. A method comprising:
receiving, by a hardware processor, a first image including a shelving unit stocking a plurality of items of varying sizes, the plurality of items having a similar appearance;
identifying an item in the first image;
generating a region of interest for the identified item in the first image;
determining an angle of aperture of a capture device that captured the first image;
determining a distance of the capture device from the shelving unit;
identifying a physical dimension of a region depicted in the first image based on the angle of aperture and the distance of the capture device from the shelving unit;
determining a first dimension of the region of interest for the identified item and a second dimension of the region depicted in the first image, wherein the first dimension and the second dimension are expressed in pixels;
determining, by a pixel-to-physical dimension ratio engine, a pixel-to-physical dimension ratio using the second dimension and the physical dimension of the region depicted in the first image;
determining, by a size engine, a physical dimension corresponding to the identified item using the pixel-to-physical dimension ratio and the first dimension; and
determining, by the size engine, a stock keeping unit identifier distinguishing the identified item of a first size from the plurality of items of varying sizes by matching the physical dimension corresponding to the identified item with a reference physical dimension for the identified item stored in a stock keeping unit table. 1. A method comprising:
receiving, by a hardware processor, a first image including a shelving unit stocking a plurality of items of varying sizes, the plurality of items having a similar appearance;
identifying an item in the first image;
generating a region of interest for the identified item in the first image;
determining an angle of aperture of a capture device that captured the first image;
determining a distance of the capture device from the shelving unit;
identifying a physical dimension of a region depicted in the first image based on the angle of aperture and the distance of the capture device from the shelving unit;
determining a first dimension of the region of interest for the identified item and a second dimension of the region depicted in the first image, wherein the first dimension and the second dimension are expressed in pixels;
determining, by a pixel-to-physical dimension ratio engine, a pixel-to-physical dimension ratio using the second dimension and the physical dimension of the region depicted in the first image;
determining, by a size engine, a physical dimension corresponding to the identified item using the pixel-to-physical dimension ratio and the first dimension; and
determining, by the size engine, a stock keeping unit identifier distinguishing the identified item of a first size from the plurality of items of varying sizes by matching the physical dimension corresponding to the identified item with a reference physical dimension for the identified item stored in a stock keeping unit table. 2. The method of claim 1 wherein determining the angle of aperture of the capture device further comprises receiving a first input from a user to align an interactive graphical geometric shape with the region depicted in the first image. 3. The method of claim 1 wherein determining the distance of the capture device from the shelving unit further comprises:
receiving a second image of a base of the shelving unit, the base of the shelving unit being positioned at a center of the second image;
determining an angle of tilt of the capture device that captured the second image;
determining a height of the capture device from the ground; and
multiplying the height of the capture device from the ground with a tangent of the angle of tilt of the capture device. 4. The method of claim 1 wherein determining the distance of the capture device from the shelving unit further comprises:
determining that the identified item is in focus in the first image;
directing a burst of ultrasound pulse from an ultrasound sensor of the capture device toward the identified item;
determining a time taken by the burst of ultrasound pulse to travel to the identified item and bounce back from the identified item; and
determining the distance of the capture device from the shelving unit based on the time taken and a speed of sound. 5. The method of claim 1 wherein determining the pixel-to-physical dimension ratio further comprises:
determining, by the pixel-to-physical dimension ratio engine, the pixel-to-physical dimension ratio by dividing the second dimension of the region depicted in the first image expressed in pixels by the physical dimension of the region depicted in the first image. 6. The method of claim 1 further comprising:
determining, by the size engine, the physical dimension corresponding to the identified item by dividing the first dimension of the region of interest expressed in pixels by the pixel-to-physical dimension ratio. 7. The method of claim 1 wherein the region depicted in the first image is one from a group of a shelf, a shelving unit, and a whole region depicted in the first image. 8. The method of claim 6 wherein the physical dimension corresponding to the identified item is one from a group of: a height, a width, an area, and a diagonal length. 9. The method of claim 1 wherein the angle of aperture of the capture device is the angle of aperture as seen from a focal point of a lens of the capture device. 10. A system comprising:
a hardware processor; and
a memory, the memory storing instructions, which when executed cause the hardware processor to implement:
an image processor configured to receive a first image including a shelving unit stocking a plurality of items of varying sizes, the plurality of items having a similar appearance, to identify an item in the first image, to generate a region of interest for the identified item in the first image, to determine an angle of aperture of a capture device that captured the first image, to determine a distance of the capture device from the shelving unit, to identify a physical dimension of a region depicted in the first image based on the angle of aperture and the distance of the capture device from the shelving unit, and to determine a first dimension of the region of interest for the identified item and a second dimension of the region depicted in the first image, wherein the first dimension and the second dimension are expressed in pixels;
a pixel-to-physical dimension ratio engine coupled to the image processor and configured to determine a pixel-to-physical dimension ratio using the second dimension and the physical dimension of the region depicted in the first image; and a size engine coupled to the pixel-to-physical dimension ratio engine and configured to determine a physical dimension corresponding to the identified item using the pixel-to-physical dimension ratio and the first dimension and a stock keeping unit identifier distinguishing the identified item of a first size from the plurality of items of varying sizes by matching the physical dimension corresponding to the identified item with a reference physical dimension for the identified item stored in a stock keeping unit table. 10. A system comprising:
a hardware processor; and
a memory, the memory storing instructions, which when executed cause the hardware processor to implement:
an image processor configured to receive a first image including a shelving unit stocking a plurality of items of varying sizes, the plurality of items having a similar appearance, to identify an item in the first image, to generate a region of interest for the identified item in the first image, to determine an angle of aperture of a capture device that captured the first image, to determine a distance of the capture device from the shelving unit, to identify a physical dimension of a region depicted in the first image based on the angle of aperture and the distance of the capture device from the shelving unit, and to determine a first dimension of the region of interest for the identified item and a second dimension of the region depicted in the first image, wherein the first dimension and the second dimension are expressed in pixels;
a pixel-to-physical dimension ratio engine coupled to the image processor and configured to determine a pixel-to-physical dimension ratio using the second dimension and the physical dimension of the region depicted in the first image; and a size engine coupled to the pixel-to-physical dimension ratio engine and configured to determine a physical dimension corresponding to the identified item using the pixel-to-physical dimension ratio and the first dimension and a stock keeping unit identifier distinguishing the identified item of a first size from the plurality of items of varying sizes by matching the physical dimension corresponding to the identified item with a reference physical dimension for the identified item stored in a stock keeping unit table. 11. The system of claim 10 wherein the image processor is configured to determine the angle of aperture of the capture device by receiving a first input from a user to align an interactive graphical geometric shape with the region depicted in the first image. 12. The system of claim 10 wherein the image processor to determine the distance of the capture device from the shelving unit is further configured to receive a second image of a base of the shelving unit, the base of the shelving unit being positioned at a center of the second image, determine an angle of tilt of the capture device that captured the second image, determine a height of the capture device from the ground, and multiply the height of the capture device from the ground with a tangent of the angle of tilt of the capture device. 13. The system of claim 10 wherein the image processor to determine the distance of the capture device from the shelving unit is further configured to determine that the identified item is in focus in the first image, direct an ultrasound pulse from the capture device toward the identified item, determine a time taken by the ultrasound pulse to travel to the identified item and bounce back from the identified item, and determine the distance of the capture device from the shelving unit based on the time taken and a speed of sound. 14. The system of claim 10 wherein the pixel-to-physical dimension ratio engine to determine the pixel-to-physical dimension ratio is further configured to determine the pixel-to-physical dimension ratio by dividing the second dimension of the region depicted in the first image expressed in pixels by the physical dimension of the region depicted in the first image. 15. The system of claim 10 wherein the size engine is further configured to determine physical dimension corresponding to the identified item by dividing the first dimension of the region of interest expressed in pixels by the pixel-to-physical dimension ratio. 16. A computer program product comprising a non-transitory computer readable medium storing a computer readable program, wherein the computer readable program when executed causes a computer to perform operations comprising:
receiving a first image including a shelving unit stocking a plurality of items of varying sizes, the plurality of items having a similar appearance;
identifying an item in the first image;
generating a region of interest for the identified item in the first image;
determining an angle of aperture of a capture device that captured the first image;
determining a distance of the capture device from the shelving unit;
identifying a physical dimension of a region depicted in the first image based on the angle of aperture and the distance of the capture device from the shelving unit;
determining a first dimension of the region of interest for the identified item and a second dimension for the region depicted in the first image, wherein the first dimension and the second dimension are expressed in pixels;
determining a pixel-to-physical dimension ratio using the second dimension and the physical dimension of the region depicted in the first image;
determining, by a size engine, a physical dimension corresponding to the identified item using the pixel-to-physical dimension ratio and the first dimension; and
determining a stock keeping unit identifier distinguishing the identified item of a first size from the plurality of items of varying sizes by matching the physical dimension corresponding to the identified item with a reference physical dimension for the identified item stored in a stock keeping unit table. 16. A computer program product comprising a non-transitory computer readable medium storing a computer readable program, wherein the computer readable program when executed causes a computer to perform operations comprising:
receiving a first image including a shelving unit stocking a plurality of items of varying sizes, the plurality of items having a similar appearance;
identifying an item in the first image;
generating a region of interest for the identified item in the first image;
determining an angle of aperture of a capture device that captured the first image;
determining a distance of the capture device from the shelving unit;
identifying a physical dimension of a region depicted in the first image based on the angle of aperture and the distance of the capture device from the shelving unit;
determining a first dimension of the region of interest for the identified item and a second dimension for the region depicted in the first image, wherein the first dimension and the second dimension are expressed in pixels;
determining a pixel-to-physical dimension ratio using the second dimension and the physical dimension of the region depicted in the first image;
determining, by a size engine, a physical dimension corresponding to the identified item using the pixel-to-physical dimension ratio and the first dimension; and
determining a stock keeping unit identifier distinguishing the identified item of a first size from the plurality of items of varying sizes by matching the physical dimension corresponding to the identified item with a reference physical dimension for the identified item stored in a stock keeping unit table. 17. The computer program product of claim 16 wherein determining the angle of aperture of the capture device further comprises receiving a first input from a user to align an interactive graphical geometric shape with the region depicted in the first image. 18. The computer program product of claim 16 wherein determining the distance of the capture device from the shelving unit further comprises:
receiving a second image of a base of the shelving unit, the base of the shelving unit being positioned at a center of the second image;
determining an angle of tilt of the capture device that captured the second image;
determining a height of the capture device from the ground; and
multiplying the height of the capture device from the ground with a tangent of the angle of tilt of the capture device. 19. The computer program product of claim 16 wherein determining the distance of the capture device from the shelving unit further comprises:
determining that the identified item is in focus in the first image;
directing a burst of ultrasound pulse from an ultrasound sensor of the capture device toward the identified item;
determining a time taken by the burst of ultrasound pulse to travel to the identified item and bounce back from the identified item; and
determining the distance of the capture device from the shelving unit based on the time taken and a speed of sound. 20. The computer program product of claim 16 wherein determining the physical dimension corresponding to the identified item comprises dividing the first dimension of the region of interest expressed in pixels by the pixel-to-physical dimension ratio.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318834B2,US10318834B2,Optimized image feature extraction,2017-05-01,"part, reduce, extraction, feature, based, corresponding, consumption, optimize, operation, determine, used, each, point, circuitry, image, provides, descriptor, bandwidth, includes, memory, processing, scale, power, location, embodiment, least, system, optimization, optimized, accelerate","One embodiment provides an image processing circuitry. The image processing circuitry includes a feature extraction circuitry and an optimization circuitry. The feature extraction circuitry is to determine a feature descriptor based, at least in part, on a feature point location and a corresponding scale. The optimization circuitry is to optimize an operation of the feature extraction circuitry. Each optimization is to at least one of accelerate the operation of the feature extraction circuitry, reduce a power consumption of the feature extraction circuitry and/or reduce a system memory bandwidth used by the feature extraction circuitry.","1. An image processing circuitry comprising:
a feature extraction circuitry to determine a feature descriptor based, at least in part, on a feature point location and a corresponding scale; and
an optimization circuitry to optimize an operation of the feature extraction circuitry, each optimization to at least one of accelerate the operation of the feature extraction circuitry, reduce a power consumption of the feature extraction circuitry and/or reduce a system memory bandwidth used by the feature extraction circuitry;
wherein:
the feature extraction circuitry comprises an integral image circuitry to determine an integral image of a feature patch associated with the feature point location, a sample point (SP) intensity circuitry to determine a pixel intensity of a square sample point region associated with a sample point, or a combination thereof;
when the feature extraction circuitry comprises said integral image circuitry, said optimization circuitry is to configure the integral image circuitry to determine each integral image value utilizing a register, a line buffer and two addition operations; and
when the feature extraction circuitry comprises said SP intensity circuitry, said optimization circuitry is to configure the SP intensity circuitry to smooth the intensity of the square sample point region using a box filter. 1. An image processing circuitry comprising:
a feature extraction circuitry to determine a feature descriptor based, at least in part, on a feature point location and a corresponding scale; and
an optimization circuitry to optimize an operation of the feature extraction circuitry, each optimization to at least one of accelerate the operation of the feature extraction circuitry, reduce a power consumption of the feature extraction circuitry and/or reduce a system memory bandwidth used by the feature extraction circuitry;
wherein:
the feature extraction circuitry comprises an integral image circuitry to determine an integral image of a feature patch associated with the feature point location, a sample point (SP) intensity circuitry to determine a pixel intensity of a square sample point region associated with a sample point, or a combination thereof;
when the feature extraction circuitry comprises said integral image circuitry, said optimization circuitry is to configure the integral image circuitry to determine each integral image value utilizing a register, a line buffer and two addition operations; and
when the feature extraction circuitry comprises said SP intensity circuitry, said optimization circuitry is to configure the SP intensity circuitry to smooth the intensity of the square sample point region using a box filter. 2. The image processing circuitry of claim 1, wherein the optimization circuitry is to determine whether a spatial locality exists between a plurality of feature points. 3. The image processing circuitry of claim 1, wherein the feature extraction circuitry comprises said SP intensity circuitry, and said optimization circuitry is to adjust a location of the sample point based, at least in part, on an orientation of a feature patch associated with the feature point location, utilizing an orientation vector. 4. The image processing circuitry of claim 1, wherein the optimization circuitry is to sort a plurality of pairs of sample points according to an index of a sample point of each pair. 5. The image processing circuitry of claim 1, wherein the feature extraction circuitry comprises said integral image circuitry, and said optimization circuitry is to configure the integral image circuitry to store a selected integral image value to an image cache. 6. The image processing circuitry of claim 1, wherein the optimization circuitry is to configure the feature extraction circuitry to determine the feature descriptor utilizing intermediate floating-point precision. 7. A method comprising:
determining, by a feature extraction circuitry, a feature descriptor based, at least in part, on a feature point location and a corresponding scale; and
optimizing, by an optimization circuitry, an operation of the feature extraction circuitry, each optimization to at least one of accelerate the operation of the feature extraction circuitry, reduce a power consumption of the feature extraction circuitry and/or reduce a system memory bandwidth used by the feature extraction circuitry;
wherein:
the feature extraction circuitry comprises an integral image circuitry, a sample point (SP) intensity circuitry, or a combination thereof, and the method further comprises:
when the feature extraction circuitry comprises said integral image circuitry, determining, with the integral image circuitry, an integral image of a feature patch associated with the feature point location, and configuring, by the optimization circuitry, the integral image circuitry to determine each integral image value utilizing a register, a line buffer and two addition operations; and
when the feature extraction circuitry comprises said SP intensity circuitry, determining, by said SP intensity circuitry, a pixel intensity of a square sample point region associated with a sample point; and configuring, by the optimization circuitry, the SP intensity circuitry to smooth the intensity of the square sample point region using a box filter. 7. A method comprising:
determining, by a feature extraction circuitry, a feature descriptor based, at least in part, on a feature point location and a corresponding scale; and
optimizing, by an optimization circuitry, an operation of the feature extraction circuitry, each optimization to at least one of accelerate the operation of the feature extraction circuitry, reduce a power consumption of the feature extraction circuitry and/or reduce a system memory bandwidth used by the feature extraction circuitry;
wherein:
the feature extraction circuitry comprises an integral image circuitry, a sample point (SP) intensity circuitry, or a combination thereof, and the method further comprises:
when the feature extraction circuitry comprises said integral image circuitry, determining, with the integral image circuitry, an integral image of a feature patch associated with the feature point location, and configuring, by the optimization circuitry, the integral image circuitry to determine each integral image value utilizing a register, a line buffer and two addition operations; and
when the feature extraction circuitry comprises said SP intensity circuitry, determining, by said SP intensity circuitry, a pixel intensity of a square sample point region associated with a sample point; and configuring, by the optimization circuitry, the SP intensity circuitry to smooth the intensity of the square sample point region using a box filter. 8. The method of claim 7, further comprising determining, by the optimization circuitry, whether a spatial locality exists between a plurality of feature points. 9. The method of claim 7, wherein the feature extraction circuitry comprises said SP intensity circuitry, and the method further comprises further comprising adjusting, by the optimization circuitry, a location of the sample point based, at least in part, on an orientation of a feature patch associated with the feature point location, utilizing an orientation vector. 10. The method of claim 7, further comprising sorting, by the optimization circuitry, a plurality of pairs of sample points according to an index of a sample point of each pair. 11. The method of claim 7, wherein the feature extraction circuitry comprises said integral image circuitry, and the method further comprises configuring, by the optimization circuitry, the integral image circuitry to store a selected integral image value to an image cache. 12. The method of claim 7, further comprising configuring, by the optimization circuitry, the feature extraction circuitry to determine the feature descriptor utilizing intermediate floating-point precision. 13. An image feature system comprising:
a processor circuitry;
a system memory; and
an image processing circuitry comprising:
a feature extraction circuitry to determine a feature descriptor based, at least in part, on a feature point location and a corresponding scale; and
an optimization circuitry to optimize an operation of the feature extraction circuitry, each optimization to at least one of accelerate the operation of the feature extraction circuitry, reduce a power consumption of the feature extraction circuitry and/or reduce a system memory bandwidth used by the feature extraction circuitry;
wherein:
the feature extraction circuitry comprises an integral image circuitry to determine an integral image of a feature patch associated with the feature point location, a sample point (SP) intensity circuitry to determine a pixel intensity of a square sample point region associated with a sample point, or a combination thereof;
when the feature extraction circuitry comprises said integral image circuitry, determining, with the integral image circuitry, an integral image of a feature patch associated with the feature point location, and configuring, by the optimization circuitry, the integral image circuitry to determine each integral image value utilizing a register, a line buffer and two addition operations;
when the feature extraction circuitry comprises said SP intensity circuitry, determining, by said SP intensity circuitry, a pixel intensity of a square sample point region associated with a sample point; and configuring, by the optimization circuitry, the SP intensity circuitry to smooth the intensity of the square sample point region using a box filter. 13. An image feature system comprising:
a processor circuitry;
a system memory; and
an image processing circuitry comprising:
a feature extraction circuitry to determine a feature descriptor based, at least in part, on a feature point location and a corresponding scale; and
an optimization circuitry to optimize an operation of the feature extraction circuitry, each optimization to at least one of accelerate the operation of the feature extraction circuitry, reduce a power consumption of the feature extraction circuitry and/or reduce a system memory bandwidth used by the feature extraction circuitry;
wherein:
the feature extraction circuitry comprises an integral image circuitry to determine an integral image of a feature patch associated with the feature point location, a sample point (SP) intensity circuitry to determine a pixel intensity of a square sample point region associated with a sample point, or a combination thereof;
when the feature extraction circuitry comprises said integral image circuitry, determining, with the integral image circuitry, an integral image of a feature patch associated with the feature point location, and configuring, by the optimization circuitry, the integral image circuitry to determine each integral image value utilizing a register, a line buffer and two addition operations;
when the feature extraction circuitry comprises said SP intensity circuitry, determining, by said SP intensity circuitry, a pixel intensity of a square sample point region associated with a sample point; and configuring, by the optimization circuitry, the SP intensity circuitry to smooth the intensity of the square sample point region using a box filter. 14. The image feature system of claim 13, wherein the optimization circuitry is to determine whether a spatial locality exists between a plurality of feature points. 15. The image feature system of claim 13, wherein the feature extraction circuitry comprises said SP intensity circuitry, and the optimization circuitry is to adjust a location of the sample point based, at least in part, on an orientation of a feature patch associated with the feature point location, utilizing an orientation vector. 16. The image feature system of claim 13, wherein the optimization circuitry is to sort a plurality of pairs of sample points according to an index of a sample point of each pair. 17. The image feature system of claim 13, wherein the feature extraction circuitry comprises said integral image circuitry, and the optimization circuitry is to configure the integral image circuitry to store a selected integral image value to an image cache. 18. The image feature system of claim 13, wherein the processor circuitry is selected from the group comprising a general purpose processor, a special purpose processor, a graphics processing unit, a digital signal processing unit, a vector processing unit, a microcontroller and a finite state machine microcontroller. 19. The image feature system of claim 13, wherein the optimization circuitry is to configure the feature extraction circuitry to determine the feature descriptor utilizing intermediate floating-point precision.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318821B2,US10318821B2,Driver assistance for a vehicle,,"configured, driver, around, controls, detect, based, systems, acquired, related, view, object, disable, navigation, image, techniques, provides, vehicle, apparatus, engine, ahead, includes, that, sensor, automated, determines, detection, processor, information, enable, idle, having, signal, function, stop, disclosed, least, detected, control, from, assistance, acquire","Disclosed are systems and techniques for automated driver assistance in a vehicle having an Idle Stop and Go (ISG) function that controls an engine of the vehicle. A driver assistance apparatus includes at least one object detection sensor configured to acquire an image of a view ahead of the vehicle or a view around the vehicle, and a processor configured to detect information from the acquired image. The processor determines navigation information related to the vehicle, and provides a control signal to enable or disable the ISG function based on at least one of the information detected from the acquired image or the navigation information.","1. A driver assistance apparatus included in a vehicle having an Idle Stop and Go (ISG) function that is turned on based at least in part on a condition comprising a vehicle speed, whether an acceleration input has been received, and whether a brake input has been received, the apparatus comprising:
at least one camera; and
at least one processor configured to:
acquire, through the at least one camera, at least one image comprising at least one of a forward image or an around view image of the vehicle;
detect information from the at least one image; and
provide at least one control signal for turning off the ISG function based on the detected information, even when the condition is satisfied,

wherein the condition being satisfied comprises the brake input being received in a state in which the vehicle speed is less than or equal to a threshold speed with no acceleration input being received,
wherein the camera is configured to acquire the at least one image of at least one of a traffic light, a traffic sign, or a road surface,
wherein the at least one processor is further configured to:
detect, from the acquired at least one image, information regarding the at least one of the traffic light, the traffic sign, or the road surface, wherein the information comprises ramp information regarding an on-ramp on which the vehicle travels to join a main road,

wherein the on-ramp is a sloping road junction that connects a first road at a first elevation with a second road at a second elevation different from the first elevation,
wherein the at least one processor is further configured to:
detect a side portion of a second vehicle from the acquired at least one image;
determine whether the vehicle temporarily stops after entering the on-ramp based on the detected side portion of the second vehicle;
determine whether a preceding vehicle is detected from the at least one image;
determine whether the detected preceding vehicle is stationary based on a stop lamp of the detected preceding vehicle;
provide, based on the preceding vehicle being detected and the detected preceding vehicle determined as being stationary, the at least one control signal to turn on the ISG function; and
provide, based on the preceding vehicle not being detected or the detected preceding vehicle determined as being traveling, the at least one control signal to turn off the ISG function, and

wherein the at least one processor is further configured to:
provide the at least one control signal to turn off the ISG function based on the vehicle being a Left Hand Drive (LHD) vehicle and slowing down or temporarily stopping for a right-turn, or based on the vehicle being a Right Hand Drive (RHD) vehicle and slowing down or temporarily stopping for a left-turn; and
provide the at least one control signal to turn off the ISG function based on detecting a pedestrian stop signal from the pedestrian traffic light detected from the at least one image. 1. A driver assistance apparatus included in a vehicle having an Idle Stop and Go (ISG) function that is turned on based at least in part on a condition comprising a vehicle speed, whether an acceleration input has been received, and whether a brake input has been received, the apparatus comprising:
at least one camera; and
at least one processor configured to:
acquire, through the at least one camera, at least one image comprising at least one of a forward image or an around view image of the vehicle;
detect information from the at least one image; and
provide at least one control signal for turning off the ISG function based on the detected information, even when the condition is satisfied,

wherein the condition being satisfied comprises the brake input being received in a state in which the vehicle speed is less than or equal to a threshold speed with no acceleration input being received,
wherein the camera is configured to acquire the at least one image of at least one of a traffic light, a traffic sign, or a road surface,
wherein the at least one processor is further configured to:
detect, from the acquired at least one image, information regarding the at least one of the traffic light, the traffic sign, or the road surface, wherein the information comprises ramp information regarding an on-ramp on which the vehicle travels to join a main road,

wherein the on-ramp is a sloping road junction that connects a first road at a first elevation with a second road at a second elevation different from the first elevation,
wherein the at least one processor is further configured to:
detect a side portion of a second vehicle from the acquired at least one image;
determine whether the vehicle temporarily stops after entering the on-ramp based on the detected side portion of the second vehicle;
determine whether a preceding vehicle is detected from the at least one image;
determine whether the detected preceding vehicle is stationary based on a stop lamp of the detected preceding vehicle;
provide, based on the preceding vehicle being detected and the detected preceding vehicle determined as being stationary, the at least one control signal to turn on the ISG function; and
provide, based on the preceding vehicle not being detected or the detected preceding vehicle determined as being traveling, the at least one control signal to turn off the ISG function, and

wherein the at least one processor is further configured to:
provide the at least one control signal to turn off the ISG function based on the vehicle being a Left Hand Drive (LHD) vehicle and slowing down or temporarily stopping for a right-turn, or based on the vehicle being a Right Hand Drive (RHD) vehicle and slowing down or temporarily stopping for a left-turn; and
provide the at least one control signal to turn off the ISG function based on detecting a pedestrian stop signal from the pedestrian traffic light detected from the at least one image. 2. The apparatus according to claim 1, wherein the at least one processor is further configured to receive, through an interface unit, navigation information, and
wherein the at least one processor is configured to provide the at least one control signal for turning off the ISG function further based on the navigation information, even when the condition is satisfied. 3. The apparatus according to claim 2, wherein the at least one processor is further configured to acquire the ramp information based on the navigation information. 4. The apparatus according to claim 1, wherein the at least one processor is further configured to acquire the ramp information from the traffic sign in the acquired at least one image. 5. The apparatus according to claim 1, wherein the LHD vehicle is configured to join the main road from the on-ramp via a right-turn, and
wherein the RHD vehicle is configured to join the main road from the on-ramp via a left-turn. 6. The apparatus according to claim 1, wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function based on the vehicle slowing down or temporarily stopping in a state in which stop information indicating temporary stop is detected from the detected traffic sign while the vehicle travels on a road around a crossroad provided with no traffic light, or a stop line indicating temporary stop is detected from the detected road surface. 7. The apparatus according to claim 6, wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function upon detecting the stop line at least a threshold number of times. 8. The apparatus according to claim 6, wherein the at least one processor is further configured to receive, through an interface unit, navigation information, and
wherein the at least one processor is further configured to determine whether the vehicle travels on the road around the crossroad provided with no traffic light based on the navigation information. 9. The apparatus according to claim 1, wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function based on the vehicle slowing down or temporarily stopping on a road without lane marks. 10. The apparatus according to claim 9, wherein the at least one processor is further configured to:
determine whether a lane mark is detected from the at least one image including the road surface; and
based on a determination that a lane mark is not detected from the at least one image, determine that the vehicle travels on the road without lane marks. 11. The apparatus according to claim 9, wherein the at least one processor is further configured to received, through an interface unit, navigation information, and
wherein the at least one processor is further configured to determine whether the vehicle travels on the road without lane marks based on the navigation information. 12. The apparatus according to claim 1, wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function upon detecting go information from the traffic light detected from the at least one image based on the vehicle slowing down toward a crossroad or temporarily stopping in front of the crossroad. 13. The apparatus according to claim 12, wherein the at least one processor is further configured to:
detect the preceding vehicle from the forward image of the vehicle;
detect go information from the traffic light detected from the at least one image; and
provide the at least one control signal to turn off the ISG function based on the preceding vehicle traveling. 14. The apparatus according to claim 1, wherein the at least one processor is further configured to receive, through an interface unit, navigation information, and
wherein the at least one processor is further configured to:
determine that the vehicle is traveling at a crossroad based on the at least one image acquired by the camera or based on the navigation information; and
provide the at least one control signal to turn off the ISG function based on the vehicle traveling at the crossroad. 15. The apparatus according to claim 1, wherein the at least one processor is further configured to receive, through an interface unit, the navigation information or turn signal information, and
wherein the at least one processor is further configured to:
determine a right-turn or left-turn situation based on the navigation information, or
determine the right-turn or left-turn situation upon receiving the turn-signal information for right-turn or left-turn in a state in which the vehicle is located close to a crossroad. 16. The apparatus according to claim 1, wherein the at least one processor is further configured to:
detect the preceding vehicle from the forward image of the vehicle; and
provide the at least one control signal to turn on the ISG function based upon detecting that the stop lamp of the detected preceding vehicle is in an ON state. 17. The apparatus according to claim 1, wherein the at least one processor is further configured to:
determine a parking situation; and
provide the at least one control signal to turn off the ISG function based on information regarding the determined parking situation. 18. The apparatus according to claim 17, wherein the at least one processor is further configured to:
detect parking lot information from a traffic sign or a road surface detected from the at least one image; and
determine the parking situation based on the detected parking lot information. 19. The apparatus according to claim 18, wherein the at least one processor is further configured to:
detect a parking space from the detected road surface; and
detect the parking lot information based on the detected parking space. 20. The apparatus according to claim 17, wherein the at least one processor is further configured to receive, through an interface unit, vehicle speed information, steering wheel rotation angle information, or gearshift information, and
wherein the at least one processor is further configured to:
determine whether a parking operation is performed based on the vehicle speed information, the steering wheel rotation angle information, or the gearshift information; and
determine the parking situation based on whether implementation of the parking operation is detected. 21. The apparatus according to claim 17, wherein the at least one processor is further configured to receive, through an interface unit, user input to begin an automated parking mode,
wherein the at least one processor is further configured to determine the parking situation based on the user input. 22. The apparatus according to claim 17, wherein the at least one processor is further configured to determine the parking situation based on whether the vehicle is located in a parking lot among the navigation information. 23. The apparatus according to claim 1, wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function based on the vehicle being an LHD vehicle and the LHD vehicle slowing down or temporarily stopping for a left-turn without presence of a traffic light, and
wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function based on the vehicle being an RHD vehicle and the RHD vehicle slowing down or temporarily stopping for a right-turn without presence of the traffic light. 24. The apparatus according to claim 23, wherein the at least one processor is further configured to receive, through an interface unit, navigation information or turn signal information,
wherein the at least one processor is further configured to determine a left-turn without the presence of a traffic light situation or a right-turn without the presence of the traffic light situation based on at least one of a traffic sign detected from the at least one image, the navigation information, or the turn signal information. 25. The apparatus according to claim 1, wherein the traffic light includes the pedestrian traffic light, and
wherein the at least one processor is further configured to, based on the vehicle being an LHD vehicle and making a left-turn without presence of the traffic light, or based on the vehicle being an RHD vehicle and making a right-turn without presence of the traffic light, provide the at least one control signal to turn off the ISG function upon detecting pedestrian go information from the at least one image including the pedestrian traffic light as the vehicle makes the left-turn or the right-turn without the presence of the traffic light. 26. The apparatus according to claim 1, wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function based on the vehicle slowing down or temporarily stopping for a left-turn in a center left-turn lane. 27. The apparatus according to claim 26, wherein the at least one processor is further configured to:
detect a center left-turn lane from the at least one image; and
determine whether the vehicle is located in the center left-turn lane based on the detected center left-turn lane. 28. The apparatus according to claim 27, wherein the at least one processor is further configured to receive, through an interface unit, turn signal information, and
wherein the at least one processor is configured to provide the at least one control signal to turn off the ISG function upon receiving turn signal information for a left-turn in a state in which the center left-turn lane is detected. 29. The apparatus according to claim 1, wherein the at least one processor is further configured to receive, through an interface unit, traffic light change information acquired from an external server, and
wherein the at least one processor is further configured to provide the at least one control signal to turn off the ISG function based on the information and the traffic light change information. 30. A vehicle comprising the driver assistance apparatus according to claim 1. 31. The apparatus according to claim 1, wherein the at least one processor is further configured to receive, through interface means, navigation information, and
wherein the at least one processor is configured to provide the at least one control signal for turning off the ISG function further based on the navigation information, even when the condition is satisfied. 32. A control method of a driver assistance apparatus included in a vehicle having an Idle Stop and Go (ISG) function that is turned on based at least in part on a condition comprising a vehicle speed, whether an acceleration input has been received, and whether a brake input has been received, the method comprising:
acquiring at least one image comprising at least one of a forward image or an around view image of the vehicle via at least one camera;
detecting information from the at least one image; and
providing at least one control signal for turning off the ISG function based on the detected information, even when the condition is satisfied,
wherein the condition being satisfied comprises the brake input being received in a state in which the vehicle speed is less than or equal to a threshold speed with no acceleration input being received,
wherein the camera is configured to acquire the at least one image of at least one of a traffic light, a traffic sign, or a road surface,
wherein detecting the information from the at least one image comprises detecting, from the acquired at least one image, information regarding the at least one of the traffic light, the traffic sign, or the road surface, the information comprising ramp information regarding an on-ramp on which the vehicle travels to join a main road, wherein the on-ramp is a sloping road junction that connects a first road at a first elevation with a second road at a second elevation different from the first elevation,
wherein the method further comprises:
detecting a side portion of a second vehicle from the acquired at least one image;
determining whether the vehicle temporarily stops after entering the on-ramp based on the detected side portion of the second vehicle;
determining whether a preceding vehicle is detected from the at least one image;
determining whether the detected preceding vehicle is stationary based on a stop lamp of the detected preceding vehicle;
providing, based on the preceding vehicle being detected and the detected preceding vehicle determined as being stationary, the at least one control signal to turn on the ISG function; and
providing, based on the preceding vehicle not being detected or the detected preceding vehicle determined as being traveling, the at least one control signal to turn off the ISG function, and

wherein the method further comprises:
providing the at least one control signal to turn off the ISG function based on the vehicle being a Left Hand Drive (LHD) vehicle and slowing down or temporarily stopping for a right-turn, or based on the vehicle being a Right Hand Drive (RHD) vehicle and slowing down or temporarily stopping for a left-turn; and
providing the at least one control signal to turn off the ISG function based on detecting a pedestrian stop signal from the pedestrian traffic light detected from the at least one image. 32. A control method of a driver assistance apparatus included in a vehicle having an Idle Stop and Go (ISG) function that is turned on based at least in part on a condition comprising a vehicle speed, whether an acceleration input has been received, and whether a brake input has been received, the method comprising:
acquiring at least one image comprising at least one of a forward image or an around view image of the vehicle via at least one camera;
detecting information from the at least one image; and
providing at least one control signal for turning off the ISG function based on the detected information, even when the condition is satisfied,
wherein the condition being satisfied comprises the brake input being received in a state in which the vehicle speed is less than or equal to a threshold speed with no acceleration input being received,
wherein the camera is configured to acquire the at least one image of at least one of a traffic light, a traffic sign, or a road surface,
wherein detecting the information from the at least one image comprises detecting, from the acquired at least one image, information regarding the at least one of the traffic light, the traffic sign, or the road surface, the information comprising ramp information regarding an on-ramp on which the vehicle travels to join a main road, wherein the on-ramp is a sloping road junction that connects a first road at a first elevation with a second road at a second elevation different from the first elevation,
wherein the method further comprises:
detecting a side portion of a second vehicle from the acquired at least one image;
determining whether the vehicle temporarily stops after entering the on-ramp based on the detected side portion of the second vehicle;
determining whether a preceding vehicle is detected from the at least one image;
determining whether the detected preceding vehicle is stationary based on a stop lamp of the detected preceding vehicle;
providing, based on the preceding vehicle being detected and the detected preceding vehicle determined as being stationary, the at least one control signal to turn on the ISG function; and
providing, based on the preceding vehicle not being detected or the detected preceding vehicle determined as being traveling, the at least one control signal to turn off the ISG function, and

wherein the method further comprises:
providing the at least one control signal to turn off the ISG function based on the vehicle being a Left Hand Drive (LHD) vehicle and slowing down or temporarily stopping for a right-turn, or based on the vehicle being a Right Hand Drive (RHD) vehicle and slowing down or temporarily stopping for a left-turn; and
providing the at least one control signal to turn off the ISG function based on detecting a pedestrian stop signal from the pedestrian traffic light detected from the at least one image.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318838B2,US10318838B2,"Drawing apparatus, operation control method for drawing apparatus, and computer-readable recording medium",,"method, near, part, obliquely, region, emit, light, configured, illuminated, emitted, readable, based, nail, cuticle, data, drawing, portion, position, formed, operation, medium, object, side, computer, with, into, disposed, root, image, above, inserted, source, apparatus, acquisition, includes, shadow, between, that, being, unit, border, having, least, detects, control, finger, recording, which, from, insertion, acquire","A drawing apparatus includes an object insertion portion to which an object is inserted, the object being a finger or a toe having a nail; a light source unit configured to emit light to the nail of the object inserted into the object insertion portion; an image acquisition unit configured to acquire image data of the object with the nail illuminated by the light; and a control unit. 
     The light source unit includes at least one light source disposed obliquely above the nail so that a shadow is formed near a border between the nail and a cuticle part on the root side of the nail by the light emitted from the light source. 
     The control unit detects a position of the border based on the shadow and detects a region of the nail based on the position of the border.","1. A drawing apparatus comprising:
an object insertion receptacle into which an object is insertable in an insertion direction, the object being a finger or a toe having a nail, the object insertion receptacle having an entrance through which the object is insertable in the insertion direction;
a nail placement structure configured to have placed thereon a nail tip portion of the nail of the object inserted in the object insertion receptacle, wherein the nail placement structure has an upper surface on which the nail tip portion of the nail of the object inserted into the object insertion receptacle is to be placed, the nail placement structure having a proximal end side which faces the entrance in the insertion direction;
a printer configured to perform printing on the nail of the object when the object is inserted in the object insertion receptacle and the nail tip portion is placed on the nail placement structure;
a light source device configured to emit light to the nail of the object when the object is inserted into the object insertion receptacle;
a camera configured to acquire image data of the object with the nail illuminated by the light; and
a processor configured to detect a region of the nail as a nail region based on the image data,
wherein:
the light source device includes at least one first light source and at least one second light source,
the first light source is disposed at a first position which is located obliquely above the upper surface of the nail placement structure, and which is farther from the entrance in the insertion direction than the proximal end side of the nail placement structure,
the second light source is disposed at a second position which is located obliquely above the upper surface of the nail placement structure, which is closer to the entrance in the insertion direction than the proximal end side of the nail placement structure,
the first light source is configured to emit light obliquely downward from the first position toward the proximal end side of the nail placement structure at a first predetermined inclination angle relative to an extension direction of the upper surface of the nail placement structure, such that, in a state in which the object is inserted in the object insertion receptacle and the nail tip portion of the object is placed on the upper surface of the nail placement structure, the first light source emits light obliquely downward from a tip side of the nail toward a root side of the nail,
the second light source is configured to emit light obliquely downward from the second position toward the proximal end side of the nail placement structure at a second predetermined inclination angle relative to the extension direction of the upper surface of the nail placement structure, such that, in the state in which the object is inserted in the object insertion receptacle and the nail tip portion of the object is placed on the upper surface of the nail placement structure, the second light source emits light obliquely downward from the root side of the nail toward the tip side of the nail, and
the processor detects a position of a border between the nail and a cuticle part on the root side of the nail based on a first shadow formed by the light emitted from the first light source, detects an end position of the nail tip portion based on a second shadow formed by the light emitted from the second light source, and detects the nail region based on the detected position of the border and the detected end position of the nail tip portion. 1. A drawing apparatus comprising:
an object insertion receptacle into which an object is insertable in an insertion direction, the object being a finger or a toe having a nail, the object insertion receptacle having an entrance through which the object is insertable in the insertion direction;
a nail placement structure configured to have placed thereon a nail tip portion of the nail of the object inserted in the object insertion receptacle, wherein the nail placement structure has an upper surface on which the nail tip portion of the nail of the object inserted into the object insertion receptacle is to be placed, the nail placement structure having a proximal end side which faces the entrance in the insertion direction;
a printer configured to perform printing on the nail of the object when the object is inserted in the object insertion receptacle and the nail tip portion is placed on the nail placement structure;
a light source device configured to emit light to the nail of the object when the object is inserted into the object insertion receptacle;
a camera configured to acquire image data of the object with the nail illuminated by the light; and
a processor configured to detect a region of the nail as a nail region based on the image data,
wherein:
the light source device includes at least one first light source and at least one second light source,
the first light source is disposed at a first position which is located obliquely above the upper surface of the nail placement structure, and which is farther from the entrance in the insertion direction than the proximal end side of the nail placement structure,
the second light source is disposed at a second position which is located obliquely above the upper surface of the nail placement structure, which is closer to the entrance in the insertion direction than the proximal end side of the nail placement structure,
the first light source is configured to emit light obliquely downward from the first position toward the proximal end side of the nail placement structure at a first predetermined inclination angle relative to an extension direction of the upper surface of the nail placement structure, such that, in a state in which the object is inserted in the object insertion receptacle and the nail tip portion of the object is placed on the upper surface of the nail placement structure, the first light source emits light obliquely downward from a tip side of the nail toward a root side of the nail,
the second light source is configured to emit light obliquely downward from the second position toward the proximal end side of the nail placement structure at a second predetermined inclination angle relative to the extension direction of the upper surface of the nail placement structure, such that, in the state in which the object is inserted in the object insertion receptacle and the nail tip portion of the object is placed on the upper surface of the nail placement structure, the second light source emits light obliquely downward from the root side of the nail toward the tip side of the nail, and
the processor detects a position of a border between the nail and a cuticle part on the root side of the nail based on a first shadow formed by the light emitted from the first light source, detects an end position of the nail tip portion based on a second shadow formed by the light emitted from the second light source, and detects the nail region based on the detected position of the border and the detected end position of the nail tip portion. 2. The drawing apparatus according to claim 1, wherein the processor detects a first outline including at least an outline of the nail on the root side based on the detected position of the border,
wherein the detecting the nail region based on the detected position of the border and the detected end position of the nail tip portion includes detecting the nail region based on the first outline. 3. The drawing apparatus according to claim 1, wherein the first predetermined inclination angle at which the first light source is configured to emit light obliquely downward from the first position toward the proximal end side of the nail placement structure is an inclination angle of 5° to 45° relative to the extension direction of the upper surface of the nail placement structure. 4. The drawing apparatus according to claim 1, wherein:
the second predetermined inclination angle at which the second light source is disposed configured to emit light with obliquely downward from the second position toward the proximal end side of the nail placement structure is an inclination angle of 5° to 45° relative to the extension direction of the upper surface of the nail placement structure. 5. The drawing apparatus according to claim 1, wherein the processor is further configured to:
acquire first image data by imaging the object illuminated by the first light source with the first light source turned on and the second light source turned off,
acquire second image data by imaging the object illuminated by the second light source with the first light source turned off and the second light source turned on,
detect the position of the border based on first specific data corresponding to the first shadow in the first image data,
detect a first outline including at least an outline of the nail on the root side based on the position of the border,
detect the end position of the nail tip portion based on second specific data corresponding to the second shadow in the second image data,
detect a second outline including at least an outline of the nail on the tip side based on the end position of the nail tip portion, and
detect the nail region by synthesizing the first outline and the second outline. 6. The drawing apparatus according to claim 1,
the printer includes an inkjet print head separated from the nail for print.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318839B2,US10318839B2,Method for automatic detection of anatomical landmarks in volumetric data,,"method, first, landmark, boundaries, till, second, stacking, structure, volumetric, define, desired, coordinate, gives, locations, initially, bony, reference, matrix, distance, thresholding, based, process, identified, corresponding, data, through, empirical, consecutively, identify, initializing, used, anatomical, plurality, point, seed, contours, estimating, continues, cephalometric, obtain, searched, automatic, interest, entities, geometry, landmarks, helps, procedure, dimensions, some, estimated, detection, embodiments, mathematical, disclosed, detected, traced, vectors, further, vector, points, dimensional, three, volume, segmented, developed","Embodiments of a method for detection of plurality of three-dimensional cephalometric landmarks in volumetric data are disclosed. In some embodiments, a three-dimensional matrix is developed by stacking of volumetric data and the bony structure is segmented through thresholding. Initially a seed point is searched for initializing the process of landmark detection. Two three-dimensional distance vectors are used to define and obtain the Volume of Interest (VOI). First 3-D distance vector helps to identify Empirical Point and consecutively second gives dimensions of the VOI. Three-dimensional contours of anatomical structure are traced in the estimated VOI. Cephalometric landmarks are identified on the boundaries of traced anatomical geometry, based on corresponding Mathematical Entities. Detected landmark can be used as a Reference Point for further detection of landmarks. Estimating the VOI and detection of points continues till all desired landmarks are detected. The detection procedure gives three-dimensional coordinate locations of the landmarks.","1. A computer-implemented method for fully automatic detection of a plurality of 3D cephalometric landmarks on anatomical structures in volumetric data comprising of prior knowledge derived from mathematical entities and steps, wherein the method comprises:
receiving three dimensional model data, wherein the three dimensional model data includes one or more of: computed tomography data (CT) produced by a CT scanner, cone beam computed tomography (CBCT) data produced by a CBCT scanner, or magnetic resonance imaging (MRI) data produced by an MRI scanner generating scan volumetric data of the skull of a patient;
detecting a Reference Point in a given volume in the three dimensional model data;
estimating an Empirical Point in the given volume based on a first vector distance from the Reference Point, wherein the Empirical Point comprises a point of maximum distance from the reference point;
estimating a VOI (Volume of Interest) in the given volume based on a second vector distance from the Empirical Point, wherein the VOI comprises a subset of the given volume;
cropping the three dimensional model data for the VOI;
detecting structural three-dimensional contours in the cropped VOI using the mathematical entities applied individually on each 2D slice stacked as three dimensional model data;
detecting a landmark on one of the detected structural three-dimensional contours by performing a hierarchical search of a plurality of mathematical entities that includes:
identifying a first point based on a first mathematical entity from the Reference Point, wherein the identifying includes identifying a point on the one of the detected structural three-dimensional contours from the Reference Point in Y-axis direction as the first point;
identifying a second point based on a second mathematical entity from the first point, wherein the identifying includes identifying a point on the one of the detected structural three-dimensional contours from the first point in the Z-axis direction as the second point; and
identifying the landmark based on a third mathematical entity from the second point, wherein the identifying includes identifying a mid-point between the second point and a third point as the landmark; and
transmitting the landmark to a display for displaying the first point, the second point, and the detected landmark on the three dimensional model data to a user in order to facilitate 3-D cephalometric analysis of the anatomical structure, wherein the display is configured to display a plurality of two-dimensional slices of volumetric data stacked into a three-dimensional model and further display the first point, the second point, and the landmark within the three-dimensional model. 1. A computer-implemented method for fully automatic detection of a plurality of 3D cephalometric landmarks on anatomical structures in volumetric data comprising of prior knowledge derived from mathematical entities and steps, wherein the method comprises:
receiving three dimensional model data, wherein the three dimensional model data includes one or more of: computed tomography data (CT) produced by a CT scanner, cone beam computed tomography (CBCT) data produced by a CBCT scanner, or magnetic resonance imaging (MRI) data produced by an MRI scanner generating scan volumetric data of the skull of a patient;
detecting a Reference Point in a given volume in the three dimensional model data;
estimating an Empirical Point in the given volume based on a first vector distance from the Reference Point, wherein the Empirical Point comprises a point of maximum distance from the reference point;
estimating a VOI (Volume of Interest) in the given volume based on a second vector distance from the Empirical Point, wherein the VOI comprises a subset of the given volume;
cropping the three dimensional model data for the VOI;
detecting structural three-dimensional contours in the cropped VOI using the mathematical entities applied individually on each 2D slice stacked as three dimensional model data;
detecting a landmark on one of the detected structural three-dimensional contours by performing a hierarchical search of a plurality of mathematical entities that includes:
identifying a first point based on a first mathematical entity from the Reference Point, wherein the identifying includes identifying a point on the one of the detected structural three-dimensional contours from the Reference Point in Y-axis direction as the first point;
identifying a second point based on a second mathematical entity from the first point, wherein the identifying includes identifying a point on the one of the detected structural three-dimensional contours from the first point in the Z-axis direction as the second point; and
identifying the landmark based on a third mathematical entity from the second point, wherein the identifying includes identifying a mid-point between the second point and a third point as the landmark; and
transmitting the landmark to a display for displaying the first point, the second point, and the detected landmark on the three dimensional model data to a user in order to facilitate 3-D cephalometric analysis of the anatomical structure, wherein the display is configured to display a plurality of two-dimensional slices of volumetric data stacked into a three-dimensional model and further display the first point, the second point, and the landmark within the three-dimensional model. 2. The method of claim 1, further comprising providing a volumetric template for detecting the first reference point called seed point which includes hard-tissues and teeth of lower mandible jaw and matching the template with the VOI to identify the seed point uniquely in a patient's three dimensional model data. 3. The method of claim 1, further comprising automatic detection of the Reference Point in three dimensional volumetric data. 4. The method of claim 1, further comprising estimating the Empirical Point using the first vector distance from the Reference Point for detection of the VOI. 5. The method of claim 1, further comprising estimating the VOI using the second vector distance from the Empirical Point. 6. The method of claim 1, further comprising detecting contours by traversing of the VOI through Sagittal, Coronal and Axial plane in the direction of X-axis, Y-axis and Z-axis respectively. 7. The method of claim 1, further comprising detecting contours by traversing of the VOI through the combination of Sagittal, Coronal and Axial plane in the direction of X-axis, Y-axis and Z-axis respectively. 8. The method of claim 1, wherein the detecting of the 3D contours uses locus of prominent points on stacked planes. 9. The method of claim 1, wherein the detecting of the 3D contours uses boundary point detection on each stacked plane of the VOI. 10. The method of claim 1, further comprising detecting contours on a projected XY-plane, a projected YZ-plane and a projected XZ-plane of the VOI where coordinates of a remaining dimension such as Z, X and Y can be zero respectively. 11. The method of claim 1, further comprising detecting a landmark which includes detecting the landmark on plane based on the contours followed by detection of corresponding third coordinate. 12. The method of claim 1, wherein detecting the structural three-dimensional contours comprises automatically detecting of a plurality of three-dimensional points by traversing of volumetric data through Sagittal, Coronal and Axial plane. 13. The method of claim 12, wherein detecting the structural three-dimensional contours further comprises detecting of a plurality of three-dimensional points by traversing of volumetric data through Sagittal, Coronal and Axial plane to detect a cephalometric anatomical curve. 14. The method of claim 1, further comprising automatically detecting of a plurality of three-dimensional points on the detected contours of an anatomical structure. 15. The method of claim 1, wherein each of the first, second, and third mathematical entities comprise at least one of:
detection of a peak point among a group of contour points;
detection of a deepest point among the group of contour points;
computing a mid-point of already detected two reference points;
detecting a point of inflection among corresponding contour Points;
determining a point among the group of contour points with a minimum slope made with a reference point;
determining a point among the group of contour points with a maximum slope made with a reference point;
determining a centroid of a contour from the group of contour points;
determining a junction point of a plurality of contours;
determining a point among contour points which is nearest to the reference point; or
determining a point among contour points which is farthest to the reference point. 16. The method of claim 1, wherein detecting the landmark comprises dividing a volumetric data in the VOI to generate a subset of the volumetric data. 17. The method of claim 1, detecting the structural three-dimensional contours comprises dividing a volumetric data in the VOI to generate a subset of the volumetric data. 18. The method of claim 1, wherein the Empirical Point comprises a point of maximum distance from the reference point not including the landmark of the given volume. 19. The method of claim 1, wherein the method is performed without using a training set.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318840B2,US10318840B2,Generating a group photo collection,2012-05-17,"method, themes, receiving, create, based, collaborate, album, interface, patterns, enabling, providing, creating, implementations, generally, plurality, group, with, where, network, users, includes, specified, photo, some, also, collection, more, objects, determining, recommendations, photos, associated, social, system, generating, relate, recognized","Implementations generally relate to generating a group photo collection. In some implementations, a method includes determining a plurality of users in a specified group of users of a social network system. The method also includes receiving photos associated with the users. The method also includes providing an interface enabling the plurality of users to collaborate in creating a group photo collection, where the group photo collection includes the plurality of photos. The method also includes providing one or more recommendations to create a photo album based on one or more themes, where the one or more themes are based on patterns of objects recognized in the plurality of photos.","1. A method comprising:
receiving respective photos from a user device of each of a plurality of users;
providing a shared interface to each of the user devices to create a collaborative photo collection of a plurality of photos of the respective photos, wherein at least specific photos of the plurality of photos are associated with an event;
analyzing visual content of the respective photos using one or more visual content recognition algorithms or matching algorithms to detect a pattern of at least one of a color, at least one object, or at least one word in the respective photos;
determining one or more event themes of two or more of the specific photos of the plurality of photos based on the pattern of the at least one of the color, at least one object, or at least one word, determined to be in the specific photos, wherein the one or more event themes indicate a context for the visual content of the respective photos including one or more activities associated with the event;
determining an event category based, at least in part, on a time span of the two or more specific photos that are associated with the one or more event themes;
recommending to each of the plurality of users, to cluster, based on the event category, the two or more specific photos of the plurality of photos that are associated with the one or more event themes, into one or more photo clusters;
receiving at least one user input through the shared interface; and
generating the collaborative photo collection that includes the one or more photo clusters, according to the at least one user input. 1. A method comprising:
receiving respective photos from a user device of each of a plurality of users;
providing a shared interface to each of the user devices to create a collaborative photo collection of a plurality of photos of the respective photos, wherein at least specific photos of the plurality of photos are associated with an event;
analyzing visual content of the respective photos using one or more visual content recognition algorithms or matching algorithms to detect a pattern of at least one of a color, at least one object, or at least one word in the respective photos;
determining one or more event themes of two or more of the specific photos of the plurality of photos based on the pattern of the at least one of the color, at least one object, or at least one word, determined to be in the specific photos, wherein the one or more event themes indicate a context for the visual content of the respective photos including one or more activities associated with the event;
determining an event category based, at least in part, on a time span of the two or more specific photos that are associated with the one or more event themes;
recommending to each of the plurality of users, to cluster, based on the event category, the two or more specific photos of the plurality of photos that are associated with the one or more event themes, into one or more photo clusters;
receiving at least one user input through the shared interface; and
generating the collaborative photo collection that includes the one or more photo clusters, according to the at least one user input. 2. The method of claim 1, wherein the shared interface enables the plurality of users to label and modify the collaborative photo collection. 3. The method of claim 1, further comprising ordering the specific photos in the one or more photo clusters in the collaborative photo collection, based, at least in part, on the one or more event themes. 4. The method of claim 1, further comprising grouping the photo clusters according to the event. 5. The method of claim 4, wherein the event is determined based on a respective capture time of the specific photos in the photo clusters. 6. The method of claim 1, wherein the color includes at least one dominant color determined to be in the specific photos. 7. The method of claim 1, further comprising enabling each user of the plurality of users to add other users to the plurality of users. 8. A non-transitory computer-readable medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
receiving respective photos from a user device of each of a plurality of users;
providing a shared interface to each of the user devices to create a collaborative photo collection of a plurality of photos of the respective photos, wherein the shared interface enables the plurality of users to perform at least one of edit at least one photo of the plurality of photos, delete at least one photo of the plurality of photos, or provide captions for at least one photo of the plurality of photos, wherein at least specific photos of the plurality of photos are associated with an event;
analyzing visual content of the respective photos using one or more visual content recognition algorithms or matching algorithms to detect a pattern of at least one of a color, at least one object, or at least one word in the respective photos;
determining one or more event themes of two or more of the specific photos of the plurality of photos based on the pattern of the at least one of the color, at least one object, or at least one word, determined to be in the specific photos, wherein the one or more event themes indicate a context for the visual content of the respective photos including one or more activities associated with the event;
determining an event category based, at least in part, on a time span of the two or more specific photos that are associated with the one or more event themes;
recommending to each of the plurality of users, to cluster, based on the event category, the two or more specific photos of the plurality of photos that are associated with the one or more event themes, into one or more photo clusters by providing user input with the shared interface;
receiving at least one user input; and
generating the collaborative photo collection that includes the one or more photo clusters, according to the at least one user input. 8. A non-transitory computer-readable medium storing instructions that, when executed by one or more processors, cause the one or more processors to perform operations comprising:
receiving respective photos from a user device of each of a plurality of users;
providing a shared interface to each of the user devices to create a collaborative photo collection of a plurality of photos of the respective photos, wherein the shared interface enables the plurality of users to perform at least one of edit at least one photo of the plurality of photos, delete at least one photo of the plurality of photos, or provide captions for at least one photo of the plurality of photos, wherein at least specific photos of the plurality of photos are associated with an event;
analyzing visual content of the respective photos using one or more visual content recognition algorithms or matching algorithms to detect a pattern of at least one of a color, at least one object, or at least one word in the respective photos;
determining one or more event themes of two or more of the specific photos of the plurality of photos based on the pattern of the at least one of the color, at least one object, or at least one word, determined to be in the specific photos, wherein the one or more event themes indicate a context for the visual content of the respective photos including one or more activities associated with the event;
determining an event category based, at least in part, on a time span of the two or more specific photos that are associated with the one or more event themes;
recommending to each of the plurality of users, to cluster, based on the event category, the two or more specific photos of the plurality of photos that are associated with the one or more event themes, into one or more photo clusters by providing user input with the shared interface;
receiving at least one user input; and
generating the collaborative photo collection that includes the one or more photo clusters, according to the at least one user input. 9. The computer-readable medium of claim 8, wherein the shared interface further enables the plurality of users to label and modify the collaborative photo collection. 10. The computer-readable medium of claim 8, wherein the operations further comprise ordering the specific photos in the one or more photo clusters in the collaborative photo collection, based, at least in part, on the one or more event themes. 11. The computer-readable medium of claim 8, wherein the operations further comprise grouping the photo clusters according to the event. 12. The computer-readable medium of claim 11, wherein the event is determined based on a respective capture time of the specific photos in the photo clusters. 13. The computer-readable medium of claim 8, wherein the color includes at least one dominant color determined to be in the specific photos. 14. The computer-readable medium of claim 8, wherein the operations further comprise enabling each user of the plurality of users to add other users to the plurality of users. 15. A system comprising:
one or more processors; and
one or more computer-readable media having instructions stored thereon that, when executed by the one or more processors, cause performance of operations comprising:
receiving respective photos from a user device of each of a plurality of users;
providing a shared interface to each of the user devices to create a collaborative photo collection of a plurality of photos of the respective photos, wherein at least specific photos of the plurality of photos are associated with an event;
analyzing visual content of the respective photos using one or more visual content recognition algorithms or matching algorithms to detect a pattern of at least one of a color, at least one object, or at least one word in the respective photos;
determining one or more event themes of two or more of the specific photos of the plurality of photos based on the pattern of the at least one of color, at least one object, or at least one word, determined to be in the specific photos, wherein the one or more event themes indicate a context for the visual content of the respective photos including one or more activities associated with at least one respective event;
determining an event category based, at least in part, on a time span of the two or more specific photos that are associated with the one or more event themes;
recommending to each of the plurality of users, to cluster, based on the event category, the two or more specific photos of the plurality of photos that are associated with the one or more event themes, into one or more photo clusters by providing user input with the shared interface;
receiving at least one user input; and
generating the collaborative photo collection that includes the one or more photo clusters, according to the at least one user input. 15. A system comprising:
one or more processors; and
one or more computer-readable media having instructions stored thereon that, when executed by the one or more processors, cause performance of operations comprising:
receiving respective photos from a user device of each of a plurality of users;
providing a shared interface to each of the user devices to create a collaborative photo collection of a plurality of photos of the respective photos, wherein at least specific photos of the plurality of photos are associated with an event;
analyzing visual content of the respective photos using one or more visual content recognition algorithms or matching algorithms to detect a pattern of at least one of a color, at least one object, or at least one word in the respective photos;
determining one or more event themes of two or more of the specific photos of the plurality of photos based on the pattern of the at least one of color, at least one object, or at least one word, determined to be in the specific photos, wherein the one or more event themes indicate a context for the visual content of the respective photos including one or more activities associated with at least one respective event;
determining an event category based, at least in part, on a time span of the two or more specific photos that are associated with the one or more event themes;
recommending to each of the plurality of users, to cluster, based on the event category, the two or more specific photos of the plurality of photos that are associated with the one or more event themes, into one or more photo clusters by providing user input with the shared interface;
receiving at least one user input; and
generating the collaborative photo collection that includes the one or more photo clusters, according to the at least one user input. 16. The system of claim 15, wherein the operations further comprise enabling each user of the plurality of users to add other users to the plurality of users. 17. The system of claim 15, wherein the operations further comprise ordering the specific photos in the one or more photo clusters in the collaborative photo collection, based, at least in part, on the one or more event themes. 18. The system of claim 15, wherein the operations further comprise grouping the photo clusters according to the event. 19. The system of claim 18, wherein the event is determined based on a respective capture time of the specific photos in the photo clusters. 20. The system of claim 15, wherein the color includes at least one dominant color determined to be in the specific photos.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318844B2,US10318844B2,Detection and presentation of differences between 3D models,2016-03-21,"method, second, product, yield, implemented, receiving, model, based, list, corresponding, comparing, program, wherein, identify, have, object, scanning, computer, each, models, between, includes, common, sorting, also, differences, detection, more, objects, rules, least, disclosed, represents, presentation, further, system, first","A computer-implemented method includes receiving a first 3D model and a second 3D model, wherein the first 3D model represents a first set of objects and the second 3D model represents includes a second set of objects. The computer-implemented method further includes scanning each of the first 3D model and the second 3D model to identify the first set of objects and the second set of objects, wherein the first set of objects and the second set of objects have at least one common object. The computer-implemented method further includes comparing the first set of objects to the second set of objects to yield one or more differences. The computer-implemented method further includes sorting each of the one or more differences based on a set of rules to yield a list of differences. A corresponding computer system and computer program product are also disclosed.","1. A computer-implemented method comprising:
reconstructing a first set of 2D images and a second set of 2D images into a first 3D model and a second 3D model, respectively;
scanning each of said first 3D model and said second 3D model to identify a first set of objects and a second set of objects, the first set of objects and the second set of objects having a plurality of common objects;
determining a plurality of differences between the first set of objects and the second set of objects based on calculating a measurement of change of an attribute of a common object included in the first set of objects and the second set of objects; and
sorting the plurality of differences to yield a list of differences, wherein sorting the plurality of differences is based, at least in part, on a weighted value associated with the attribute of the common object, wherein:
the weighted value associated with the attribute of the common object is adjusted based, at least in part, on learning a pattern of an accepted range of values of the measurement of change for the attribute of the common object over a predetermined period of time. 1. A computer-implemented method comprising:
reconstructing a first set of 2D images and a second set of 2D images into a first 3D model and a second 3D model, respectively;
scanning each of said first 3D model and said second 3D model to identify a first set of objects and a second set of objects, the first set of objects and the second set of objects having a plurality of common objects;
determining a plurality of differences between the first set of objects and the second set of objects based on calculating a measurement of change of an attribute of a common object included in the first set of objects and the second set of objects; and
sorting the plurality of differences to yield a list of differences, wherein sorting the plurality of differences is based, at least in part, on a weighted value associated with the attribute of the common object, wherein:
the weighted value associated with the attribute of the common object is adjusted based, at least in part, on learning a pattern of an accepted range of values of the measurement of change for the attribute of the common object over a predetermined period of time. 2. The computer-implemented method of claim 1, further comprising:
ignoring a difference between the attribute of the common object based, at least in part, on the measurement of change of the attribute of the common object falling below a given threshold value. 3. The computer-implemented method of claim 1, further comprising:
sorting the plurality of differences based on a relevance score, wherein the relevance score is calculated by multiplying the measurement of change by the weighted value of the attribute. 4. A computer program product, the computer program product comprising one or more computer readable storage media and program instructions stored on the one or more computer readable storage media, the program instructions comprising instructions to:
reconstruct a first set of 2D images and a second set of 2D images into a first 3D model and a second 3D model, respectively;
scan each of said first 3D model and said second 3D model to identify a first set of objects and a second set of objects, said first set of objects and said second set of objects having a plurality of common objects;
determine a plurality of differences between said first set of objects and said second set of objects based on calculating a measurement of change of an attribute of a common object included in the first set of objects and the second set of objects; and
sort the plurality of differences to yield a list of differences, wherein the instructions to sort the plurality of differences is based, at least in part, on a weighted value associated with the attribute of the common object, wherein:
the weighted value associated with the attribute of the common object is adjusted based, at least in part, on instructions to learn a pattern of an accepted range of values of the measurement of change for the attribute of the common object over a predetermined period of time. 4. A computer program product, the computer program product comprising one or more computer readable storage media and program instructions stored on the one or more computer readable storage media, the program instructions comprising instructions to:
reconstruct a first set of 2D images and a second set of 2D images into a first 3D model and a second 3D model, respectively;
scan each of said first 3D model and said second 3D model to identify a first set of objects and a second set of objects, said first set of objects and said second set of objects having a plurality of common objects;
determine a plurality of differences between said first set of objects and said second set of objects based on calculating a measurement of change of an attribute of a common object included in the first set of objects and the second set of objects; and
sort the plurality of differences to yield a list of differences, wherein the instructions to sort the plurality of differences is based, at least in part, on a weighted value associated with the attribute of the common object, wherein:
the weighted value associated with the attribute of the common object is adjusted based, at least in part, on instructions to learn a pattern of an accepted range of values of the measurement of change for the attribute of the common object over a predetermined period of time. 5. The computer program product of claim 4, further comprising instructions to:
ignore a difference between the attribute of the common object based, at least in part, on the measurement of change of the attribute of the common object falling below a given threshold value. 6. The computer program product of claim 4, further comprising instructions to:
sort the plurality of differences based on a relevance score, wherein the relevance score is calculated by multiplying the measurement of change by the weighted value of the attribute. 7. A computer system, the computer system comprising:
one or more computer processors;
one or more computer readable storage media;
computer program instructions;
the computer program instructions being stored on the one or more computer readable storage media;
the computer program instructions comprising instructions to:
reconstruct a first set of 2D images and a second set of 2D images into a first 3D model and a second 3D model, respectively;
scan each of said first 3D model and said second 3D model to identify a first set of objects and a second set of objects, said first set of objects and said second set of objects having a plurality of common objects;
determine a plurality of differences between said first set of objects and said second set of objects based on calculating a measurement of change of an attribute of a common object included in the first set of objects and the second set of objects; and
sort the plurality of differences to yield a list of differences, wherein the instructions to sort the plurality of differences is based, at least in part, on a weighted value associated with the attribute of the common object, wherein:
the weighted value associated with the attribute of the common object is adjusted based, at least in part, on instructions to learn a pattern of an accepted range of values of the measurement of change for the attribute of the common object over a predetermined period of time. 7. A computer system, the computer system comprising:
one or more computer processors;
one or more computer readable storage media;
computer program instructions;
the computer program instructions being stored on the one or more computer readable storage media;
the computer program instructions comprising instructions to:
reconstruct a first set of 2D images and a second set of 2D images into a first 3D model and a second 3D model, respectively;
scan each of said first 3D model and said second 3D model to identify a first set of objects and a second set of objects, said first set of objects and said second set of objects having a plurality of common objects;
determine a plurality of differences between said first set of objects and said second set of objects based on calculating a measurement of change of an attribute of a common object included in the first set of objects and the second set of objects; and
sort the plurality of differences to yield a list of differences, wherein the instructions to sort the plurality of differences is based, at least in part, on a weighted value associated with the attribute of the common object, wherein:
the weighted value associated with the attribute of the common object is adjusted based, at least in part, on instructions to learn a pattern of an accepted range of values of the measurement of change for the attribute of the common object over a predetermined period of time. 8. The computer system of claim 7, further comprising instructions to:
ignore a difference between the attribute of the common object based, at least in part, on the measurement of change of the attribute of the common object falling below a given threshold value. 9. The computer system of claim 7, further comprising instructions to:
sort the plurality of differences based on a relevance score, wherein the relevance score is calculated by multiplying the measurement of change by the weighted value of the attribute.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318845B2,US10318845B2,Coupon reader,2016-04-14,"method, reading, camera, profile, mixture, test, channel, device, digital, best, this, improvement, exposure, exposing, readable, representation, interface, perceptible, pixel, match, exceeds, intensity, providing, response, similarity, indication, computer, coupon, placing, passing, displays, image, substance, reader, using, take, user, includes, that, being, memory, automatically, finding, pixels, fluid, contiguous, determining, having, target, uses, exposed, human, detected, pattern, after, matches, threshold, assembly, from, been, section, logic","A method of reading a coupon channel that displays a test section pattern after being exposed to a target substance, the method uses a device having a computer readable memory, digital camera, logic assembly and user interface; providing a pixel target intensity profile; placing the coupon in the device and exposing the coupon channel to a test fluid mixture; automatically using the digital camera to take a digital image of the coupon channel test section after the exposure. The improvement in the method includes finding the contiguous set of pixels from the test section of the coupon channel that best matches the intensity profile of the target pattern representation and determining if this best match set of pixels exceeds a similarity threshold and in response to a best match set of pixels passing the similarity threshold, automatically providing a human perceptible indication that the target substance has been detected.","1. A method of reading a coupon channel that displays a test pattern in a test section after being exposed to a target substance, said method includes providing a device having a computer readable memory, a digital camera, a logic assembly and a user interface; providing a color-ratio pixel target pattern representation, representative of an exposed and at least partially developed coupon channel control section pattern; placing said color-ratio pixel target pattern into said memory; placing said coupon in said device and exposing said coupon channel to a test fluid mixture; automatically using said digital camera to take a digital image of said coupon channel test section after said exposure, said digital image formed of color pixels, each containing at least two sub-pixels representing the intensity of two different colors; and
a. wherein the improvement in said method includes for each color pixel forming a color-ratio pixel by forming the ratio of a first one of said two subpixels with the other, and automatically using said logic assembly to compare said color-ratio pixels in said test section as shown in said digital image to said color-ratio pixel target pattern representation to determine if said test section passes a similarity threshold; and in response to a test section passing said similarity threshold, automatically providing a human perceptible indication that the target substance has been detected. 1. A method of reading a coupon channel that displays a test pattern in a test section after being exposed to a target substance, said method includes providing a device having a computer readable memory, a digital camera, a logic assembly and a user interface; providing a color-ratio pixel target pattern representation, representative of an exposed and at least partially developed coupon channel control section pattern; placing said color-ratio pixel target pattern into said memory; placing said coupon in said device and exposing said coupon channel to a test fluid mixture; automatically using said digital camera to take a digital image of said coupon channel test section after said exposure, said digital image formed of color pixels, each containing at least two sub-pixels representing the intensity of two different colors; and
a. wherein the improvement in said method includes for each color pixel forming a color-ratio pixel by forming the ratio of a first one of said two subpixels with the other, and automatically using said logic assembly to compare said color-ratio pixels in said test section as shown in said digital image to said color-ratio pixel target pattern representation to determine if said test section passes a similarity threshold; and in response to a test section passing said similarity threshold, automatically providing a human perceptible indication that the target substance has been detected. 2. The method of claim 1, wherein said coupon channel includes a control section that develops a target pattern after exposure to said test fluid mixture, without regard to the presence of said target substance in said test fluid mixture, and wherein said color-ratio pixel target pattern representation is provided by digital imagery from said digital image of said target pattern in said control section, and further processing in which said color-ratio pixel values are computed and wherein said providing a color-ratio pixel target pattern representation occurs after said exposing said coupon channel to a test fluid mixture. 3. The method of claim 1, wherein said color-ratio pixel target pattern representation is provided by taking a color digital photograph, having color pixels, of a coupon channel that has been exposed to said target substance after a predetermined development time has elapsed since said exposure and computing said color-ratio pixels from said color pixels. 4. The method of claim 1, wherein said threshold is user adjustable and further including the initial step of adjusting said threshold. 5. The method of claim 1, wherein said coupon channel is hosted on a coupon having a single channel only. 6. The method of claim 1, wherein said coupon channel is hosted on a coupon having multiple channels.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318843B2,US10318843B2,Method and apparatus for image processing and comparison based on spatial relationships between image features,2016-03-30,"method, multiple, identifying, device, spatial, proximities, images, persons, locations, including, captured, features, based, process, interface, physical, proximity, relationships, systems, remote, similarity, updates, processes, determine, operatively, computer, each, measure, communicatively, circuitry, generates, image, different, identities, apparatus, item, between, includes, measured, that, memory, processing, communication, provided, also, storage, comparison, capture, server, having, determining, coupled, identifies, acquires, associated, system, from, pair, items","An apparatus that includes a computer server having processing circuitry operatively coupled to a memory and a communication interface is provided. A remote image capture device is communicatively coupled to the computer server via the communication interface. The apparatus acquires a captured image of a storage system from the image capture device. The apparatus identifies a pair of physical items in the image, including determining identities of each item of the pair of physical items, and identifying locations in the storage system of each physical item of the pair of physical items. The apparatus processes the image to measure a physical proximity between the pair of physical items, and generates or updates a measure of similarity between the pair of physical items based on the measured physical proximity. The apparatus can also process multiple images to determine similarity between different storage systems or associated persons based on item proximities.","1. An apparatus comprising:
circuitry configured to
acquire a captured image of a storage system via a network from a remote image capture device;
identify a pair of physical items in the image, including determining identities of each item of the pair of physical items, and identifying locations in the storage system of each physical item of the pair of physical items;
process the image to measure a physical proximity between the pair of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generate or update a measure of similarity between the pair of physical items based on the measured physical proximity. 1. An apparatus comprising:
circuitry configured to
acquire a captured image of a storage system via a network from a remote image capture device;
identify a pair of physical items in the image, including determining identities of each item of the pair of physical items, and identifying locations in the storage system of each physical item of the pair of physical items;
process the image to measure a physical proximity between the pair of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generate or update a measure of similarity between the pair of physical items based on the measured physical proximity. 2. The apparatus of claim 1, further configured to:
receive, from the remote image capture device or another remote image capture device, one or more further images of one or more further respective storage systems;
identify other instances of the pair of physical items in each of the further images, including determining said identities of each physical item of the pair of physical items, and identifying locations, in each of the further storage systems, of each physical item of the instance of the pair of physical items;
process each of the further images to measure respective further physical proximities between the instance of the pair of physical items as occurring in each of the further storage systems; and
update the measure of similarity between the pair of physical items based on the determined further physical proximities. 3. The apparatus of claim 1, wherein the physical items are books, the storage system is a bookshelf, and the identities comprise book titles. 4. The apparatus of claim 3, further configured to recommend, via a user interface of the remote image capture device or another device, books to a user based on the measure of similarity. 5. The apparatus of claim 4, wherein the user has indicated interest in one of the pair of books, and wherein recommending books to the user comprises recommending other of the pair of books to the user with a probability or recommendation strength which increases with the measure of similarity between the pair of books. 6. The apparatus of claim 1, wherein the measure of similarity is a nondecreasing function of a number of observed images of storage systems in which instances of the pair of physical items are identified, and is further a nondecreasing function of the measured physical proximity of instances of the pair of physical items in each of said observed images of storage systems in which instances of the pair of physical items are identified. 7. The apparatus of claim 1, wherein the measure of similarity is a weighted sum, and wherein generating or updating the measure of similarity comprises initializing or updating the weighted sum by adding a term thereto, the term having a numerical value which increases with the measured physical proximity, wherein the weighted sum is reflective of past and current observations of storage systems holding the pair of physical items. 8. The apparatus of claim 1, further comprising or operatively coupled to a database, the apparatus further configured to store, in the database, one or both of the measure of similarity and the measure of proximity, and provide, based on data stored in the database, information indicative of one or more of: similarities between physical items, similarities between storage systems, and similarities between persons associated with storage systems. 9. An apparatus for measuring similarities between storage systems or persons associated therewith, the storage systems containing collections of physical items, the apparatus comprising circuitry configured to:
direct capturing of images of two storage systems using one or more remote image capture devices;
receive the images of the two storage systems;
for each one of the images:
identify one or more pairs of physical items in the image, including determining identities of each physical item of each of the pairs of physical items, and identifying locations in the storage system of each physical item of each of the pairs of physical items;
process the image to measure physical proximities between physical items of each of the pairs of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generate or update a measure of similarity between the storage systems or persons associated therewith, based on the physical proximities. 9. An apparatus for measuring similarities between storage systems or persons associated therewith, the storage systems containing collections of physical items, the apparatus comprising circuitry configured to:
direct capturing of images of two storage systems using one or more remote image capture devices;
receive the images of the two storage systems;
for each one of the images:
identify one or more pairs of physical items in the image, including determining identities of each physical item of each of the pairs of physical items, and identifying locations in the storage system of each physical item of each of the pairs of physical items;
process the image to measure physical proximities between physical items of each of the pairs of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generate or update a measure of similarity between the storage systems or persons associated therewith, based on the physical proximities. 10. The apparatus of claim 9, further comprising or operatively coupled to a database and configured to:
store, in the database: indications of a plurality of the storage systems or persons associated therewith; and measures of similarities between pairs of the plurality of storage systems or persons associated therewith; and
upon receiving an indication of a first one of the plurality of storage systems or persons associated therewith, provide an indication of another of the plurality of storage systems or persons associated therewith having a relatively high measured similarity to said first one of the plurality of storage systems or persons associated therewith. 11. The apparatus of claim 9, wherein the measure of similarity is a nondecreasing function of a number of instances of pairs of physical items which occur in both of the two storage systems, and is further a nondecreasing function of similarity between the measured physical proximities between said instances of pairs of physical items which occur in both of the two storage systems. 12. The apparatus of claim 9, wherein the physical items are books, the storage systems are bookshelves, and the identities comprise book titles. 13. The apparatus of claim 9, wherein the measure of similarity is generated by computing a weighted sum over all of said one or more pairs of items appearing in the images of both of the two storage systems, each term in the weighted sum having a numerical value which is nondecreasing with measured physical proximity between a corresponding one of said pairs of items. 14. A method for automatically measuring similarities between physical items comprising:
acquiring an image of a storage system via a network from a remote image capture device;
identifying, using processing circuitry, a pair of physical items in the image, including determining identities of each item of the pair of physical items, and identifying locations in the storage system of each physical item of the pair of physical items;
processing, using the processing circuitry, the image to measure a physical proximity between the pair of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generating or updating, using the processing circuitry, a measure of similarity between the pair of physical items based on the measured physical proximity. 14. A method for automatically measuring similarities between physical items comprising:
acquiring an image of a storage system via a network from a remote image capture device;
identifying, using processing circuitry, a pair of physical items in the image, including determining identities of each item of the pair of physical items, and identifying locations in the storage system of each physical item of the pair of physical items;
processing, using the processing circuitry, the image to measure a physical proximity between the pair of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generating or updating, using the processing circuitry, a measure of similarity between the pair of physical items based on the measured physical proximity. 15. The method of claim 14, further comprising:
receiving one or more further images of one or more further respective storage systems;
identifying other instances of the pair of physical items in each of the further images, including determining said identities of each physical item of the pair of physical items, and identifying locations, in each of the further storage systems, of each physical item of the instance of the pair of physical items;
processing each of the further images to measure respective further physical proximities between the instance of the pair of physical items as occurring in each of the further storage systems; and
updating the measure of similarity between the pair of physical items based on the determined further physical proximities. 16. The method of claim 14, wherein the physical items are books, the storage system is a bookshelf, and the identities comprise book titles. 17. The method of claim 16, further comprising recommending books to a user based on the measure of similarity. 18. The method of claim 17, wherein the user has indicated interest in one of the pair of books, and wherein recommending books to the user comprises recommending other of the pair of books to the user with a probability or recommendation strength which increases with the measure of similarity between the pair of books. 19. The method of claim 14, wherein the measure of similarity is a nondecreasing function of a number of observed images of storage systems in which instances of the pair of physical items are identified, and is further a nondecreasing function of the measured physical proximity of instances of the pair of physical items in each of said observed images of storage systems in which instances of the pair of physical items are identified. 20. The method of claim 14, further comprising storing, in a database, one or both of the measure of similarity and the measure of proximity, and providing, based on data stored in the database, information indicative of one or more of: similarities between physical items, similarities between storage systems, and similarities between persons associated with storage systems. 21. A method for measuring similarities between storage systems or persons associated therewith, the storage systems containing collections of physical items, the method comprising:
acquiring images of two storage systems via a network from one or more remote image capture devices;
for each one of the images:
identifying, using processing circuitry, one or more pairs of physical items in the image, including determining identities of each physical item of each of the pairs of physical items, and identifying locations in the storage system of each physical item of each of the pairs of physical items;
processing the image, using the processing circuitry, to measure physical proximities between physical items of each of the pairs of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generating or updating, using the processing circuitry, a measure of similarity between the storage systems or persons associated therewith, based on the physical proximities. 21. A method for measuring similarities between storage systems or persons associated therewith, the storage systems containing collections of physical items, the method comprising:
acquiring images of two storage systems via a network from one or more remote image capture devices;
for each one of the images:
identifying, using processing circuitry, one or more pairs of physical items in the image, including determining identities of each physical item of each of the pairs of physical items, and identifying locations in the storage system of each physical item of each of the pairs of physical items;
processing the image, using the processing circuitry, to measure physical proximities between physical items of each of the pairs of physical items, the physical proximity between the pair of physical items being based on a distance between each item of the pair of physical items, the distance between a first physical item and a second physical item of the pair of physical items being based on a number of physical items separating the first physical item and the second physical item; and
generating or updating, using the processing circuitry, a measure of similarity between the storage systems or persons associated therewith, based on the physical proximities. 22. The method of claim 21, further comprising:
storing, in a database: indications of a plurality of storage systems or persons associated therewith; and measures of similarities between pairs of the plurality of storage systems or persons associated therewith; and
upon receiving an indication of a first one of the plurality of storage systems or persons associated therewith, providing an indication of another of the plurality of storage systems or persons associated therewith having a relatively high measured similarity to said first one of the plurality of storage systems or persons associated therewith. 23. The method of claim 21, wherein the physical items are books, the storage systems are bookshelves, and the identities comprise book titles.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318841B2,US10318841B2,"Medical-image processing apparatus, ultrasonic diagnostic apparatus, and medical-image processing method",,"method, differentiating, included, based, corresponding, data, acquired, ultrasonic, updates, value, with, circuitry, image, corresponds, initial, apparatus, outline, includes, subject, that, processing, respect, diagnostic, medical, embodiment, cost, function, derivative, acquires, vector, according, circuit","A medical-image processing apparatus according to an embodiment includes processing circuitry. The processing circuit acquires an initial value of an outline corresponding vector that corresponds to an outline of a subject included in medical image data. The processing circuitry updates the outline corresponding vector based on a derivative that is acquired by differentiating a cost function with respect to the outline corresponding vector by the outline corresponding vector, and on the initial value of the outline corresponding vector.","1. A medical-image processing apparatus, comprising:
processing circuitry configured to
acquire an initial value of an outline corresponding vector that corresponds to an outline of a subject included in medical image data of a heart,
update the outline corresponding vector based on a derivative that is acquired by differentiating a cost function with respect to the outline corresponding vector by the outline corresponding vector, and based on the initial value of the outline corresponding vector, and
display, on a display, a medical image expressing the updated outline. 1. A medical-image processing apparatus, comprising:
processing circuitry configured to
acquire an initial value of an outline corresponding vector that corresponds to an outline of a subject included in medical image data of a heart,
update the outline corresponding vector based on a derivative that is acquired by differentiating a cost function with respect to the outline corresponding vector by the outline corresponding vector, and based on the initial value of the outline corresponding vector, and
display, on a display, a medical image expressing the updated outline. 2. The medical-image processing apparatus according to claim 1, wherein
the processing circuitry is further configured to calculate a gradient direction vector expressing a direction toward which the cost function decreases, by substituting the outline corresponding vector into the derivative, and update the outline corresponding vector by using the calculated gradient direction vector. 3. The medical-image processing apparatus according to claim 1, wherein the processing circuitry is further configured to
determine whether a degree of deformation accompanied by updating of the outline has converged based on convergence information of the outline corresponding vector to be updated,
output, when determining that the degree of the deformation has converged, information relating to the outline by using the outline corresponding vector to be updated, and
update, when determining that the degree of the deformation has not converged yet, the outline corresponding vector again. 4. The medical-image processing apparatus according to claim 1, wherein
the outline corresponding vector is any one of an outline vector expressing the outline, and a vector that is expressed by a coefficient acquired by principal component analysis of the outline vector. 5. The medical-image processing apparatus according to claim 1, wherein the processing circuitry is further configured to
acquire an initial value of an outline vector expressing the outline, and
update the outline vector based on a derivative acquired by differentiating a cost function with respect to the outline vector by a coefficient acquired by principal component analysis of the outline vector. 6. The medical-image processing apparatus according to claim 1, wherein
the processing circuitry is further configured to accept an operation to specify at least one coordinate value among coordinate values of an apex, a heart valve annulus, an edge line of an inner boundary of a myocardium, and an edge line of an outer boundary of the myocardium, and acquire the initial value based on the specified at least one coordinate value. 7. The medical-image processing apparatus according to claim 1, wherein
the processing circuitry is further configured to acquire an outline vector of the outline, a degree of the deformation of which has converged in a frame out of a plurality of frames of a moving image of the subject imaged therein, as the initial value of the outline vector. 8. The medical-image processing apparatus according to claim 1, wherein
the subject includes at least one of a left ventricle, a right ventricle, a left atrium, and a right atrium in a myocardium of the heart, and
the outline includes at least one of an inner boundary and an outer boundary of the myocardium. 9. The medical-image processing apparatus according to claim 1, wherein
the processing circuitry is further configured to generate the medical image data based on reflected wave data collected by ultrasonic scanning of a region including the subject. 10. An ultrasonic diagnostic apparatus, comprising:
a scanner configured to scan a subject body by using ultrasonic waves, and collect reflected wave data; and
processing circuitry configured to
acquire an initial value of an outline corresponding vector that corresponds to an outline of a subject included in medical image data of a heart generated by the reflected wave data, and
update the outline corresponding vector based on a derivative acquired by differentiating a cost function with respect to the outline corresponding vector by the outline corresponding vector, and based on the initial value of the outline corresponding vector, and
display, on a display, a medical image expressing the updated outline. 10. An ultrasonic diagnostic apparatus, comprising:
a scanner configured to scan a subject body by using ultrasonic waves, and collect reflected wave data; and
processing circuitry configured to
acquire an initial value of an outline corresponding vector that corresponds to an outline of a subject included in medical image data of a heart generated by the reflected wave data, and
update the outline corresponding vector based on a derivative acquired by differentiating a cost function with respect to the outline corresponding vector by the outline corresponding vector, and based on the initial value of the outline corresponding vector, and
display, on a display, a medical image expressing the updated outline. 11. A medical-image processing method, comprising:
acquiring an initial value of an outline corresponding vector that corresponds to an outline of a subject included in medical image data of a heart;
updating the outline corresponding vector based on a derivative that is acquired by differentiating a cost function with respect to the outline corresponding vector by the outline corresponding vector, and based on the initial value of the outline corresponding vector; and
displaying, on a display, a medical image expressing the updated outline. 11. A medical-image processing method, comprising:
acquiring an initial value of an outline corresponding vector that corresponds to an outline of a subject included in medical image data of a heart;
updating the outline corresponding vector based on a derivative that is acquired by differentiating a cost function with respect to the outline corresponding vector by the outline corresponding vector, and based on the initial value of the outline corresponding vector; and
displaying, on a display, a medical image expressing the updated outline.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318846B2,US10318846B2,Clustering historical images using a convolutional neural net and labeled data bootstrapping,2016-12-28,"second, incorrect, device, extractor, display, images, create, confidence, subset, feature, input, based, certain, corresponding, systems, data, assigned, related, classification, particular, plurality, each, neural, with, assigning, image, label, corrected, probability, convolutional, using, labeled, methods, replacement, performed, bootstrapping, classifying, include, that, received, labels, user, displayed, historical, correctly, vectors, which, statistical, from, first, clustering","Systems and methods for classifying historical images. A feature extractor may create feature vectors corresponding to a plurality of images. A first classification of the plurality of images may be performed based on the plurality of feature vectors, which may include assigning a label to each of the plurality of images and assigning a probability for each of the assigned labels. The assigned probability for each of the assigned labels may be related to a statistical confidence that a particular assigned label is correctly assigned to a particular image. A subset of the plurality of images may be displayed to a display device. An input corresponding to replacement of an incorrect label with a corrected label for a certain image may be received from a user. A second classification of the plurality of images based on the input from the user may be performed.","1. A method for classifying a plurality of images comprising:
creating, by a feature extractor, a plurality of feature vectors corresponding to the plurality of images;
performing, by a feature classifier, a first classification of the plurality of images based on the plurality of feature vectors, wherein performing the first classification includes:
assigning at least one of a plurality of labels to each of the plurality of images; and
assigning a first probability for each of the assigned labels, wherein the assigned first probability for each of the assigned labels is related to a statistical confidence that a particular assigned label is correctly assigned to a particular image;

determining a subset of probabilities of the assigned first probabilities, wherein the subset of probabilities includes all of the assigned first probabilities that are less than an upper probability threshold;
determining a subset of the plurality of images corresponding to the subset of probabilities;
outputting, to a display device, the subset of the plurality of images corresponding to the subset of probabilities;
receiving user input corresponding to replacement of an incorrect label with a corrected label for a certain image of the subset of the plurality of images;
receiving user input corresponding to a confidence level associated with the corrected label;
adjusting the feature classifier using the corrected label and the confidence level associated with the corrected label; and
performing, by the adjusted feature classifier, a second classification of the plurality of images based on the plurality of feature vectors, wherein performing the second classification includes:
assigning at least one of the plurality of labels to each of the plurality of images, including assigning the corrected label to the certain image; and
assigning a second probability for each of the assigned labels. 1. A method for classifying a plurality of images comprising:
creating, by a feature extractor, a plurality of feature vectors corresponding to the plurality of images;
performing, by a feature classifier, a first classification of the plurality of images based on the plurality of feature vectors, wherein performing the first classification includes:
assigning at least one of a plurality of labels to each of the plurality of images; and
assigning a first probability for each of the assigned labels, wherein the assigned first probability for each of the assigned labels is related to a statistical confidence that a particular assigned label is correctly assigned to a particular image;

determining a subset of probabilities of the assigned first probabilities, wherein the subset of probabilities includes all of the assigned first probabilities that are less than an upper probability threshold;
determining a subset of the plurality of images corresponding to the subset of probabilities;
outputting, to a display device, the subset of the plurality of images corresponding to the subset of probabilities;
receiving user input corresponding to replacement of an incorrect label with a corrected label for a certain image of the subset of the plurality of images;
receiving user input corresponding to a confidence level associated with the corrected label;
adjusting the feature classifier using the corrected label and the confidence level associated with the corrected label; and
performing, by the adjusted feature classifier, a second classification of the plurality of images based on the plurality of feature vectors, wherein performing the second classification includes:
assigning at least one of the plurality of labels to each of the plurality of images, including assigning the corrected label to the certain image; and
assigning a second probability for each of the assigned labels. 2. The method of claim 1, wherein the feature extractor is a convolutional neural network (CNN), the CNN having been previously trained and the CNN being compatible with the plurality of images such that the plurality of images are receivable as inputs by the CNN. 3. The method of claim 1, wherein the plurality of images are historical images. 4. The method of claim 1, further comprising:
determining a second subset of probabilities of the assigned second probabilities;
determining a second subset of the plurality of images corresponding to the second subset of probabilities;
outputting, to the display device, the second subset of the plurality of images; and
receiving user input corresponding to replacement of a second incorrect label with a second corrected label for a second certain image of the second subset of the plurality of images. 5. The method of claim 1, wherein each of the plurality of feature vectors comprise 4096 numbers. 6. The method of claim 1, wherein the subset of probabilities includes all of the
assigned first probabilities that are greater than a lower probability threshold and less than the upper probability threshold. 7. The method of claim 1, further comprising:
receiving user input corresponding to creation of a new label, wherein the new label is added to the plurality of labels. 8. A computer readable storage media comprising instructions to cause one or more processors to perform operations comprising:
creating, by a feature extractor, a plurality of feature vectors corresponding to the plurality of images;
performing, by a feature classifier, a first classification of the plurality of images based on the plurality of feature vectors, wherein performing the first classification includes:
assigning at least one of a plurality of labels to each of the plurality of images; and
assigning a first probability for each of the assigned labels, wherein the assigned first probability for each of the assigned labels is related to a statistical confidence that a particular assigned label is correctly assigned to a particular image;

determining a subset of probabilities of the assigned first probabilities, wherein the subset of probabilities includes all of the assigned first probabilities that are less than an upper probability threshold;
determining a subset of the plurality of images corresponding to the subset of probabilities;
outputting, to a display device, the subset of the plurality of images corresponding to the subset of probabilities;
receiving user input corresponding to replacement of an incorrect label with a corrected label for a certain image of the subset of the plurality of images;
receiving user input corresponding to a confidence level associated with the corrected label;
adjusting the feature classifier using the corrected label and the confidence level associated with the corrected label; and
performing, by the adjusted feature classifier, a second classification of the plurality of images based on the plurality of feature vectors, wherein performing the second classification includes:
assigning at least one of the plurality of labels to each of the plurality of images, including assigning the corrected label to the certain image; and
assigning a second probability for each of the assigned labels. 8. A computer readable storage media comprising instructions to cause one or more processors to perform operations comprising:
creating, by a feature extractor, a plurality of feature vectors corresponding to the plurality of images;
performing, by a feature classifier, a first classification of the plurality of images based on the plurality of feature vectors, wherein performing the first classification includes:
assigning at least one of a plurality of labels to each of the plurality of images; and
assigning a first probability for each of the assigned labels, wherein the assigned first probability for each of the assigned labels is related to a statistical confidence that a particular assigned label is correctly assigned to a particular image;

determining a subset of probabilities of the assigned first probabilities, wherein the subset of probabilities includes all of the assigned first probabilities that are less than an upper probability threshold;
determining a subset of the plurality of images corresponding to the subset of probabilities;
outputting, to a display device, the subset of the plurality of images corresponding to the subset of probabilities;
receiving user input corresponding to replacement of an incorrect label with a corrected label for a certain image of the subset of the plurality of images;
receiving user input corresponding to a confidence level associated with the corrected label;
adjusting the feature classifier using the corrected label and the confidence level associated with the corrected label; and
performing, by the adjusted feature classifier, a second classification of the plurality of images based on the plurality of feature vectors, wherein performing the second classification includes:
assigning at least one of the plurality of labels to each of the plurality of images, including assigning the corrected label to the certain image; and
assigning a second probability for each of the assigned labels. 9. The computer readable storage media of claim 8, wherein the feature extractor is a convolutional neural network (CNN), the CNN having been previously trained and the CNN being compatible with the plurality of images such that the plurality of images are receivable as inputs by the CNN. 10. The computer readable storage media of claim 8, wherein the plurality of images are historical images. 11. The computer readable storage media of claim 8, further comprising instructions to cause one or more processors to perform operations further comprising:
determining a second subset of probabilities of the assigned second probabilities;
determining a second subset of the plurality of images corresponding to the second subset of probabilities;
outputting, to the display device, the second subset of the plurality of images; and
receiving user input corresponding to replacement of a second incorrect label with a second corrected label for a second certain image of the second subset of the plurality of images. 12. The computer readable storage media of claim 8, wherein each of the plurality of feature vectors comprise 4096 numbers. 13. The computer readable storage media of claim 8, wherein the subset of probabilities includes all of the
assigned first probabilities that are greater than a lower probability threshold and less than the upper probability threshold. 14. The computer readable storage media of claim 8, further comprising instructions to cause one or more processors to perform operations further comprising:
receiving user input corresponding to creation of a new label, wherein the new label is added to the plurality of labels. 15. A system for classifying a plurality of images, the system comprising:
one or more processors;
a display device in communication with the one or more processors;
one or more computer readable storage mediums comprising instructions to cause the one or more processors to perform operations comprising:
creating, by a feature extractor, a plurality of feature vectors corresponding to the plurality of images;
performing, by the feature classifier, a first classification of the plurality of images based on the plurality of feature vectors, wherein performing the first classification includes:
assigning at least one of a plurality of labels to each of the plurality of images; and
assigning a first probability for each of the assigned labels, wherein the assigned first probability for each of the assigned labels is related to a statistical confidence that a particular assigned label is correctly assigned to a particular image;

determining a subset of probabilities of the assigned first probabilities, wherein the subset of probabilities includes all of the assigned first probabilities that are less than an upper probability threshold;
determining a subset of the plurality of images corresponding to the subset of probabilities;
outputting, to the display device, the subset of the plurality of images corresponding to the subset of probabilities;
receiving user input corresponding to replacement of an incorrect label with a corrected label for a certain image of the subset of the plurality of images;
receiving user input corresponding to a confidence level associated with the corrected label;
adjusting the feature classifier using the corrected label and the confidence level associated with the corrected label; and
performing, by the adjusted feature classifier, a second classification of the plurality of images based on the plurality of feature vectors, wherein performing the second classification includes:
assigning at least one of the plurality of labels to each of the plurality of images, including assigning the corrected label to the certain image; and
assigning a second probability for each of the assigned labels. 15. A system for classifying a plurality of images, the system comprising:
one or more processors;
a display device in communication with the one or more processors;
one or more computer readable storage mediums comprising instructions to cause the one or more processors to perform operations comprising:
creating, by a feature extractor, a plurality of feature vectors corresponding to the plurality of images;
performing, by the feature classifier, a first classification of the plurality of images based on the plurality of feature vectors, wherein performing the first classification includes:
assigning at least one of a plurality of labels to each of the plurality of images; and
assigning a first probability for each of the assigned labels, wherein the assigned first probability for each of the assigned labels is related to a statistical confidence that a particular assigned label is correctly assigned to a particular image;

determining a subset of probabilities of the assigned first probabilities, wherein the subset of probabilities includes all of the assigned first probabilities that are less than an upper probability threshold;
determining a subset of the plurality of images corresponding to the subset of probabilities;
outputting, to the display device, the subset of the plurality of images corresponding to the subset of probabilities;
receiving user input corresponding to replacement of an incorrect label with a corrected label for a certain image of the subset of the plurality of images;
receiving user input corresponding to a confidence level associated with the corrected label;
adjusting the feature classifier using the corrected label and the confidence level associated with the corrected label; and
performing, by the adjusted feature classifier, a second classification of the plurality of images based on the plurality of feature vectors, wherein performing the second classification includes:
assigning at least one of the plurality of labels to each of the plurality of images, including assigning the corrected label to the certain image; and
assigning a second probability for each of the assigned labels. 16. The system of claim 15, wherein the feature extractor is a convolutional neural network (CNN), the CNN having been previously trained and the CNN being compatible with the plurality of images such that the plurality of images are receivable as inputs by the CNN. 17. The system of claim 15, wherein the one or more computer readable storage mediums further comprise instructions to cause the one or more processors to perform operations further comprising:
determining a second subset of probabilities of the assigned second probabilities;
determining a second subset of the plurality of images corresponding to the second subset of probabilities;
outputting, to the display device, the second subset of the plurality of images; and
receiving user input corresponding to replacement of a second incorrect label with a second corrected label for a second certain image of the second subset of the plurality of images. 18. The system of claim 15, wherein each of the plurality of feature vectors comprise 4096 numbers. 19. The system of claim 15, wherein the subset of probabilities includes all of the
assigned first probabilities that are greater than a lower probability threshold and less than the upper probability threshold. 20. The system of claim 15, wherein the one or more computer readable storage mediums further comprise instructions to cause the one or more processors to perform operations further comprising:
receiving user input corresponding to creation of a new label, wherein the new label is added to the plurality of labels.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318852B2,US10318852B2,Smart card simultaneously having two read/write modes and method for producing same,2014-11-10,"method, module, invention, laminating, producing, slots, device, back, cutting, card, completing, protection, finally, encapsulating, printed, base, electrically, modes, wherein, milling, circuits, side, steps, with, provides, same, smart, embedding, layer, above, obtain, aforesaid, respectively, write, includes, front, simultaneously, films, obtained, sheets, connected, also, chip, then, conductive, underneath, elastic, manufacturing, having, carrier, whole, antenna, bedding, after, treated, which, from, circuit, sheet, read","A smart card with two read-write modes includes antenna layer, and an antenna and a chip module circuits on the antenna layer, wherein the antenna and the chip module circuit are electrically connected via an elastic conductive device. The invention also provides a manufacturing method of the aforesaid smart card with two read-write modes, which includes steps of: embedding an antenna on a back side or a front side of an antenna layer; after completing embedding on the antenna layer, add bedding sheets, printed sheets and protection films respectively above and underneath the antenna layer, then laminating to obtain a card base carrier; cutting card from the treated whole-sheet card base carrier to obtain a card base, and milling slots on the obtained card base, then finally encapsulating.","1. A smart card simultaneously having two read-write modes, comprising an antenna layer (2) and an antenna (3) and a chip module circuit layer (4) arranged on the antenna layer (2), wherein the antenna (3) and the chip module circuit layer (4) are electrically connected through a metallic or non-metallic elastic conductive device (5), a coil wire end (31) of the antenna (3) is electrically connected with the elastic conductive device (5), the elastic conductive device (5) is arranged in a region corresponding to a circuit contact point of the chip module circuit layer (4), one surface of the elastic conductive device (5) is in electric contact connection with the circuit contact point of the chip module circuit layer (4), and the coil wire end (31) of the antenna (3) is made into a contact pad in a single-loop or multi-loop meandering winding manner and is arranged in a region corresponding to the circuit contact point of the chip module circuit layer (4) on the antenna layer (2). 1. A smart card simultaneously having two read-write modes, comprising an antenna layer (2) and an antenna (3) and a chip module circuit layer (4) arranged on the antenna layer (2), wherein the antenna (3) and the chip module circuit layer (4) are electrically connected through a metallic or non-metallic elastic conductive device (5), a coil wire end (31) of the antenna (3) is electrically connected with the elastic conductive device (5), the elastic conductive device (5) is arranged in a region corresponding to a circuit contact point of the chip module circuit layer (4), one surface of the elastic conductive device (5) is in electric contact connection with the circuit contact point of the chip module circuit layer (4), and the coil wire end (31) of the antenna (3) is made into a contact pad in a single-loop or multi-loop meandering winding manner and is arranged in a region corresponding to the circuit contact point of the chip module circuit layer (4) on the antenna layer (2). 2. The smart card, as recited in claim 1, wherein the coil wire end (31) of the antenna (3) is electrically connected with the other surface of the elastic conductive device (5) in a welding manner. 3. The smart card, as recited in claim 1, wherein the coil wire end (31) of the antenna (3) is electrically connected with the other surface of the elastic conductive device (5) in a direct contact manner. 4. The smart card, as recited in claim 2, wherein the thickness of the antenna layer (2) is 0.13-0.16 mm. 5. The smart card, as recited in claim 3, wherein the thickness of the antenna layer (2) is 0.13-0.16 mm. 6. A production method of a smart card simultaneously having two read-write modes, comprising steps of:
1) embedding antenna (3): embedding an antenna (3) on a back surface or a surface of an antenna layer (2), and placing a coil wire end (31) of the antenna (3) in a region corresponding to circuit contact points of a chip module circuit layer (4), wherein the coil wire end (31) of the antenna (3) is made into a contact pad in a meandering winding manner, and the contact pad is located in a region corresponding to the circuit contact point of the chip module circuit layer (4);
2) laminating: after embedding the antenna (3) on the antenna layer (2), respectively adding a pad layer, a printing layer and a protection layer on an upper and a lower parts of the antenna layer (2), and laminating to obtain a card base carrier;
3) cutting card and milling grooves: cutting a card from the laminated integral card base carrier to finally obtain a card base, milling grooves on the obtained card base, at first milling grooves on a position where a chip module is placed, firstly milling a first recess (B5), wherein a thickness of the first recess (B5) is equal to that of the chip module boundary, then, milling a second recess (B6) in a middle of the first recess (B5), wherein a milling cutter provided with a special sensor is used for milling the position, and the milling cutter detects whether an antenna (3) embedding layer is milled in the groove milling process in real time, immediately stops at a maximum milled depth value according to a preset program once milling the coil wire end (31) of the embedded antenna (3) and memorizes the maximum milled depth value, and finally, milling a third recess (B3) on a position where an elastic device is placed, wherein a depth of the third recess (B3) is determined by the memorized value; and
4) encapsulating: firstly placing the elastic device in the third recess (B3) and electrically connecting the elastic device with the coil wire end (31) of the antenna (3), putting the chip module circuit layer (4) in the first recess (B5) and the second recess (B6) on positions where the circuit contact points are corresponding to the elastic device, and finally, securing the chip module layer into the first and/or the second recesses. 6. A production method of a smart card simultaneously having two read-write modes, comprising steps of:
1) embedding antenna (3): embedding an antenna (3) on a back surface or a surface of an antenna layer (2), and placing a coil wire end (31) of the antenna (3) in a region corresponding to circuit contact points of a chip module circuit layer (4), wherein the coil wire end (31) of the antenna (3) is made into a contact pad in a meandering winding manner, and the contact pad is located in a region corresponding to the circuit contact point of the chip module circuit layer (4);
2) laminating: after embedding the antenna (3) on the antenna layer (2), respectively adding a pad layer, a printing layer and a protection layer on an upper and a lower parts of the antenna layer (2), and laminating to obtain a card base carrier;
3) cutting card and milling grooves: cutting a card from the laminated integral card base carrier to finally obtain a card base, milling grooves on the obtained card base, at first milling grooves on a position where a chip module is placed, firstly milling a first recess (B5), wherein a thickness of the first recess (B5) is equal to that of the chip module boundary, then, milling a second recess (B6) in a middle of the first recess (B5), wherein a milling cutter provided with a special sensor is used for milling the position, and the milling cutter detects whether an antenna (3) embedding layer is milled in the groove milling process in real time, immediately stops at a maximum milled depth value according to a preset program once milling the coil wire end (31) of the embedded antenna (3) and memorizes the maximum milled depth value, and finally, milling a third recess (B3) on a position where an elastic device is placed, wherein a depth of the third recess (B3) is determined by the memorized value; and
4) encapsulating: firstly placing the elastic device in the third recess (B3) and electrically connecting the elastic device with the coil wire end (31) of the antenna (3), putting the chip module circuit layer (4) in the first recess (B5) and the second recess (B6) on positions where the circuit contact points are corresponding to the elastic device, and finally, securing the chip module layer into the first and/or the second recesses.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318849B2,US10318849B2,Check image data interference processing,2015-05-29,"method, optical, amount, account, part, alternatively, device, another, other, images, recognition, included, based, process, amounts, systems, data, herein, updating, stored, each, group, where, with, image, inferring, respective, methods, check, inference, various, includes, include, memory, processing, missing, some, also, once, more, date, embodiment, embodiments, software, least, associated, poorly, interference, inferred, character, example, read","Various embodiments herein each include at least one of systems, methods, and software for check image data inference processing. Another example method embodiment includes inferring a check amount of a check image included in an account group of check images stored in a memory device. Where the check amount is missing in check data associated with the check image or was poorly read by an optical character recognition process, the method includes inferring of the check amount based at least in part on one or more check amounts of check data associated with other check images of the account group. Once inferred, the method includes updating the check amount of the check data associated with the respective check image with the inferred check amount of the check image. Some embodiments also or alternatively include inferring a check date.","1. A method comprising:
extracting, by executing instructions on a computer processor, at least a portion of text included in each digital document image of a plurality of check images and storing, on a data storage device, the extracted text as check data associated with a text field of the respective check image;
identifying a plurality of checks in the stored check data that are associated with a same account;
identifying a text field in the stored data of one check from which text was not reliably extracted; and
inferring, by executing instructions on the computer processor, content of the text field based on the extracted text and on text extracted from corresponding text fields in check data of other check images associated with the same account, the inferring including a distance comparison according to a distance measuring algorithm that measures a distance between extracted text and text extracted from corresponding text fields in check data of other check images associated with the same account; and
storing the inferred content on the data storage device. 1. A method comprising:
extracting, by executing instructions on a computer processor, at least a portion of text included in each digital document image of a plurality of check images and storing, on a data storage device, the extracted text as check data associated with a text field of the respective check image;
identifying a plurality of checks in the stored check data that are associated with a same account;
identifying a text field in the stored data of one check from which text was not reliably extracted; and
inferring, by executing instructions on the computer processor, content of the text field based on the extracted text and on text extracted from corresponding text fields in check data of other check images associated with the same account, the inferring including a distance comparison according to a distance measuring algorithm that measures a distance between extracted text and text extracted from corresponding text fields in check data of other check images associated with the same account; and
storing the inferred content on the data storage device. 2. The method of claim 1, wherein the check images are received with check code line data read from a code line of each respective check, the check code line data included in check data associated with a respective check image. 3. The method of claim 2, wherein the extracting includes performing optical character recognition to extract a portion of text from a text field and adding the text to the check data associated with a respective check image from which the text was extracted. 4. The method of claim 3, wherein:
performing the optical character recognition further obtains a confidence value with regard to the extracted text, a confidence value obtained with specific regard to extracted text of at least one of a date and an amount of a check image, the confidence value added to check data associated with the respective check image; and
the inferring of content of the text field from which text was not reliability extracted is performed when a confidence value of a date or amount read from a check image is below a confidence threshold. 5. The method of claim 4, wherein when inferring date content for a text field, the inferring of the date content includes:
ordering the plurality of documents associated with the same account as the document from which the text field was not reliability extracted by at least one of dates and check numbers extracted from the respective documents;
identifying a frequency of dates of the check images; and
inferring the date content of the text field that was not reliability extracted based at least in part on the identified frequency of check image dates. 6. The method of claim 4, wherein a text field that was not reliably extracted includes an ambiguously read date, an ambiguously read date including a date extracted from a check image where a month and day are both represented as a number less than thirteen (13), the inferring of the text content of an ambiguously read date performed according to a set of ambiguity resolution rules, the ambiguity resolution rules including:
identifying an order of month and day extracted from at least one other check image of the documents associated with the same account and applying that order to the ambiguously read date as the inferred content;
when the ambiguously read date, when considering month/day or day/month order is a date prior to a current date, applying an order that is a future date as the inferred content; and
when one of the month/day or day/month order fills a date frequency gap between dates of other check images associated with the same account, applying an order that fills the date frequency gap as the inferred content. 7. The method of claim 4, further comprising:
ordering the plurality of documents associated with the same account as the document from which the text field was not reliability extracted by at least one of dates and check numbers extracted from the respective documents;
when check data of a check image includes inferring a check amount that was not reliably extracted:
inferring content of the check amount of the check image based on check values included in check data of at least one other check image associated with the same account as the check image from which the check amount was not reliably extracted; and
updating the check data check amount of the check image from which the check amount was not reliably extracted. 8. The method of claim 7, wherein inferring the check amount based on check values included in check data of at least one other check image associated with the same account as the check image from which the check amount was not reliably extracted includes:
ordering check data of check images of the plurality of documents associated with the same account by at least one of check numbers and dates;
for check amounts in check data of the plurality of documents associated with a same account having a confidence level above a confidence threshold, identifying a most common check amount;
performing a similarity comparison between the identified most common amount and the check amount extracted from the check image with the check amount that was not reliably extracted; and
when the similarity is within an acceptable tolerance, modifying the check amount of the check data of the check image that was not reliably extracted to be the same as the identified most common check amount. 9. The method of claim 8, wherein the similarity comparison is performed according to a Levenshtein distance algorithm and the acceptable tolerance is a threshold value within which an output of the Levenshtein distance algorithm must fit to infer the check amount. 10. A method comprising:
inferring, by executing instructions on a computer processor, a check amount of a check image included in an account group of check images stored in a memory device where the check amount is missing in check data associated with the check image or was poorly read by an optical character recognition process, the inferring of the check amount based at least in part on one or more check amounts of check data associated with other check images of the account group, each stored check image including check data of text fields of the respective check images, the inferring the check amount of the check image missing or having a poorly read check amount based at least in part on one or more check amounts of check data associated with other check images of the account group includes:
ordering check data of check images within the account group by at least one of check numbers and dates;
for check amounts in check data of the account group read by an optical character recognition process with a confidence level above a confidence threshold, identifying a most common check amount;
performing a similarity comparison between the identified most common amount and the check amount of the check image missing or having a poorly read check amount, the similarity comparison including applying a distance measuring algorithm that measures a distance between extracted text and text extracted from corresponding text fields in check data of other check images associated with the same account; and
when the similarity is within an acceptable tolerance, modifying the check amount of the check data of the check image missing or having a poorly read check amount to be the same as the identified most common check amount; and

updating the check amount of the check data associated with the respective check image with the inferred check amount of the check image. 10. A method comprising:
inferring, by executing instructions on a computer processor, a check amount of a check image included in an account group of check images stored in a memory device where the check amount is missing in check data associated with the check image or was poorly read by an optical character recognition process, the inferring of the check amount based at least in part on one or more check amounts of check data associated with other check images of the account group, each stored check image including check data of text fields of the respective check images, the inferring the check amount of the check image missing or having a poorly read check amount based at least in part on one or more check amounts of check data associated with other check images of the account group includes:
ordering check data of check images within the account group by at least one of check numbers and dates;
for check amounts in check data of the account group read by an optical character recognition process with a confidence level above a confidence threshold, identifying a most common check amount;
performing a similarity comparison between the identified most common amount and the check amount of the check image missing or having a poorly read check amount, the similarity comparison including applying a distance measuring algorithm that measures a distance between extracted text and text extracted from corresponding text fields in check data of other check images associated with the same account; and
when the similarity is within an acceptable tolerance, modifying the check amount of the check data of the check image missing or having a poorly read check amount to be the same as the identified most common check amount; and

updating the check amount of the check data associated with the respective check image with the inferred check amount of the check image. 11. The method of claim 10, wherein the similarity comparison is performed according to a Levenshtein distance algorithm and the acceptable tolerance is a threshold value within which an output of the Levenshtein distance algorithm must fit to infer the check amount. 12. The method of claim 10, further comprising:
inferring a check date of the check image included in the account group of check images stored in the memory device where the check date is missing in the check data associated with the check image or was poorly read by the optical character recognition process, the inferring of the check date based at least in part on the check data associated with the other check images of the account group; and
updating the check date of the check data associated with the respective check image with the inferred date of the check image. 13. The method of claim 10, further comprising:
receiving images of a plurality of post-dated checks;
performing optical character recognition, by the optical character recognition process, on at least a portion of text included in each check image to obtain the check data for each check image;
adding the check data to data associated with each respective check image; and
associating images of checks written from the same account into an account group. 14. The method of claim 13, wherein the images of the plurality of post-dated checks are received via a network interface device. 15. A system comprising:
at least one processor;
at least one memory; and
an instruction set accessible in the memory and executable by the at least one processor, the instruction set including a set of modules, the set of modules comprising:
a check amount inference module including instructions executable by the at least one processor to perform data processing activities comprising:
inferring a check amount of a check image included in an account group of check images stored in a memory device where the check amount is missing in check data associated with the check image or was poorly read by an optical character recognition process, the inferring of the check amount based at least in part on one or more check amounts of check data associated with other check images of the account group, each stored check image including check data of text fields of the respective check images; and
updating the check amount of the check data associated with the respective check image with the inferred check amount of the check image; and

a check date inference module including instructions executable by the at least one processor to perform data processing activities comprising:
inferring a check date of the check image included in the account group of check images stored in the memory device where the check date is missing in the check data associated with the check image or was poorly read by the optical character recognition process, the inferring of the check date based on the extracted text and at least in part on the check data associated with the other check images of the account group, the inferring including a distance comparison according to a distance measuring algorithm that measures a distance between extracted text and text extracted from corresponding text fields in check data of other check images associated with the same account; and
updating the check date of the check data associated with the respective check image with the inferred date of the check image. 15. A system comprising:
at least one processor;
at least one memory; and
an instruction set accessible in the memory and executable by the at least one processor, the instruction set including a set of modules, the set of modules comprising:
a check amount inference module including instructions executable by the at least one processor to perform data processing activities comprising:
inferring a check amount of a check image included in an account group of check images stored in a memory device where the check amount is missing in check data associated with the check image or was poorly read by an optical character recognition process, the inferring of the check amount based at least in part on one or more check amounts of check data associated with other check images of the account group, each stored check image including check data of text fields of the respective check images; and
updating the check amount of the check data associated with the respective check image with the inferred check amount of the check image; and

a check date inference module including instructions executable by the at least one processor to perform data processing activities comprising:
inferring a check date of the check image included in the account group of check images stored in the memory device where the check date is missing in the check data associated with the check image or was poorly read by the optical character recognition process, the inferring of the check date based on the extracted text and at least in part on the check data associated with the other check images of the account group, the inferring including a distance comparison according to a distance measuring algorithm that measures a distance between extracted text and text extracted from corresponding text fields in check data of other check images associated with the same account; and
updating the check date of the check data associated with the respective check image with the inferred date of the check image. 16. The system of claim 15, the set of modules further comprising:
a check image receiving module including instructions executable by the at least one processor to perform data processing activities comprising:
receiving images of a plurality of checks;
performing optical character recognition, by the optical character recognition process, on at least a portion of text included in each check image to obtain the check data for each check image;
adding the check data to data associated with each respective check image;
associating images of checks written from the same account into an account group; and
ordering check data of check images within each account group by at least one of check numbers and dates. 17. The system of claim 16, further comprising:
a network interface device; and
wherein the check image receiving modules receives the image of the plurality of checks via the network interface device.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318853B2,US10318853B2,Antenna module and portable terminal having same,,"connecting, module, made, amount, second, portable, connection, other, without, stacked, surface, including, reduced, directly, plating, magnetic, through, portion, electrically, formed, thereon, thus, therein, same, body, layer, includes, thickness, terminal, having, antenna, substrate, ferrite, pattern, hole, member, first, sheet","An antenna module includes a magnetic body portion made of a ferrite sheet, a first pattern portion directly formed on one surface of the magnetic body portion and including a first pattern and a first plating layer stacked thereon, a second pattern portion directly formed on the other surface of the magnetic body portion and including a second pattern and a second plating layer stacked thereon, and a via hole formed through the magnetic body portion and having a connection member for electrically connecting the first pattern portion and the second pattern portion formed therein. The first pattern portion and the second pattern portion are formed directly on the magnetic body portion without a substrate portion. Thus, the thickness is reduced by the amount of the thickness of the substrate portion.","1. An antenna module comprising:
a magnetic body portion made of a ferrite sheet;
a first pattern portion directly formed on one surface of the magnetic body portion and made by stacking a first pattern and a first plating layer;
a second pattern portion directly formed on the other surface of the magnetic body portion and made by stacking a second pattern and a second plating layer; and
a via hole formed to pass through the magnetic body portion and provided therein with a connection member formed to electrically connect the first pattern portion and the second pattern portion,
wherein the first pattern portion and the second pattern portion are directly formed on the magnetic body portion without a substrate portion. 1. An antenna module comprising:
a magnetic body portion made of a ferrite sheet;
a first pattern portion directly formed on one surface of the magnetic body portion and made by stacking a first pattern and a first plating layer;
a second pattern portion directly formed on the other surface of the magnetic body portion and made by stacking a second pattern and a second plating layer; and
a via hole formed to pass through the magnetic body portion and provided therein with a connection member formed to electrically connect the first pattern portion and the second pattern portion,
wherein the first pattern portion and the second pattern portion are directly formed on the magnetic body portion without a substrate portion. 2. The antenna module of claim 1, wherein the first pattern portion and the second pattern portion are formed to have different thicknesses. 3. The antenna module of claim 2, wherein the first plating layer and the second plating layer have different thicknesses. 4. The antenna module of claim 2, wherein the first pattern and the second pattern have the same thickness. 5. The antenna module of claim 1, wherein the connection member is formed by plating. 6. The antenna module of claim 1, wherein the first pattern portion and the second pattern portion are formed to have different widths. 7. The antenna module of claim 2, wherein the first pattern portion and the second pattern portion are formed to have different widths. 8. A portable terminal comprising:
an antenna module including a magnetic body portion made of a ferrite sheet, a first pattern portion and a second pattern portion attached to both surfaces of the magnetic body portion, a via hole formed to sequentially pass through the first pattern portion, the magnetic body portion, and the second pattern portion, and a connection member formed inside the via hole and configured to electrically connect the first and second pattern portions to each other; and
a substrate provided with a power feeding portion and a grounding portion, electrically connected to the antenna module, and configured to feed power to or ground the first pattern portion or the second pattern portion. 8. A portable terminal comprising:
an antenna module including a magnetic body portion made of a ferrite sheet, a first pattern portion and a second pattern portion attached to both surfaces of the magnetic body portion, a via hole formed to sequentially pass through the first pattern portion, the magnetic body portion, and the second pattern portion, and a connection member formed inside the via hole and configured to electrically connect the first and second pattern portions to each other; and
a substrate provided with a power feeding portion and a grounding portion, electrically connected to the antenna module, and configured to feed power to or ground the first pattern portion or the second pattern portion.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318851B2,US10318851B2,Generating and decoding two-dimensional code,2015-09-10,"method, encoding, multiple, encode, version, determined, matrix, finally, corresponding, mask, dots, determine, updating, barcode, generation, decoding, length, number, selecting, includes, selected, information, performing, hidden, encoded, generate, standard, black, system, generating, according, dimensional, code, extracting","A two-dimensional code generation method includes: performing a multiple system barcode encoding on hidden information to generate multiple system barcode of hidden encoded information; selecting a corresponding two-dimensional code version to encode standard information to generate to-be-determined standard encoded information; selecting a corresponding mask to generate a bit matrix of a two-dimensional code, and extracting the number of black dots in the bit matrix; updating the selected mask and the two-dimensional code version according to the number of black dots and the length of the hidden encoded information to determine a finally selected mask and two-dimensional code version; generating standard encoded information according to the finally selected mask and two-dimensional code version; and generating a two-dimensional code according to the hidden encoded information and the standard encoded information.","1. A two-dimensional code generation method comprising:
performing a multiple system barcode encoding on hidden information to generate multiple system barcode of hidden encoded information;
selecting a corresponding two-dimensional code version according to a two-dimensional code version level to encode standard information to generate to-be-determined standard encoded information;
selecting a corresponding mask among a plurality of masks according to the to-be-determined standard encoded information to generate a bit matrix of the two-dimensional code;
extracting a number of black dots in the bit matrix;
updating the selected mask among the plurality of masks and the two-dimensional code version according to whether a length of the hidden encoded information is less than the number of black dots in the bit matrix to determine a finally selected mask and two-dimensional code version;
generating standard encoded information according to the finally selected mask and two-dimensional code version; and
generating a two-dimensional code according to the hidden encoded information and the standard encoded information. 1. A two-dimensional code generation method comprising:
performing a multiple system barcode encoding on hidden information to generate multiple system barcode of hidden encoded information;
selecting a corresponding two-dimensional code version according to a two-dimensional code version level to encode standard information to generate to-be-determined standard encoded information;
selecting a corresponding mask among a plurality of masks according to the to-be-determined standard encoded information to generate a bit matrix of the two-dimensional code;
extracting a number of black dots in the bit matrix;
updating the selected mask among the plurality of masks and the two-dimensional code version according to whether a length of the hidden encoded information is less than the number of black dots in the bit matrix to determine a finally selected mask and two-dimensional code version;
generating standard encoded information according to the finally selected mask and two-dimensional code version; and
generating a two-dimensional code according to the hidden encoded information and the standard encoded information. 2. The two-dimensional code generation method of claim 1, wherein the selecting the corresponding mask according to the to-be-determined standard encoded information to generate the bit matrix of the two-dimensional code includes:
determining a priority of the mask according to the to-be-determined standard encoded information;
determining the corresponding mask according to the priority of the mask; and
generating the bit matrix of the two-dimensional code according to the corresponding mask. 3. The two-dimensional code generation method of claim 2, wherein the updating the selected mask and the two-dimensional code version according to the number of black dots in the bit matrix and the length of the hidden encoded information to determine the finally selected mask and two-dimensional code version includes:
in response to determining that the length of the hidden encoded information is greater than the number of black dots in the bit matrix,
selecting a mask with a next lower priority;
comparing whether the length of the hidden encoded information is less than the number of black dots in the bit matrix according to the selected mask with the next lower priority until the length of the hidden encoded information is less than the number of black dots in the bit matrix;
using a current mask as the finally selected mask; and
using a current two-dimensional code version as the finally selected two-dimensional code version. 4. The two-dimensional code generation method of claim 3, wherein in response to determining that the length of the hidden encoded information is greater than the number of black dots in the bit matrix after a priority of the mask reaches a lowest priority, the two-dimensional code generation method further comprises:
selecting a two-dimensional code version of a next higher level;
encoding the standard information again according to the two-dimensional code version of the next higher level to generate the to-be-determined standard encoded information; and
selecting a corresponding mask according to the to-be-determined standard encoded information to generate the bit matrix of the two-dimensional code. 5. The two-dimensional code generation method of claim 1, wherein the updating the selected mask and the two-dimensional code version according to the number of black dots in the bit matrix and the length of the hidden encoded information to determine the finally selected mask and two-dimensional code version includes
judging whether the length of the hidden encoded information is less than the number of black dots in the bit matrix; and
in response to determining that the length of the hidden encoded information is less than the number of black dots in the bit matrix,
using the current mask as the finally selected mask; and
using the current two-dimensional code version as the finally selected two-dimensional code version. 6. The two-dimensional code generation method of claim 1, wherein each bit of the multiple system barcode of hidden encoded information corresponds to a color. 7. The two-dimensional code generation method of claim 1, wherein the two-dimensional code is a Quick Response Code (QRCODE). 8. The two-dimensional code generation method of claim 1, wherein the multiple system barcode hidden encoding is quaternary hidden encoding. 9. The two-dimensional code generation method of claim 8, wherein the drawing the data storage area of the two-dimensional code according to the hidden encoded information and the standard encoded information includes:
starting to draw the standard encoded information from a lower left corner of the data storage area; and
successively retrieving a bit value from the hidden encoded information for filling when encountering a black dot. 10. The two-dimensional code generation of claim 9, wherein the drawing the data storage area of the two-dimensional code according to the hidden encoded information and the standard encoded information includes:
randomly selecting a color from a plurality of colors corresponding to the multiple system barcode of hidden encoded information to fill the black dot when all the values in the hidden encoded information have been retrieved. 11. The two-dimensional code generation method of claim 1, wherein the generating the two-dimensional code according to the hidden encoded information and the standard encoded information includes:
acquiring decoding auxiliary information;
performing the multiple system barcode encoding on the decoding auxiliary information to generate multiple system barcode of decoding auxiliary information, wherein the two-dimensional code includes a plurality of locators that are used for drawing the multiple system barcode of decoding auxiliary information; and
drawing a data storage area of the two-dimensional code according to the hidden encoded information and the standard encoded information. 12. A two-dimensional code generation device comprising:
one or more processors; and
one or more memories stored thereon computer-executable instructions, executable by the one or more processors, to cause the one or more processors to perform acts comprising:
performing a multiple system barcode encoding on hidden information to generate multiple system barcode of hidden encoded information;
selecting a corresponding two-dimensional code version according to a two-dimensional code version level to encode standard information to generate to-be-determined standard encoded information;
selecting a corresponding mask among a plurality of masks according to the to-be-determined standard encoded information to generate a bit matrix of the two-dimensional code;
extracting a number of black dots in the bit matrix; and
updating the selected mask among the plurality of masks and the two-dimensional code version according to whether a length of the hidden encoded information is less than the number of black dots in the bit matrix to determine a finally selected mask and two-dimensional code version. 12. A two-dimensional code generation device comprising:
one or more processors; and
one or more memories stored thereon computer-executable instructions, executable by the one or more processors, to cause the one or more processors to perform acts comprising:
performing a multiple system barcode encoding on hidden information to generate multiple system barcode of hidden encoded information;
selecting a corresponding two-dimensional code version according to a two-dimensional code version level to encode standard information to generate to-be-determined standard encoded information;
selecting a corresponding mask among a plurality of masks according to the to-be-determined standard encoded information to generate a bit matrix of the two-dimensional code;
extracting a number of black dots in the bit matrix; and
updating the selected mask among the plurality of masks and the two-dimensional code version according to whether a length of the hidden encoded information is less than the number of black dots in the bit matrix to determine a finally selected mask and two-dimensional code version. 13. The two-dimensional code generation device of claim 12, wherein the acts further comprise:
generating standard encoded information according to the finally selected mask and two-dimensional code version; and
generating a two-dimensional code according to the hidden encoded information and the standard encoded information. 14. The two-dimensional code generation device of claim 12, wherein the generating the two-dimensional code according to the hidden encoded information and the standard encoded information includes:
acquiring decoding auxiliary information;
performing the multiple system barcode encoding on the decoding auxiliary information to generate multiple system barcode of decoding auxiliary information, wherein the two-dimensional code includes a plurality of locators that are used for drawing the multiple system barcode of decoding auxiliary information; and
drawing a data storage area of the two-dimensional code according to the hidden encoded information and the standard encoded information. 15. The two-dimensional code generation device of claim 14, wherein the drawing the data storage area of the two-dimensional code according to the hidden encoded information and the standard encoded information includes:
starting to draw the standard encoded information from a lower left corner of the data storage area; and
successively retrieving a bit value from the hidden encoded information for filling when encountering a black dot. 16. The two-dimensional code generation device of claim 12, wherein the selecting the corresponding mask according to the to-be-determined standard encoded information to generate the bit matrix of the two-dimensional code includes:
determining a priority of the mask according to the to-be-determined standard encoded information;
determining the corresponding mask according to the priority of the mask; and
generating the bit matrix of the two-dimensional code according to the corresponding mask. 17. The two-dimensional code generation device of claim 12, wherein each bit of the multiple system barcode of hidden encoded information corresponds to a color. 18. The two-dimensional code generation device of claim 12, wherein the two-dimensional code is a Quick Response Code (QRCODE). 19. The two-dimensional code generation device of claim 12, wherein the multiple system barcode hidden encoding is quaternary hidden encoding.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318856B2,US10318856B2,Method for the manufacturing of dielectric bridges for contactless identification,,"method, integrated, made, positioning, connection, device, machine, designed, beneath, positioned, produce, continuous, investment, thereof, beam, transfer, productivity, together, systems, radiation, support, have, polyimide, immobilized, laser, speeds, contactless, relation, used, arranged, result, professions, into, when, become, costs, contacts, welded, many, using, bridges, ensuring, allows, that, increase, pressure, held, conductive, much, manufacturing, identification, produces, heat, resistant, under, stop, registered, dielectric, contact, faster, tags, built, been, widespread, over, circuit, rfid","A method produces non-contact dielectric bridges using a transfer machine for positioning an integrated circuit on a conductive circuit and a laser for ensuring the connection of the contacts thereof. The contacts of the integrated circuit that have been registered by a transfer machine in relation to the contacts of the conductive circuit, arranged on a continuous support made of heat- and radiation-resistant polyimide and held under pressure by the device, are welded together using a laser beam. The laser is positioned beneath the continuous support and built into the transfer machine. When the laser is used, the continuous support is immobilized by a stop and go device. The method is designed to increase the productivity of systems used to produce RFID tags, as a result of low investment costs and much faster speeds of connection of the contacts of the integrated circuit and the conductive circuit. The method allows the use of non-contact identification tags to become widespread over many professions.","1. A method for the manufacturing of dielectric bridges, or straps, for contactless identification, with a standardised pitch and with a broadened tolerance, or of contactless labels, or tags, of small width, the method comprising:
providing a standard silicon wafer with integrated circuits with an active face facing upwards, previously prepared by sawing, for a laser connection of contact pads of said circuits to contact pads of conductive circuits forming an antenna on a surface of a dielectric medium made of a polyimide, which is resistant to heat and to rays emitted by lasers;
picking the integrated circuits using a gripping system of an integrated circuit pick and place machine which flips the integrated circuits over;
inputting integrated circuit positioning data using a camera and a vision system for location;
placing the picked integrated circuits on to the dielectric medium made of polyimide, such that bumps of the integrated circuits are positioned facing the contact pads of the conductive circuits forming an antenna, held in place by a slight contact pressure of the bumps of the integrated circuits on the contact pads of the conductive circuit, wherein a strip of dielectric medium is immobilised during the placing;
connecting the integrated circuit with the conductive circuit using a laser with one head and one or more beams, or with several heads, depending on a configuration and a size of the bumps, and
advancing one pitch of the dielectric strip to undertake a next operation. 1. A method for the manufacturing of dielectric bridges, or straps, for contactless identification, with a standardised pitch and with a broadened tolerance, or of contactless labels, or tags, of small width, the method comprising:
providing a standard silicon wafer with integrated circuits with an active face facing upwards, previously prepared by sawing, for a laser connection of contact pads of said circuits to contact pads of conductive circuits forming an antenna on a surface of a dielectric medium made of a polyimide, which is resistant to heat and to rays emitted by lasers;
picking the integrated circuits using a gripping system of an integrated circuit pick and place machine which flips the integrated circuits over;
inputting integrated circuit positioning data using a camera and a vision system for location;
placing the picked integrated circuits on to the dielectric medium made of polyimide, such that bumps of the integrated circuits are positioned facing the contact pads of the conductive circuits forming an antenna, held in place by a slight contact pressure of the bumps of the integrated circuits on the contact pads of the conductive circuit, wherein a strip of dielectric medium is immobilised during the placing;
connecting the integrated circuit with the conductive circuit using a laser with one head and one or more beams, or with several heads, depending on a configuration and a size of the bumps, and
advancing one pitch of the dielectric strip to undertake a next operation. 2. The method according to claim 1, wherein the bumps of the integrated circuit and of the conductive circuit are connected by an infrared frequency laser with one or more heads. 3. The method according to claim 2, wherein the bumps of the integrated circuit and of the conductive circuit are connected by an infrared frequency laser with one or more heads when a composition of connection surfaces is made of an alloy consisting of several metals. 4. The method according to claim 1, wherein the bumps of the integrated circuit and of the conductive circuit are connected by a green frequency laser with one or more heads. 5. The method according to claim 4, wherein the bumps of the integrated circuit and of the conductive circuit are connected by a green frequency laser with one or more heads when a composition of connection surfaces is made of non-ferrous metals. 6. The method according to claim 1, wherein the conductive circuits forming an antenna are positioned on a strip made of polymide which is resistant to heat and to the laser beams. 7. The method according to claim 1, wherein the conductive circuits forming an antenna positioned on a strip made of polymide are protected by an anti-reflective coating, which depends on the frequency and power of the laser used. 8. The method according to claim 1, wherein the integrated circuits are placed on the conductive circuit forming an antenna with positioning tolerances equal to or less than 50 μm. 9. The method according to claim 1, wherein the integrated circuits are placed on the conductive circuit forming an antenna at a nominal rate of 30,000 units per hour. 10. The method according to claim 1, wherein that the pick and place machine is a standard machine designed for installing surface components. 11. The method according to claim 1, wherein a special module is integrated in the pick and place machine, under the machine, operation of which is closely coordinated with the pick and place machine and its vision system. 12. The method according to claim 1, wherein the conductive circuits forming an antenna are small. 13. The method according to claim 12, wherein the conductive circuits forming an antenna are small are standardised according to standard JEDEC MO-283.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318848B2,US10318848B2,Methods for object localization and image classification,2015-12-15,"method, labelled, including, present, fully, class, assigned, classification, object, indication, with, assigning, image, methods, training, interest, includes, include, framed, localization, also, labelling, partially, whether, crop, from, background","A method of training for image classification includes labelling a crop from an image including an object of interest. The crop may be labelled with an indication of whether the object of interest is framed, partially framed or not present in the crop. The method may also include assigning a fully framed class to the labelled crop, including the object of interest, if the object of interest is framed. A labelled crop may be assigned a partially framed class if the object of interest is partially framed. A background class may be assigned to a labelled crop if the object of interest is not present in the crop.","1. A method for image classification, comprising:
labelling, by a first artificial neural network, a crop from an image including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop;
assigning a fully framed class to the labelled crop, including the object of interest, when the object of interest is framed;
assigning a partially framed class to the labelled crop, including the object of interest, when the object of interest is partially framed;
assigning a background class to the labelled crop when the object of interest is not present in the crop; and
classifying the object of interest in the image based on a class assigned to the labelled crop. 1. A method for image classification, comprising:
labelling, by a first artificial neural network, a crop from an image including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop;
assigning a fully framed class to the labelled crop, including the object of interest, when the object of interest is framed;
assigning a partially framed class to the labelled crop, including the object of interest, when the object of interest is partially framed;
assigning a background class to the labelled crop when the object of interest is not present in the crop; and
classifying the object of interest in the image based on a class assigned to the labelled crop. 2. The method of claim 1, further comprising:
weighting a class estimate from a second artificial neural network based on whether the labelled crop is indicated as the background class. 3. The method of claim 1, further comprising:
determining a proportion of the object of interest that is included in the crop; and
comparing the determined proportion to at least one threshold value, and in which the labelling is based on the comparing. 4. A method of training an artificial neural network for image classification, comprising:
selecting a crop from an image;
determining whether the crop of the image includes an area surrounded by a predetermined bounding box;
discarding, prior to the training, the crop when the crop of the image does not include a portion of the area surrounded by the predetermined bounding box; and
training the artificial neural network to determine a label of the crop of the image including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop, the artificial network trained for the image classification of the object of interest in the image with a plurality of crops from the image, the classification based on a class assigned to the labelled crop, each crop comprising at least a portion of the area surrounded by the predetermined bounding box. 4. A method of training an artificial neural network for image classification, comprising:
selecting a crop from an image;
determining whether the crop of the image includes an area surrounded by a predetermined bounding box;
discarding, prior to the training, the crop when the crop of the image does not include a portion of the area surrounded by the predetermined bounding box; and
training the artificial neural network to determine a label of the crop of the image including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop, the artificial network trained for the image classification of the object of interest in the image with a plurality of crops from the image, the classification based on a class assigned to the labelled crop, each crop comprising at least a portion of the area surrounded by the predetermined bounding box. 5. The method of claim 4, further comprising generating a proposed bounding box for the image if the image lacks the predetermined bounding box, before discarding the crop. 6. The method of claim 5, further comprising:
selecting a new crop from the image;
determining whether the new crop of the image includes the area surrounded by the proposed bounding box; and
discarding the new crop if the new crop of the image does not include a portion of the area surrounded by the proposed bounding box. 7. The method of claim 5, in which the generating comprises:
localizing at least one object in the image; and
adding a border around the localized at least one object. 8. The method of claim 7, further comprising:
selecting a second crop including the proposed bounding box;
determining a proportion of the localized at least one object that is included in the second crop; and
updating the proposed bounding box based on the determined proportion. 9. An apparatus for image classification, comprising:
a memory; and
at least one processor coupled to the memory, the at least one processor configured:
to label, by a first artificial neural network, a crop from an image, including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop;
to assign a fully framed class to the labelled crop, including the object of interest, when the object of interest is framed;
to assign a partially framed class to the labelled crop, including the object of interest, when the object of interest is partially framed;
to assign a background class to the labelled crop when the object of interest is not present in the crop; and
to classify the object of interest in the image based on a class assigned to the labelled crop. 9. An apparatus for image classification, comprising:
a memory; and
at least one processor coupled to the memory, the at least one processor configured:
to label, by a first artificial neural network, a crop from an image, including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop;
to assign a fully framed class to the labelled crop, including the object of interest, when the object of interest is framed;
to assign a partially framed class to the labelled crop, including the object of interest, when the object of interest is partially framed;
to assign a background class to the labelled crop when the object of interest is not present in the crop; and
to classify the object of interest in the image based on a class assigned to the labelled crop. 10. The apparatus of claim 9, in which the at least one processor is further configured to weight a class estimate from a second artificial neural network, based on whether the labelled crop is indicated as the background class. 11. The apparatus of claim 9, in which the at least one processor is further configured:
to determine a proportion of the object of interest that is included in the crop; and
to compare the determined proportion to at least one threshold value, and in which the labelling is based on the comparing. 12. An apparatus for training an artificial neural network for image classification, comprising:
a memory; and
at least one processor coupled to the memory, the at least one processor configured:
to select a crop from an image;
to determine whether the crop of the image includes an area surrounded by a predetermined bounding box;
to discard, prior to the training, the crop when the crop of the image does not include a portion of the area surrounded by the predetermined bounding box; and
to train the artificial neural network to determine a label of the crop of the image including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop, the artificial network trained for the image classification of the object of interest in the image with a plurality of crops from the image, the classification based on a class assigned to the labelled crop, each crop comprising at least a portion of the area surrounded by the predetermined bounding box. 12. An apparatus for training an artificial neural network for image classification, comprising:
a memory; and
at least one processor coupled to the memory, the at least one processor configured:
to select a crop from an image;
to determine whether the crop of the image includes an area surrounded by a predetermined bounding box;
to discard, prior to the training, the crop when the crop of the image does not include a portion of the area surrounded by the predetermined bounding box; and
to train the artificial neural network to determine a label of the crop of the image including an object of interest with an indication of whether the object of interest is framed, partially framed, or not present in the crop, the artificial network trained for the image classification of the object of interest in the image with a plurality of crops from the image, the classification based on a class assigned to the labelled crop, each crop comprising at least a portion of the area surrounded by the predetermined bounding box. 13. The apparatus of claim 12, in which the at least one processor is further configured to generate a proposed bounding box for the image if the image lacks the predetermined bounding box, before discarding the crop. 14. The apparatus of claim 13, in which the at least one processor is further configured:
to select a new crop from the image;
to determine whether the new crop of the image includes the area surrounded by the proposed bounding box; and
to discard the new crop if the new crop of the image does not include a portion of the area surrounded by the proposed bounding box. 15. The apparatus of claim 13 in which the at least one processor is further configured to generate the proposed bounding box:
by localizing at least one object in the image; and
by adding a border around the localized at least one object. 16. The apparatus of claim 15, in which the at least one processor is further configured:
to select a second crop including the proposed bounding box;
to determine a proportion of the localized at least one object that is included in the second crop; and
to update the proposed bounding box based on the determined proportion.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318854B2,US10318854B2,Systems and methods for protecting sensitive information stored on a mobile device,2015-05-13,"protecting, method, optical, device, protection, satisfies, requirement, capable, only, magnetic, systems, data, permitting, transmission, sensitive, stored, wireless, predetermined, methods, received, communication, provided, sensor, information, described, from, mobile",A method and device for protecting sensitive information stored on a mobile device capable of wireless communication is described. The protection is provided by permitting wireless transmission of the sensitive information only if data received from an optical sensor or a magnetic sensor satisfies a predetermined requirement.,"1. A method for authorizing wireless communications from a mobile device to a recipient device of an access control reader, comprising:
receiving, at a processor of the mobile device having sensitive information stored in a computer readable memory thereof, from a magnetic sensor of the mobile device, a signal corresponding to a detected sequence of motions of the mobile device within a magnetic field detected by the magnetic sensor, the detected sequence of motions occurring once the mobile device is within a predetermined proximity to the recipient device generating the magnetic field;
analyzing the signal, with the processor, to determine if the detected sequence of motions correlates to a predetermined mobile device sequence of motions requirement, the analyzing the signal comprising generating magnetic field data corresponding to the detected sequence of motions of the mobile device within the predetermined proximity to the recipient device;
determining, with the processor and based on the generated magnetic field data, whether the detected sequence of motions of the mobile device within the predetermined proximity to the recipient device satisfies the predetermined mobile device sequence of motions requirement, the determining comprising comparing the magnetic field data corresponding to the detected sequence of motions of the mobile device with predetermined magnetic field information stored in the computer readable memory, wherein the magnetic field information comprises information about a magnetic field direction; and
selectively transmitting, based on the determination, the sensitive information from the mobile device using a wireless communication protocol. 1. A method for authorizing wireless communications from a mobile device to a recipient device of an access control reader, comprising:
receiving, at a processor of the mobile device having sensitive information stored in a computer readable memory thereof, from a magnetic sensor of the mobile device, a signal corresponding to a detected sequence of motions of the mobile device within a magnetic field detected by the magnetic sensor, the detected sequence of motions occurring once the mobile device is within a predetermined proximity to the recipient device generating the magnetic field;
analyzing the signal, with the processor, to determine if the detected sequence of motions correlates to a predetermined mobile device sequence of motions requirement, the analyzing the signal comprising generating magnetic field data corresponding to the detected sequence of motions of the mobile device within the predetermined proximity to the recipient device;
determining, with the processor and based on the generated magnetic field data, whether the detected sequence of motions of the mobile device within the predetermined proximity to the recipient device satisfies the predetermined mobile device sequence of motions requirement, the determining comprising comparing the magnetic field data corresponding to the detected sequence of motions of the mobile device with predetermined magnetic field information stored in the computer readable memory, wherein the magnetic field information comprises information about a magnetic field direction; and
selectively transmitting, based on the determination, the sensitive information from the mobile device using a wireless communication protocol. 2. The method of claim 1, wherein the magnetic sensor is a magnetometer. 3. The method of claim 1, wherein the signal comprises information about a strength or direction of the magnetic field. 4. The method of claim 3, wherein the signal comprises information about a strength or direction of the magnetic field at a first time and information about the strength or direction of the magnetic field at a second time later than the first time. 5. The method of claim 1, wherein the access control reader comprises a physical access control reader configured to protect a physical asset. 6. The method of claim 1, further comprising establishing a communication channel with the recipient device of the access control reader, and wherein the sensitive information is selectively transmitted from the mobile device to the recipient device via the communication channel. 7. The method of claim 1, wherein the magnetic field data comprises information about a change in direction of the sensed magnetic field. 8. The method of claim 1, wherein the magnetic field data comprises a distance between the magnetic sensor and a source of the magnetic field. 9. A wireless communication device comprising:
a wireless communication radio;
a magnetic sensor;
a processor; and
a memory containing sensitive information and instructions for execution by the processor, the instructions, when executed, causing the processor to:
obtain data from the magnetic sensor corresponding to a detected sequence of motions of the wireless communication device within a magnetic field detected by the magnetic sensor, the detected sequence of motions occurring once the wireless communication device is within a predetermined proximity to a recipient device generating the magnetic field;
generating, based on the obtained data, magnetic field data corresponding to the detected sequence of motions of the wireless communication device within the predetermined proximity to the recipient device;
determine, based on the generated magnetic field data, whether the detected sequence of motions satisfies a predetermined wireless communication device sequence of motions requirement, the determining comprising comparing the generated magnetic field data corresponding to the detected sequence of motions of the wireless communication device with predetermined magnetic field information stored in the memory, the magnetic field information comprising information about a magnetic field direction; and
allow transmission of the sensitive information to the recipient device, using the wireless communication radio, if the predetermined wireless communication device sequence of motions requirement has been satisfied. 9. A wireless communication device comprising:
a wireless communication radio;
a magnetic sensor;
a processor; and
a memory containing sensitive information and instructions for execution by the processor, the instructions, when executed, causing the processor to:
obtain data from the magnetic sensor corresponding to a detected sequence of motions of the wireless communication device within a magnetic field detected by the magnetic sensor, the detected sequence of motions occurring once the wireless communication device is within a predetermined proximity to a recipient device generating the magnetic field;
generating, based on the obtained data, magnetic field data corresponding to the detected sequence of motions of the wireless communication device within the predetermined proximity to the recipient device;
determine, based on the generated magnetic field data, whether the detected sequence of motions satisfies a predetermined wireless communication device sequence of motions requirement, the determining comprising comparing the generated magnetic field data corresponding to the detected sequence of motions of the wireless communication device with predetermined magnetic field information stored in the memory, the magnetic field information comprising information about a magnetic field direction; and
allow transmission of the sensitive information to the recipient device, using the wireless communication radio, if the predetermined wireless communication device sequence of motions requirement has been satisfied. 10. The wireless communication device of claim 9, wherein the magnetic sensor is a magnetometer. 11. The wireless communication device of claim 9, wherein the obtained data comprises information about a strength or direction of the detected magnetic field. 12. The wireless communication device of claim 11, wherein the recipient device comprises a physical access control reader configured to protect a physical asset. 13. The wireless communication device of claim 11, wherein the obtained data comprises first information about a strength or direction of the detected magnetic field at a first time and second information about the strength or direction of the detected magnetic field at a second time later than the first time. 14. The wireless communication device of claim 9, wherein the instructions, when executed, further cause the processor to establish a communication channel with the recipient device, and further wherein the allowing transmission comprises allowing transmission via the communication channel. 15. The wireless communication device of claim 14, wherein the magnetic field data comprises information about a direction of the detected magnetic field. 16. The wireless communication device of claim 9, wherein the predetermined magnetic field information stored in the memory comprises one or more features of a known magnetic field. 17. The wireless communication device of claim 9, wherein the determining comprises determining from the obtained data a distance between the magnetic sensor and the recipient device. 18. A system, comprising:
a processor;
a magnetic sensor; and
a memory, the memory storing sensitive information and instructions for execution by the processor that, when executed by the processor, cause the processor to:
detect, by the magnetic sensor, a magnetic field;
detect, by the magnetic sensor and while the magnetic sensor is within a predetermined proximity to a recipient device generating the magnetic field, a sequence of motions of the magnetic sensor relative to the detected magnetic field;
generate, based on data obtained from the magnetic sensor, magnetic field data corresponding to the detected sequence of motions of the magnetic sensor within the predetermined proximity to the recipient device;
evaluate whether the detected sequence of motions of the magnetic sensor corresponds to a predetermined sequence of motions based on information about the predetermined sequence of motions stored in the memory, the evaluating comprising comparing the magnetic field data with magnetic field information stored in the memory, the magnetic field information comprising information about a magnetic field direction; and
selectively transmit, via a wireless communication radio and based on the evaluating, the sensitive information. 18. A system, comprising:
a processor;
a magnetic sensor; and
a memory, the memory storing sensitive information and instructions for execution by the processor that, when executed by the processor, cause the processor to:
detect, by the magnetic sensor, a magnetic field;
detect, by the magnetic sensor and while the magnetic sensor is within a predetermined proximity to a recipient device generating the magnetic field, a sequence of motions of the magnetic sensor relative to the detected magnetic field;
generate, based on data obtained from the magnetic sensor, magnetic field data corresponding to the detected sequence of motions of the magnetic sensor within the predetermined proximity to the recipient device;
evaluate whether the detected sequence of motions of the magnetic sensor corresponds to a predetermined sequence of motions based on information about the predetermined sequence of motions stored in the memory, the evaluating comprising comparing the magnetic field data with magnetic field information stored in the memory, the magnetic field information comprising information about a magnetic field direction; and
selectively transmit, via a wireless communication radio and based on the evaluating, the sensitive information. 19. The method of claim 18, wherein the detecting a magnetic field occurs without user input. 20. The method of claim 18, wherein the predetermined sequence of motions comprises a sequence of motions in a single plane.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318858B2,US10318858B2,Multi-antenna tuned wearable article,2015-04-08,"method, part, second, structure, configured, wearable, than, within, through, position, modality, article, antennas, transceiver, operatively, with, enclose, communicate, body, wireless, differently, external, multi, includes, communication, being, antenna, coupled, least, system, according, tuned, first","A wearable article, system, and method includes a structure configured to enclose a body part, a first antenna, in a first position on or within the structure, tuned to communicate according to a wireless communication modality through air, a second antenna, in a second position on or within the structure, tuned to communicate according to the wireless communication modality through the body part, the first antenna being tuned differently than the second antenna, and a transceiver, operatively coupled to at least one of the first antenna and the second antenna, configured to communicate with an external antenna via the at least one of the first and second antennas according to the wireless communication modality.","1. A wearable article, comprising:
a structure configured to enclose a body part;
a first antenna, in a first position on or within the structure, tuned to communicate according to a wireless communication modality through air;
a second antenna, in a second position on or within the structure, tuned to communicate according to the wireless communication modality through the body part, the first antenna being tuned differently than the second antenna; and
a transceiver, operatively coupled to at least one of the first antenna and the second antenna, configured to communicate with an external antenna via the at least one of the first and second antennas according to the wireless communication modality;
wherein the wireless communication modality is a first wireless communication modality and further comprising a third antenna, in a third position on or within the structure and coupled to the transceiver, tuned to a second wireless communication modality different than the first wireless communication modality; and
wherein the wearable article is an article of footwear, wherein the structure comprises a tongue, and wherein the third position is on or within the tongue. 1. A wearable article, comprising:
a structure configured to enclose a body part;
a first antenna, in a first position on or within the structure, tuned to communicate according to a wireless communication modality through air;
a second antenna, in a second position on or within the structure, tuned to communicate according to the wireless communication modality through the body part, the first antenna being tuned differently than the second antenna; and
a transceiver, operatively coupled to at least one of the first antenna and the second antenna, configured to communicate with an external antenna via the at least one of the first and second antennas according to the wireless communication modality;
wherein the wireless communication modality is a first wireless communication modality and further comprising a third antenna, in a third position on or within the structure and coupled to the transceiver, tuned to a second wireless communication modality different than the first wireless communication modality; and
wherein the wearable article is an article of footwear, wherein the structure comprises a tongue, and wherein the third position is on or within the tongue. 2. The wearable article of claim 1, wherein the first wireless communication modality is an ultra high frequency (UHF) communication modality and wherein the second wireless communication modality is at least one of a near field communication (NEC) modality and an ISO 15693 modality. 3. The wearable article of claim 1, wherein the transceiver comprises:
a first transceiver, coupled to the first and second antennas, configured to communicate according to the first wireless communication modality via the first and second antennas; and
a second transceiver, coupled to the third antenna, configured to communicate according to the second wireless communication modality via the third antenna. 4. The wearable article of claim 1, wherein the transceiver comprises:
a first transceiver, coupled to the first antenna, configured to communicate according to the wireless communication modality via the first antenna; and
a second transceiver, coupled to the second antenna, configured to communicate according to the wireless communication modality via the second antenna. 5. The wearable article of claim 1, wherein the first and second antennas and the transceiver are disposed on a common substrate, the first and second positions are a common position, and the substrate is positioned in the common position. 6. The wearable article of claim 1, wherein the first and second antennas are discrete components with respect to one another and the first and second positions are separate with respect to one another. 7. The wearable article of claim 1, further comprising an electronic data storage, operatively coupled to the transceiver, configured to store information about at least one of: the wearable article; and a person associated with the wearable article. 8. The wearable article of claim 1, wherein the first and second antennas are further tuned to communicate according to the wireless communication modality through the structure of the article of footwear in addition to through air and the human foot, respectively. 9. The wearable article of claim 1, further comprising an information tag secured to the structure, wherein the first and second antennas are secured, at least in part, to the information tag. 10. The wearable article of claim 9, wherein the information tag includes visual information related to the article of apparel, and wherein the transceiver is configured to transmit wireless information related to the article of apparel, the wireless information including at least some information included in the visual information. 11. The wearable article of claim 9, wherein the wireless communication modality is a first wireless communication modality and further comprising a third antenna, secured, at least in part, to the information tag and coupled to the transceiver, the third antenna tuned to a second wireless communication modality different than the first wireless communication modality. 12. The wearable article of claim 11, wherein the information tag includes visual information related to the article of apparel, and wherein the transceiver is configured to transmit wireless information related to the article of apparel, the wireless information including at least some information included in the visual information. 13. The wearable article of claim 1, further comprising an output device, coupled to the first and second antennas, configured to provide a first indication if the first antenna receives a wireless signal and a second indication if the second antenna receives a wireless signal. 14. The wearable article of claim 13, wherein the first and second indications are at least one of a visual indication and an audio indication. 15. A wearable article, comprising:
a structure configured to enclose a body part;
a first antenna, in a first position on or within the structure, tuned to communicate according to a wireless communication modality through air;
a second antenna, in a second position on or within the structure, tuned to communicate according to the wireless communication modality through the body part, the first antenna being tuned differently than the second antenna; and
a transceiver, operatively coupled to at least one of the first antenna and the second antenna, configured to communicate with an external antenna via the at least one of the first and second antennas according to the wireless communication modality;
wherein the wireless communication modality is a first wireless communication modality and further comprising a third antenna, in a third position on or within the structure and coupled to the transceiver, tuned to a second wireless communication modality different than the first wireless communication modality; and
wherein the structure comprises an outsole and an insole and wherein the first, second, and third positions and the transceiver are at least one of within the outsole and between the outsole and the insole. 15. A wearable article, comprising:
a structure configured to enclose a body part;
a first antenna, in a first position on or within the structure, tuned to communicate according to a wireless communication modality through air;
a second antenna, in a second position on or within the structure, tuned to communicate according to the wireless communication modality through the body part, the first antenna being tuned differently than the second antenna; and
a transceiver, operatively coupled to at least one of the first antenna and the second antenna, configured to communicate with an external antenna via the at least one of the first and second antennas according to the wireless communication modality;
wherein the wireless communication modality is a first wireless communication modality and further comprising a third antenna, in a third position on or within the structure and coupled to the transceiver, tuned to a second wireless communication modality different than the first wireless communication modality; and
wherein the structure comprises an outsole and an insole and wherein the first, second, and third positions and the transceiver are at least one of within the outsole and between the outsole and the insole. 16. The wearable article of claim 15, wherein the first wireless communication modality is an ultra high frequency (UHF) communication modality and wherein the second wireless communication modality is at least one of a near field communication (NEC) modality and an ISO 15693 modality. 17. The wearable article of claim 15, wherein the transceiver comprises:
a first transceiver, coupled to the first and second antennas, configured to communicate according to the first wireless communication modality via the first and second antennas; and
a second transceiver, coupled to the third antenna, configured to communicate according to the second wireless communication modality via the third antenna. 18. The wearable article of claim 15, wherein the transceiver comprises:
a first transceiver, coupled to the first antenna, configured to communicate according to the wireless communication modality via the first antenna; and
a second transceiver, coupled to the second antenna, configured to communicate according to the wireless communication modality via the second antenna. 19. The wearable article of claim 15, wherein the first and second antennas and the transceiver are disposed on a common substrate, the first and second positions are a common position, and the substrate is positioned in the common position. 20. The wearable article of claim 15, wherein the first and second antennas are discrete components with respect to one another and the first and second positions are separate with respect to one another. 21. The wearable article of claim 15, further comprising an electronic data storage, operatively coupled to the transceiver, configured to store information about at least one of: the wearable article; and a person associated with the wearable article. 22. The wearable article of claim 15, wherein the first and second antennas are further tuned to communicate according to the wireless communication modality through the structure of the article of footwear in addition to through air and the human foot, respectively. 23. The wearable article of claim 15, further comprising an information tag secured to the structure, wherein the first and second antennas are secured, at least in part, to the information tag. 24. The wearable article of claim 23, wherein the information tag includes visual information related to the article of apparel, and wherein the transceiver is configured to transmit wireless information related to the article of apparel, the wireless information including at least some information included in the visual information. 25. The wearable article of claim 23, wherein the wireless communication modality is a first wireless communication modality and further comprising a third antenna, secured, at least in part, to the information tag and coupled to the transceiver, the third antenna tuned to a second wireless communication modality different than the first wireless communication modality. 26. The wearable article of claim 25, wherein the information tag includes visual information related to the article of apparel, and wherein the transceiver is configured to transmit wireless information related to the article of apparel, the wireless information including at least some information included in the visual information. 27. The wearable article of claim 15, further comprising an output device, coupled to the first and second antennas, configured to provide a first indication if the first antenna receives a wireless signal and a second indication if the second antenna receives a wireless signal. 28. The wearable article of claim 27, wherein the first and second indications are at least one of a visual indication and an audio indication.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
https://patents.google.com/patent/US10318855B2,US10318855B2,Computing device with NFC and active load modulation for mass transit ticketing,2008-08-08,"enhance, inductive, device, card, computing, distance, capable, host, receives, enough, smartcard, circuits, ticketing, modulation, form, with, load, reader, mass, small, includes, enhancement, write, that, memory, read, power, also, factor, coupling, usable, controller, from, active, rfid, transit",An RFID card includes a smartcard controller that receives power from a host device. The RFID card also includes a small inductive device capable of inductive coupling with an RFID reader. The small inductive device is small enough to fit in the form factor of a memory card or SIM card. Enhancement circuits enhance the usable read and write distance of the RFID card.,"1. A computing device comprising:
a smartcard controller;
an inductive element to interact with a near field communication (NFC) reader, wherein the inductive element is too small to power the smartcard controller when placed in an interrogating radio frequency field; and
an active transmit driver coupled between the smartcard controller and the inductive element to transmit digital data for mass transit ticketing. 1. A computing device comprising:
a smartcard controller;
an inductive element to interact with a near field communication (NFC) reader, wherein the inductive element is too small to power the smartcard controller when placed in an interrogating radio frequency field; and
an active transmit driver coupled between the smartcard controller and the inductive element to transmit digital data for mass transit ticketing. 2. The computing device of claim 1 wherein the smartcard controller is coupled to receive power from the computing device. 3. The computing device of claim 1 wherein the inductive element is tuned to be resonant at 13.56 MHz. 4. The computing device of claim 3 wherein the smartcard controller is coupled to receive power from the computing device. 5. The computing device of claim 1 wherein the active transmit driver includes a load modulation driver circuit. 6. The computing device of claim 5 wherein the smartcard controller is coupled to receive power from the computing device. 7. The computing device of claim 1 wherein the mobile device comprises a mobile phone. 8. The computing device of claim 1 wherein the computing device comprises a mobile computing device. 9. A computing device comprising:
a smartcard controller;
an inductive element to interact with a near field communication (NFC) reader, wherein the inductive element is too small to power the smartcard controller when placed in an interrogating radio frequency field; and
a load modulation circuit coupled between the smartcard controller and the inductive element to transmit digital data for mass transit ticketing. 9. A computing device comprising:
a smartcard controller;
an inductive element to interact with a near field communication (NFC) reader, wherein the inductive element is too small to power the smartcard controller when placed in an interrogating radio frequency field; and
a load modulation circuit coupled between the smartcard controller and the inductive element to transmit digital data for mass transit ticketing. 10. The computing device of claim 9 wherein the smartcard controller is coupled to receive power from the computing device. 11. The computing device of claim 9 wherein the inductive element is tuned to be resonant at 13.56 MHz. 12. The computing device of claim 9 wherein the smartcard controller is coupled to receive power from the computing device. 13. The computing device of claim 12 wherein the smartcard controller is coupled to receive power from the computing device. 14. The computing device of claim 9 wherein the computing device comprises a mobile phone. 15. The computing device of claim 9 wherein the computing device comprises a mobile computing device. 16. A computing device comprising:
a smartcard controller;
an inductive element to interact with a near field communication (NFC) reader, wherein the inductive element is too small to power the smartcard controller when placed in an interrogating radio frequency field; and
a performance enhancement circuit coupled between the smartcard controller and the inductive element, the performance enhancement circuit including a load modulation driver circuit to drive the inductive element to transmit digital data for mass transit ticketing. 16. A computing device comprising:
a smartcard controller;
an inductive element to interact with a near field communication (NFC) reader, wherein the inductive element is too small to power the smartcard controller when placed in an interrogating radio frequency field; and
a performance enhancement circuit coupled between the smartcard controller and the inductive element, the performance enhancement circuit including a load modulation driver circuit to drive the inductive element to transmit digital data for mass transit ticketing. 17. The computing device of claim 16 wherein the smartcard controller is coupled to receive power from the computing device. 18. The computing device of claim 16 wherein the inductive element is tuned to be resonant at 13.56 MHz. 19. The computing device of claim 16 wherein the computing device comprises a mobile phone. 20. The computing device of claim 16 wherein the performance enhancement circuit further includes an amplifier coupled to receive a signal from the inductive coil.",,Résumé généré automatiquement,Problème identifié,Solution proposée,Classification automatique,Non traité
